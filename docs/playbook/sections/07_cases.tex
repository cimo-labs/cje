\section{Case Studies}

\subsection{Overview}

This section presents three compact case studies illustrating CJE in practice: (1) a clean DM comparison, (2) an off-policy IPS evaluation with overlap challenges, and (3) a DR analysis combining logged data with fresh draws.

\subsection{Case 1: DM for model selection (on-policy)}

\paragraph{Setting.} A team is choosing between three candidate models for a code-generation task: GPT-3.5, GPT-4, and Claude-3. They have 1,000 diverse coding prompts and can generate one output per model per prompt. The KPI is ``correctness'' (binary: code passes unit tests or not). They use GPT-4-as-judge to score outputs on a 1--10 scale.

\paragraph{Data collection.}
\begin{itemize}
\item Generate outputs: 3 models $\times$ 1,000 prompts $=$ 3,000 responses.
\item Judge all 3,000 with GPT-4, get scores $S \in [1, 10]$.
\item Sample 150 responses uniformly, run unit tests to get ground truth $Y \in \{0, 1\}$.
\end{itemize}

\paragraph{Calibration.}
\begin{itemize}
\item Fit \autocal{} (5-fold isotonic regression) on the 150-sample oracle slice.
\item Diagnostics: Coverage 96\% (pass), OOF MAE 0.03 (pass), mean preservation $|\bar{f(S)} - \bar{Y}| = 0.01$ (excellent).
\item Map all 3,000 scores to calibrated rewards $R = f(S) \in [0, 1]$.
\end{itemize}

\paragraph{Estimates (with \oua).}
\begin{align*}
\est{V}_{\dm}(\text{GPT-3.5}) &= 0.62 \; [0.58, 0.66] \\
\est{V}_{\dm}(\text{GPT-4}) &= 0.78 \; [0.75, 0.81] \\
\est{V}_{\dm}(\text{Claude-3}) &= 0.81 \; [0.78, 0.84]
\end{align*}
Paired contrasts (tighter CIs due to pairing):
\begin{align*}
\est{\Delta}(\text{GPT-4}, \text{GPT-3.5}) &= +0.16 \; [0.11, 0.21] \\
\est{\Delta}(\text{Claude-3}, \text{GPT-4}) &= +0.03 \; [-0.01, 0.07]
\end{align*}

\paragraph{Diagnostics.}
\begin{itemize}
\item \oua{} share: 12\% (labels sufficient, variance dominated by prompt variation).
\item All diagnostics pass.
\end{itemize}

\paragraph{Conclusion.} Claude-3 and GPT-4 are statistically indistinguishable; both substantially outperform GPT-3.5. Team chooses Claude-3 for deployment based on cost.

\subsection{Case 2: IPS with overlap challenges (off-policy)}

\paragraph{Setting.} A team deployed GPT-3.5-turbo in production for a month, logging 10,000 prompts with responses, judge scores, and logprobs. They now want to estimate what the KPI would have been if they had deployed GPT-4-turbo instead, without regenerating.

\paragraph{Data.}
\begin{itemize}
\item Logged data: $n = 10{,}000$ with $(X_i, A_i, S_i, \log \pi_0(A_i \mid X_i), \log \pi'(A_i \mid X_i))$.
\item Oracle slice: 200 samples with ground truth $Y$ (user satisfaction, binary).
\item No fresh draws (cannot generate for GPT-4-turbo on these old prompts).
\end{itemize}

\paragraph{Calibration.}
\begin{itemize}
\item Fit \autocal{} on 200-sample oracle.
\item Diagnostics: Coverage 89\% (warn), OOF MAE 0.06 (pass), mean preservation OK.
\item Action: Add 20 labels targeting low-$S$ bins to improve coverage to 94\%.
\end{itemize}

\paragraph{Weights and stabilization.}
\begin{itemize}
\item Raw weights: $W_i = \exp(\log \pi'(A_i \mid X_i) - \log \pi_0(A_i \mid X_i))$.
\item Raw ESS: 8\% (poor overlap; GPT-4-turbo is quite different from GPT-3.5-turbo).
\item Apply \simcal{} with $\rho = 0.95$: stabilized ESS improves to 18\%.
\item Hill index $\alpha = 2.6$ (pass, finite variance).
\end{itemize}

\paragraph{Estimate.}
\begin{align*}
\est{V}_{\ips}(\text{GPT-4-turbo}) &= 0.74 \; [0.68, 0.80] \\
\text{(Baseline GPT-3.5-turbo)} &= 0.65 \; [0.62, 0.68] \\
\est{\Delta} &= +0.09 \; [0.02, 0.16]
\end{align*}

\paragraph{Diagnostics.}
\begin{itemize}
\item ESS 18\% (warn, but acceptable given no fresh draws available).
\item \oua{} share 22\% (warn; labels are becoming a bottleneck).
\item Weight max/median ratio: 45 (moderate tail).
\end{itemize}

\paragraph{Sensitivity check.}
\begin{itemize}
\item Cohort restriction: Re-run on prompts with $|\log W| < 2$ ($n = 6{,}000$). ESS improves to 35\%, estimate is $0.76 \; [0.71, 0.81]$ (consistent).
\item Trimming: Drop top 1\% of weights. Estimate shifts to $0.73 \; [0.68, 0.78]$ (minor change, robust).
\end{itemize}

\paragraph{Conclusion.} Switching to GPT-4-turbo would likely improve satisfaction by $\approx 9$ percentage points. The estimate is moderately reliable (ESS 18\%, sensitivity checks agree). Team decides to run a small A/B test to confirm before full deployment.

\subsection{Case 3: DR for tight CIs (off-policy + fresh draws)}

\paragraph{Setting.} Same as Case 2, but the team can now afford to generate 2,000 fresh responses from GPT-4-turbo (20\% of the logged data) to train a critic for DR.

\paragraph{Data.}
\begin{itemize}
\item Logged data: $n = 10{,}000$ (same as Case 2).
\item Oracle slice: 200 (same).
\item Fresh draws: 2,000 prompts sampled uniformly from the 10k, with GPT-4-turbo outputs and judge scores.
\end{itemize}

\paragraph{Weights and calibration.} Same as Case 2 (ESS 18\% after \simcal, $\alpha = 2.6$).

\paragraph{Critic training.}
\begin{itemize}
\item Train a gradient-boosted tree $\hat{g}(X)$ on the 2,000 fresh draws (5-fold cross-fitted) to predict calibrated reward $R$.
\item OOF $R^2 = 0.58$ (moderate; prompt features include length, first-token embedding, topic cluster).
\end{itemize}

\paragraph{DR estimate.}
\begin{align*}
\est{V}_{\dr}(\text{GPT-4-turbo}) &= 0.75 \; [0.71, 0.79]
\end{align*}

\paragraph{Diagnostics.}
\begin{itemize}
\item Orthogonality score: $0.02 \; [-0.03, 0.07]$ (pass, CI contains zero).
\item ESS: still 18\% (unchanged by DR; weights are the same).
\item \oua{} share: 18\% (slightly lower than IPS due to variance reduction from the critic).
\item Compare to IPS: IPS gave $[0.68, 0.80]$ (width 0.12); DR gives $[0.71, 0.79]$ (width 0.08). DR is 33\% tighter.
\end{itemize}

\paragraph{Conclusion.} DR delivers a tighter CI and a more stable estimate. The point estimate (0.75) is consistent with IPS (0.74) and the sensitivity checks. The team is now confident enough to deploy GPT-4-turbo without an A/B test.

\subsection{Lessons learned}

\begin{itemize}
\item \textbf{DM is simplest when you can generate.} Case 1 shows clean paired comparisons with tight CIs.
\item \textbf{IPS works when overlap is moderate.} Case 2 shows that ESS 18\% is usable, especially with sensitivity checks.
\item \textbf{DR tightens CIs and boosts confidence.} Case 3 shows a 33\% CI reduction with only 2,000 fresh draws (20\% of logged data).
\item \textbf{Always check diagnostics.} Coverage, ESS, orthogonality, and \oua{} share catch issues early.
\item \textbf{Sensitivity checks build trust.} Cohort restriction, trimming, and alternative calibrators confirm robustness.
\end{itemize}
