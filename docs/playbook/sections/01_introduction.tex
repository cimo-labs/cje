\section{Introduction}

Large language models (LLMs) are increasingly used as judges: a model (or rubricized prompt) assigns a scalar score $S = s(X, A)$ to an answer $A$ given a prompt $X$. This setup is attractive: it is fast, cheap, and often strongly correlated with human preferences. As a result, many teams now compare policies by averaging raw judge scores and declaring a winner.

\subsection{What practitioners really want}

In day-to-day evaluation work there are two goals: (i) rank candidate policies on a shared prompt set; and (ii) when stakes justify it, estimate levels for a KPI that matters (e.g., pass rate), with a confidence interval. Both questions are causal: \emph{what would happen if we shipped policy $\pi$?} Formally, the target is
\begin{equation}
V(\pi) = \E[Y(\pi)],
\end{equation}
where $Y \in [0, 1]$ is the outcome of interest under the policy we would deploy.

\subsection{Why the heuristic fails}

Naively averaging judge scores is a heuristic that breaks in predictable ways:

\begin{itemize}
\item \textbf{Wrong scale.} Judge scores are not on your KPI scale; deltas can be misleading.
\item \textbf{Hidden drift and slice bias.} The meaning of a score can change across time, prompt families, or length; averages hide this.
\item \textbf{No uncertainty.} Without CIs it is hard to tell real uplifts from noise, especially at small label budgets.
\item \textbf{Off-policy illusion.} Reusing one judged log to assess many candidates without correction answers the wrong counterfactual (it reflects the logger, not the candidate).
\end{itemize}

\subsection{CJE in one paragraph}

\textbf{Causal Judge Evaluation (\cje)} is a practitioner-first wrapper that turns judge scores into causally interpretable estimates and reliable rankings. \cje{} learns a small, mean-preserving mapping from score to outcome (\autocal), evaluates policies either with fresh generations (Direct Modeling, \dm) or by safely reusing a judged log (Calibrated \ips/\dr), and attaches oracle-uncertainty-aware confidence intervals (\oua). For off-policy reuse, \cje{} stabilizes importance weights with a mean-one, score-indexed projection (\simcal) and, when helpful, adds a lightweight critic (an outcome model) to form a doubly robust estimate.

\subsection{Notation at a glance}

\begin{table}[h]
\centering
\caption{Key notation and terminology}
\label{tab:notation}
\begin{tabular}{ll}
\toprule
\textbf{Symbol/Term} & \textbf{Meaning} \\
\midrule
$X$ & Prompt (input) \\
$A$ & Response (action/output) \\
$\pi_0$ & Logging policy (baseline, deployed) \\
$\pi'$ & Candidate policy (target for evaluation) \\
$S$ & Judge score $S = s(X, A) \in \mathbb{R}$ \\
$Y$ & Oracle label (ground truth outcome) $\in [0, 1]$ \\
$R$ & Calibrated reward $R = f(S) \in [0, 1]$ \\
$W_i$ & Raw importance weight $W_i = \pi'(A_i \mid X_i) / \pi_0(A_i \mid X_i)$ \\
$\tilde{w}_i$ & Stabilized, mean-one weight after \simcal \\
$w^{\text{sn}}_i$ & Self-normalized (Hájek) weight $w^{\text{sn}}_i = W_i / \bar{W}$ \\
\midrule
\textbf{\autocal} & \textbf{AutoCal-R}: Automatic Calibration for Rewards \\
 & Isotonic map $f: S \to [0,1]$ learned from oracle slice \\
\textbf{\simcal} & \textbf{SIMCal}: Score-Indexed Monotone Calibration \\
 & Stabilizes weights via monotone projection + variance cap \\
\textbf{\oua} & \textbf{OUA}: Oracle-Uncertainty Aware inference \\
 & Accounts for calibrator noise in confidence intervals \\
\midrule
\textbf{\dm} & Direct Modeling (on-policy, fresh draws only) \\
\textbf{\ips} & Importance Sampling (off-policy, logged data) \\
\textbf{\dr} & Doubly Robust (off-policy + critic/outcome model) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Quick Start: Direct Mode for Model Comparison}

\textbf{Just want to compare models on an eval set?} This is the fastest path to reliable estimates with confidence intervals.

\paragraph{What you need:}
\begin{itemize}
\item 500--2000 prompts representative of deployment
\item Responses from each policy you want to compare
\item Judge scores for all responses (any scale works)
\item 50--150 oracle labels (ground truth on a random sample)
\end{itemize}

\paragraph{Steps:}
\begin{enumerate}
\item Generate outputs from each policy on the same prompts (paired sampling reduces variance).
\item Score all outputs with your judge (fixed version, frozen config).
\item Label a random sample of 50--150 examples with ground truth $Y$.
\item Run \textbf{Direct Mode} (§2)---CJE learns the judge$\to$oracle calibration $f(S)$ automatically via \autocal.
\item Check three core diagnostics: coverage (§4.1), calibration (§4.2), judge stability (§4.3).
\item Report estimates with 95\% CIs (includes \oua{} to account for calibrator uncertainty).
\end{enumerate}

\paragraph{What to read:}
\begin{itemize}
\item \textbf{Essential:} §2 (Direct Mode), §4.1--4.4 (core diagnostics), §6.1 (DM workflow).
\item \textbf{Skip for now:} §3 (off-policy methods), §4.5--4.7 (weight diagnostics).
\end{itemize}

You can return to off-policy methods (§3) later if you want counterfactual inference (``What if we deployed policy X?'') without regenerating outputs.

\subsection{On-policy vs. off-policy at a glance}

\begin{itemize}
\item \textbf{\dm{} (on-policy, offline).} Generate outputs for each policy on the same prompt set, convert scores to calibrated rewards $R = f(S)$, average, and report a CI that includes calibrator noise (\oua).

\item \textbf{Calibrated \ips{} (off-policy).} Reweight calibrated rewards from a single judged log using per-sequence likelihood ratios; stabilize weights with \simcal{} for usable ESS and tame tails.

\item \textbf{Calibrated \dr{} (off-policy + critic).} Combine a critic with stabilized weights to gain robustness and tighter intervals; valid if either weights behave or the critic is decent.
\end{itemize}

\subsection{High-leverage diagnostics (and how to fix issues)}

\cje{} emphasizes a short checklist that catches the most important failure modes and suggests concrete fixes:

\begin{enumerate}[label=(\alph*)]
\item \textbf{Score coverage} of the labeled slice across the evaluation $S$-range (fix: targeted labeling in uncovered $S$ bins).

\item \textbf{Calibration reliability} of \autocal{} by region (fix: two-stage \autocal{} or add a coarse index such as prompt family/length; add a few labels).

\item \textbf{Judge stability} via Kendall $\tau$ on anchor set or temporal batches (fix: freeze judge version, refresh oracle if drift detected).

\item \textbf{Effective sample size (ESS)} for \ips/\dr{} after \simcal{} (fix: cohort restriction or switch to \dr{} with a stronger critic).

\item \textbf{Tail heaviness} of weights (Hill index) (fix: \simcal{} tuning, overlap-aware cohorting, provider TF checks, trimming sensitivity).

\item \textbf{\dr{} orthogonality} moment near zero with a CI (fix: improve critic/regularization or revisit \simcal/overlap).
\end{enumerate}

These diagnostics are designed to detect when the heuristic breaks and to point directly to high-leverage remediations.

\subsection{What this paper is (and is not)}

The main text is a playbook: assumptions in plain English, minimal recipes for \dm/\ips/\dr, the diagnostics above, and reporting templates with honest CIs. Advanced material---why the projections preserve the target and reduce variance, and full statistical guarantees---lives in the appendix.

\cje{} builds on the broader treatment effect estimation literature, particularly work on efficient estimation with surrogates and limited outcome data \cite{kallus2024role}. Like that work, we avoid restrictive surrogacy conditions (e.g., statistical surrogacy requiring $Y \perp T \mid S$) and instead study efficiency gains when surrogates supplement---rather than replace---primary outcomes under standard missing-at-random assumptions.

\subsection{Contributions}

\begin{itemize}
\item A clear estimand and three analysis modes (\dm, Calibrated \ips, Calibrated \dr) tailored to common evaluation workflows.

\item \autocal{} to put judge scores on the KPI scale and \oua{} to keep intervals honest.

\item \simcal{} to stabilize off-policy weighting, plus a critic option for \dr{} robustness.

\item A six-diagnostic dashboard (four core, three off-policy) with concrete operator fixes and lightweight artifacts for audit and reproducibility.
\end{itemize}

\subsection{Reader's guide}

\paragraph{For quick model comparison:} Start with the Quick Start (§1.5), then read §2 (Direct Mode), §4.1--4.4 (core diagnostics), and §6.1 (DM workflow). Skip §3 and §4.5--4.7 unless you later need counterfactual inference.

\paragraph{For comprehensive coverage:} We begin with \dm---the default when you can generate on shared prompts---then show how to reuse judged logs with Calibrated \ips{} and Calibrated \dr, followed by diagnostics, assumptions for causal interpretation, a practical playbook, compact case studies, and implementation notes.
