\section{Diagnostics: The Five High-Leverage Checks}

\subsection{Overview}

CJE's diagnostic dashboard is designed to catch the most important failure modes with minimal operator overhead. Each diagnostic points directly to a concrete fix. This section details the five checks, their interpretation, thresholds, and remediations.

\subsection{The diagnostic checklist}

\begin{enumerate}
\item \textbf{Score coverage} (for all modes): Does the oracle slice span the evaluation $S$-range?
\item \textbf{Calibration reliability} (for all modes): Is $f(S)$ accurate across the $S$ spectrum?
\item \textbf{Effective sample size (ESS)} (for \ips/\dr): Do we have enough effective samples after reweighting?
\item \textbf{Tail heaviness} (for \ips/\dr): Are importance weights catastrophically heavy-tailed?
\item \textbf{DR orthogonality} (for \dr): Is the critic good enough for doubly robust guarantees?
\end{enumerate}

Each diagnostic produces a compact artifact (a number, a CI, or a plot) and a traffic-light signal (pass / warn / fail).

\subsection{Diagnostic 1: Score coverage}

\paragraph{What it measures.} The fraction of evaluated scores $S$ that fall within the range of oracle scores used to train \autocal. Also checks boundary slopes for extrapolation risk.

\paragraph{Why it matters.} If \autocal{} must extrapolate beyond the labeled $S$-range, calibrated rewards $R = f(S)$ can be wildly wrong. This is the single most common source of large errors.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{Coverage fraction:} $\#\{i : S_{\min}^{\text{oracle}} \le S_i \le S_{\max}^{\text{oracle}}\} / n$.
\item \textbf{Boundary slopes:} slope of $f$ near $S_{\min}$ and $S_{\max}$.
\item \textbf{Histogram overlay:} oracle $S$ distribution vs.\ evaluation $S$ distribution.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} Coverage $\ge 95\%$, boundary slopes moderate.
\item \textbf{Warn:} Coverage $\in [85\%, 95\%)$ or steep boundary slopes.
\item \textbf{Fail:} Coverage $< 85\%$ or nearly-flat/nearly-vertical boundary slopes.
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Add a small number of labels (5--20) targeting the uncovered $S$ bins.
\item Narrow the prompt set to the intended deployment slice.
\item If the evaluation set is meant to stress-test edge cases, ensure the oracle slice includes examples from those edges.
\end{itemize}

\subsection{Diagnostic 2: Calibration reliability}

\paragraph{What it measures.} Out-of-fold (OOF) calibration accuracy: how well $f(S)$ predicts $Y$ on held-out oracle data. Includes overall error and regional error (low/mid/high $S$).

\paragraph{Why it matters.} Even if coverage is good, \autocal{} can be miscalibrated in specific regions (e.g., $f(S)$ is too optimistic for high $S$). Regional miscalibration biases estimates and breaks mean preservation.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{OOF calibration curve:} scatter plot of $f(S)$ vs.\ $Y$ on held-out folds, with diagonal reference.
\item \textbf{Mean preservation check:} $|\E_{\text{oracle}}[f(S)] - \E_{\text{oracle}}[Y]|$ (should be $\approx 0$).
\item \textbf{Regional error:} mean absolute error (MAE) in low, mid, high $S$ terciles.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} OOF MAE $< 0.05$, regional MAE balanced, mean preservation within $\pm 0.02$.
\item \textbf{Warn:} OOF MAE $\in [0.05, 0.10]$ or one region has $2\times$ the error of others.
\item \textbf{Fail:} OOF MAE $> 0.10$ or severe regional imbalance or mean drift $> 0.05$.
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Enable two-stage \autocal{} (spline index $\to$ isotonic) for regional adaptivity.
\item Add a coarse index (prompt family, length bin) to allow family-specific calibration.
\item Collect 10--30 additional labels in the problematic $S$ region.
\item Check if judge drift occurred (see judge stability diagnostic).
\end{itemize}

\subsection{Diagnostic 3: Effective sample size (ESS)}

\paragraph{What it measures.} The number of ``effective'' independent samples after importance weighting:
\begin{equation}
\text{ESS} = \frac{\left( \sum_i W_i \right)^2}{\sum_i W_i^2}.
\end{equation}
ESS ranges from 1 (one weight dominates) to $n$ (all weights equal).

\paragraph{Why it matters.} Low ESS means a few samples dominate the estimate, leading to high variance and unreliable CIs. ESS is the primary diagnostic for off-policy viability.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{ESS (absolute and \%)}: report both raw ESS and ESS$/n$.
\item \textbf{Weight distribution:} histogram or quantiles (min, median, 95th percentile, max) before and after \simcal.
\item \textbf{Weight vs.\ $S$ scatter:} check if weights are monotone in $S$ (validates J2-M assumption).
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} ESS $\ge 50\%$ of $n$ (excellent overlap).
\item \textbf{Warn:} ESS $\in [10\%, 50\%)$ (moderate overlap; \dr{} recommended).
\item \textbf{Fail:} ESS $< 10\%$ (poor overlap; estimates unreliable unless \dr{} with strong outcome model).
\item \textbf{Critical:} ESS $< 1\%$ (catastrophic; do not trust any estimate).
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Use \simcal{} with aggressive variance cap $\rho$ (e.g., 0.90).
\item Restrict to a high-overlap cohort (e.g., prompts where $|\log W_i| < 2$).
\item Switch from \ips{} to \dr{} with a strong outcome model.
\item If ESS $< 1\%$, regenerate fresh outputs for the candidate policy and use \dm{} instead.
\end{itemize}

\subsection{Diagnostic 4: Tail heaviness (Hill index)}

\paragraph{What it measures.} The tail index $\alpha$ of the weight distribution, estimated via the Hill estimator. If $\alpha < 2$, the weights have infinite variance; if $\alpha < 1$, infinite mean.

\paragraph{Why it matters.} Heavy-tailed weights break standard inference. Even if ESS looks reasonable, $\alpha < 2$ means variance estimates are unreliable and CIs can be arbitrarily wrong.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{Hill index $\alpha$:} estimated from the upper tail of $W$.
\item \textbf{QQ-plot:} compare weight quantiles to Pareto($\alpha$) reference.
\item \textbf{Max/median ratio:} if $> 100$, likely heavy-tailed.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} $\alpha \ge 3$ (well-behaved tails).
\item \textbf{Warn:} $\alpha \in [2, 3)$ (finite variance, but higher moments unstable).
\item \textbf{Fail:} $\alpha < 2$ (infinite variance; estimates unstable).
\item \textbf{Critical:} $\alpha < 1$ (infinite mean; nonsensical).
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Apply \simcal{} with a strict variance cap $\rho < 0.95$.
\item Cohort restriction to high-overlap prompts.
\item Check for provider drift (temperature, frequency penalty) that might cause unexpected tail behavior.
\item If $\alpha < 2$ persists, switch to \dr{} or regenerate.
\end{itemize}

\subsection{Diagnostic 5: DR orthogonality score}

\paragraph{What it measures.} The empirical moment
\begin{equation}
\text{OrthoScore} = \frac{1}{n} \sum_{i=1}^n W_i \cdot (R_i - \hat{g}(S_i)),
\end{equation}
with a 95\% CI. Under ideal conditions (mean-one weights and well-specified outcome model), this should be near zero.

\paragraph{Why it matters.} Orthogonality is the key to doubly robust inference. If the orthogonality score is far from zero, either the weights or the outcome model (or both) are poor, and \dr{} loses its efficiency and robustness guarantees.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{Orthogonality score with CI:} point estimate $\pm 1.96 \cdot \SE$.
\item \textbf{Residual plot:} $(R_i - \hat{g}(S_i))$ vs.\ $W_i$ to spot patterns.
\item \textbf{Outcome model performance:} OOF $R^2$ or MAE on fresh draws.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} 95\% CI contains zero, $|$OrthoScore$| < 0.05$.
\item \textbf{Warn:} CI contains zero, but $|$OrthoScore$| \in [0.05, 0.10]$.
\item \textbf{Fail:} CI does not contain zero or $|$OrthoScore$| > 0.10$.
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Improve the outcome model: add regularization, use a more flexible model (e.g., two-stage isotonic), or increase fresh draw sample size.
\item Revisit \simcal{} settings or overlap diagnostics (poor weights can break orthogonality).
\item Check cross-fitting: ensure folds are balanced and representative.
\item If orthogonality fails persistently, fall back to \ips{} or regenerate for \dm.
\end{itemize}

\subsection{Judge stability diagnostic (supplementary)}

\paragraph{What it measures.} Rank stability of judge scores on a small anchor set (20--50 fixed prompts) over time, measured via Kendall $\tau$ or Spearman $\rho$.

\paragraph{Why it matters.} If the judge's interpretation of scores drifts between the time the log was collected and the time fresh draws are evaluated, calibration breaks. This is especially critical for off-policy evaluation.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{Rank correlation:} Kendall $\tau$ between scores at time $t_0$ (logging) and $t_1$ (evaluation).
\item \textbf{Score drift plot:} scatter of $S_{t_0}$ vs.\ $S_{t_1}$ for anchor prompts.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} $\tau \ge 0.90$ (strong stability).
\item \textbf{Warn:} $\tau \in [0.75, 0.90)$ (moderate drift; check config).
\item \textbf{Fail:} $\tau < 0.75$ (severe drift; refresh oracle and re-calibrate).
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Freeze judge version and config during the evaluation window.
\item If drift is detected, refresh the oracle slice with new labels and re-fit \autocal.
\item Consider using a judging protocol with locked temperature/sampling to reduce stochasticity.
\end{itemize}

\paragraph{Automated temporal drift detection.} When evaluation data includes timestamps, CJE can automatically detect drift over time using \texttt{timestamp\_field} and \texttt{check\_drift=True}. This sorts samples by timestamp, computes sequential Kendall $\tau$ correlations across time batches, and flags drift points. This automates the anchor-set workflow above for datasets with temporal metadata.

\subsection{Oracle uncertainty (OUA) share diagnostic}

\paragraph{What it measures.} The fraction of total variance attributable to oracle uncertainty (calibrator noise):
\begin{equation}
\text{OUA share} = \frac{\Var_{\oua}}{\SE^2_{\text{total}}}.
\end{equation}

\paragraph{Why it matters.} High OUA share means label scarcity is the bottleneck, not prompt scarcity. Adding more prompts won't help; you need more labels.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{OUA share:} reported as a percentage.
\item \textbf{Variance decomposition:} $\SE^2_{\text{total}} = \Var_{\text{main}} + \Var_{\oua}$.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} OUA share $< 20\%$ (labels sufficient).
\item \textbf{Warn:} OUA share $\in [20\%, 50\%]$ (labels becoming a bottleneck).
\item \textbf{Fail:} OUA share $> 50\%$ (labels are the main source of uncertainty).
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Add labels, prioritizing regions where $S$ is sparse or where policy comparisons are sensitive.
\item For high OUA share, adding prompts yields diminishing returns; focus labeling budget on coverage and regional balance.
\end{itemize}

\subsection{Traffic-light summary and refusal gates}

CJE estimators can optionally enforce \textbf{refusal gates}: if a diagnostic crosses a critical threshold, the estimator returns \texttt{NaN} instead of a potentially misleading number.

\paragraph{Default gates:}
\begin{itemize}
\item ESS $< 1\%$ $\to$ refuse to estimate (off-policy only).
\item Hill index $\alpha < 2$ $\to$ refuse (off-policy only).
\item Coverage $< 50\%$ $\to$ warn strongly (all modes).
\end{itemize}

Operators can adjust these gates or disable them for exploratory analysis, but the default is to fail loudly rather than produce garbage.

\subsection{Diagnostic workflow (flowchart)}

\begin{enumerate}
\item \textbf{Check score coverage.} If $< 85\%$, add labels or narrow scope. If pass, continue.
\item \textbf{Check calibration reliability.} If regional error is high, enable two-stage \autocal{} or add labels. If pass, continue.
\item \textbf{(Off-policy only) Check ESS.} If $< 10\%$, use \simcal{} or switch to \dr. If $< 1\%$, regenerate. If pass, continue.
\item \textbf{(Off-policy only) Check tail index.} If $\alpha < 2$, apply stricter \simcal{} or cohort restriction. If pass, continue.
\item \textbf{(DR only) Check orthogonality.} If CI excludes zero, improve outcome model or weights. If pass, trust the estimate.
\item \textbf{Check OUA share.} If $> 50\%$, add labels. If pass, you're done.
\end{enumerate}

If any diagnostic fails and the fix is not immediately available, report the failure and do not ship the estimate.

\subsection{Visualization gallery (artifacts)}

For full audit trails, CJE produces:
\begin{itemize}
\item \textbf{Calibration curve:} $f(S)$ vs.\ $Y$ with OOF fit and mean-preservation line.
\item \textbf{Coverage histogram:} oracle $S$ vs.\ evaluation $S$ distributions.
\item \textbf{Weight distribution:} before/after \simcal, with ESS and Hill index annotations.
\item \textbf{Orthogonality residual plot:} $(R - \hat{g}(S))$ vs.\ $W$ with CI band.
\item \textbf{OUA variance decomposition:} stacked bar showing $\Var_{\text{main}}$ vs.\ $\Var_{\oua}$.
\end{itemize}

These plots are auto-generated by the CJE visualization module and can be included in reports or dashboards.

\subsection{Example: interpreting a diagnostic report}

\begin{lstlisting}
=== CJE Diagnostic Report ===
Mode: Calibrated DR
Estimator: stacked-dr
Target policy: gpt-4-mini
Baseline policy: gpt-3.5-turbo

[1] Score Coverage: 92% (PASS)
    Oracle S range: [2.1, 9.8]
    Eval S range: [1.9, 9.9]
    Boundary slopes: moderate

[2] Calibration Reliability: OOF MAE = 0.04 (PASS)
    Mean preservation: -0.01 (excellent)
    Regional MAE: low=0.03, mid=0.04, high=0.05 (balanced)

[3] ESS: 342 / 1000 = 34% (WARN)
    Raw ESS: 18%
    Post-SIMCal ESS: 34%
    Recommendation: acceptable for DR; consider cohort restriction if CI too wide

[4] Tail Index: alpha = 2.8 (PASS)
    Max/median weight ratio: 12.3
    Tail behavior: well-behaved

[5] DR Orthogonality: 0.03 [-0.02, 0.08] (PASS)
    CI contains zero
    Outcome model OOF R^2: 0.67

[6] OUA Share: 15% (PASS)
    Labels sufficient; variance dominated by prompt variation

Overall: PASS (with ESS warning)
Recommendation: Estimate is reliable. Consider adding fresh draws or cohort restriction for tighter CIs.
\end{lstlisting}

\subsection{Summary}

The five diagnostics (coverage, reliability, ESS, tail index, orthogonality) catch the most common failure modes. Each produces a compact artifact, a traffic-light signal, and a concrete fix. Always inspect diagnostics before trusting estimates, and always report them alongside results.
