\section{Diagnostics: The Six High-Leverage Checks}

\subsection{Overview}

CJE's diagnostic dashboard is designed to catch the most important failure modes with minimal operator overhead. Each diagnostic points directly to a concrete fix. This section details the six checks, their interpretation, thresholds, and remediations.

\subsection{The diagnostic checklist}

\paragraph{Core diagnostics (all modes):}
\begin{enumerate}
\item \textbf{Score coverage}: Does the oracle slice span the evaluation $S$-range?
\item \textbf{Calibration reliability}: Is $f(S)$ accurate across the $S$ spectrum?
\item \textbf{Judge stability}: Is the judge's scoring consistent over time or across batches?
\item \textbf{Oracle uncertainty share}: Is label scarcity the bottleneck for precision?
\end{enumerate}

\paragraph{Off-policy diagnostics (for \ips/\dr{} only):}
\begin{enumerate}[resume]
\item \textbf{Effective sample size (ESS)}: Do we have enough effective samples after reweighting?
\item \textbf{Tail heaviness}: Are importance weights catastrophically heavy-tailed?
\item \textbf{DR orthogonality} (for \dr{} only): Is the critic good enough for doubly robust guarantees?
\end{enumerate}

Each diagnostic produces a compact artifact (a number, a CI, or a plot) and a traffic-light signal (pass / warn / fail).

\textbf{If you're using Direct Mode only:} Focus on diagnostics 1--4. Skip diagnostics 5--7 unless you later need counterfactual inference.

\subsection{Diagnostic 1: Score coverage}

\paragraph{What it measures.} The fraction of evaluated scores $S$ that fall within the range of oracle scores used to train \autocal. Also checks boundary slopes for extrapolation risk.

\paragraph{Why it matters.} If \autocal{} must extrapolate beyond the labeled $S$-range, calibrated rewards $R = f(S)$ can be wildly wrong. This is the single most common source of large errors.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{Coverage fraction:} $\#\{i : S_{\min}^{\text{oracle}} \le S_i \le S_{\max}^{\text{oracle}}\} / n$.
\item \textbf{Boundary slopes:} slope of $f$ near $S_{\min}$ and $S_{\max}$.
\item \textbf{Histogram overlay:} oracle $S$ distribution vs.\ evaluation $S$ distribution.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} Coverage $\ge 95\%$, boundary slopes moderate.
\item \textbf{Warn:} Coverage $\in [85\%, 95\%)$ or steep boundary slopes.
\item \textbf{Fail:} Coverage $< 85\%$ or nearly-flat/nearly-vertical boundary slopes.
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Add a small number of labels (5--20) targeting the uncovered $S$ bins.
\item Narrow the prompt set to the intended deployment slice.
\item If the evaluation set is meant to stress-test edge cases, ensure the oracle slice includes examples from those edges.
\end{itemize}

\subsection{Diagnostic 2: Calibration reliability}

\paragraph{What it measures.} Out-of-fold (OOF) calibration accuracy: how well $f(S)$ predicts $Y$ on held-out oracle data. Includes overall error and regional error (low/mid/high $S$).

\paragraph{Why it matters.} Even if coverage is good, \autocal{} can be miscalibrated in specific regions (e.g., $f(S)$ is too optimistic for high $S$). Regional miscalibration biases estimates and breaks mean preservation.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{OOF calibration curve:} scatter plot of $f(S)$ vs.\ $Y$ on held-out folds, with diagonal reference.
\item \textbf{Mean preservation check:} $|\E_{\text{oracle}}[f(S)] - \E_{\text{oracle}}[Y]|$ (should be $\approx 0$).
\item \textbf{Regional error:} mean absolute error (MAE) in low, mid, high $S$ terciles.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} OOF MAE $< 0.05$, regional MAE balanced, mean preservation within $\pm 0.02$.
\item \textbf{Warn:} OOF MAE $\in [0.05, 0.10]$ or one region has $2\times$ the error of others.
\item \textbf{Fail:} OOF MAE $> 0.10$ or severe regional imbalance or mean drift $> 0.05$.
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Enable two-stage \autocal{} (spline index $\to$ isotonic) for regional adaptivity.
\item Add a coarse index (prompt family, length bin) to allow family-specific calibration.
\item Collect 10--30 additional labels in the problematic $S$ region.
\item Check if judge drift occurred (see ยง4.3).
\end{itemize}

\subsection{Diagnostic 3: Judge stability}

\paragraph{What it measures.} Rank stability of judge scores on a small anchor set (20--50 fixed prompts) over time or across batches, measured via Kendall $\tau$ or Spearman $\rho$.

\paragraph{Why it matters.} If the judge's interpretation of scores drifts between the time the log was collected and the time fresh draws are evaluated, calibration breaks. Judge drift affects \emph{all} evaluation modes (DM, IPS, DR) because it invalidates the assumption that $S = s(X, A)$ is stable. This is a fundamental validity check.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{Rank correlation:} Kendall $\tau$ between scores at time $t_0$ (logging) and $t_1$ (evaluation).
\item \textbf{Score drift plot:} scatter of $S_{t_0}$ vs.\ $S_{t_1}$ for anchor prompts.
\item \textbf{Temporal batch correlation:} sequential $\tau$ across time-ordered batches (if timestamps available).
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} $\tau \ge 0.90$ (strong stability).
\item \textbf{Warn:} $\tau \in [0.75, 0.90)$ (moderate drift; check config).
\item \textbf{Fail:} $\tau < 0.75$ (severe drift; refresh oracle and re-calibrate).
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Freeze judge version and config during the evaluation window.
\item If drift is detected, refresh the oracle slice with new labels and re-fit \autocal.
\item Consider using a judging protocol with locked temperature/sampling to reduce stochasticity.
\item Use automated temporal drift detection: set \texttt{check\_drift=True} and \texttt{timestamp\_field} to monitor stability across batches.
\end{itemize}

\paragraph{Automated detection.} When evaluation data includes timestamps, CJE can automatically detect drift over time using \texttt{timestamp\_field} and \texttt{check\_drift=True}. This sorts samples by timestamp, computes sequential Kendall $\tau$ correlations across time batches, and flags drift points. This automates the anchor-set workflow for datasets with temporal metadata.

\subsection{Diagnostic 4: Oracle uncertainty (OUA) share}

\paragraph{What it measures.} The fraction of total variance attributable to oracle uncertainty (calibrator noise):
\begin{equation}
\text{OUA share} = \frac{\Var_{\oua}}{\SE^2_{\text{total}}}.
\end{equation}

\paragraph{Why it matters.} High OUA share means label scarcity is the bottleneck, not prompt scarcity. Adding more prompts won't help; you need more labels.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{OUA share:} reported as a percentage.
\item \textbf{Variance decomposition:} $\SE^2_{\text{total}} = \Var_{\text{main}} + \Var_{\oua}$.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} OUA share $< 20\%$ (labels sufficient).
\item \textbf{Warn:} OUA share $\in [20\%, 50\%]$ (labels becoming a bottleneck).
\item \textbf{Fail:} OUA share $> 50\%$ (labels are the main source of uncertainty).
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Add labels, prioritizing regions where $S$ is sparse or where policy comparisons are sensitive.
\item For high OUA share, adding prompts yields diminishing returns; focus labeling budget on coverage and regional balance.
\end{itemize}

\subsection*{Off-Policy Diagnostics (IPS/DR Only)}

The following diagnostics only apply when reusing logged data. \textbf{If you're using Direct Mode, you can skip to ยง4.8.}

\subsection{Diagnostic 5: Effective sample size (ESS)}

\paragraph{What it measures.} The number of ``effective'' independent samples after importance weighting:
\begin{equation}
\text{ESS} = \frac{\big(\sum_{i=1}^n \tilde{w}_i\big)^2}{\sum_{i=1}^n \tilde{w}_i^2},
\end{equation}
where $\tilde{w}_i$ are the stabilized, mean-one weights after \simcal. ESS ranges from 1 (one weight dominates) to $n$ (all weights equal).

\paragraph{Why it matters.} Low ESS means a few samples dominate the estimate, leading to high variance and unreliable CIs. ESS is the primary diagnostic for off-policy viability.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{ESS (absolute and \%)}: report both raw ESS and ESS$/n$.
\item \textbf{Weight distribution:} histogram or quantiles (min, median, 95th percentile, max) before and after \simcal.
\item \textbf{Weight vs.\ $S$ scatter:} check if weights are monotone in $S$ (validates J2-M assumption).
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} ESS $\ge 30\%$ of $n$ (excellent overlap).
\item \textbf{Warn:} ESS $\in [10\%, 30\%)$ (moderate overlap; \dr{} recommended).
\item \textbf{Fail:} ESS $< 10\%$ (poor overlap; estimates unreliable unless \dr{} with strong outcome model).
\item \textbf{Refuse:} ESS $< 1\%$ (catastrophic; do not trust any estimate).
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Use \simcal{} with aggressive variance cap $\rho$ (e.g., 0.90).
\item Restrict to a high-overlap cohort (e.g., prompts where $|\log \tilde{w}_i| < 2$).
\item Switch from \ips{} to \dr{} with a strong outcome model.
\item If ESS $< 1\%$, regenerate fresh outputs for the candidate policy and use \dm{} instead.
\end{itemize}

\subsection{Diagnostic 6: Tail heaviness (Hill index)}

\paragraph{What it measures.} The tail index $\alpha$ of the weight distribution, estimated via the Hill estimator. If $\alpha < 2$, the weights have infinite variance; if $\alpha < 1$, infinite mean.

\paragraph{Why it matters.} Heavy-tailed weights break standard inference. Even if ESS looks reasonable, $\alpha < 2$ means variance estimates are unreliable and CIs can be arbitrarily wrong.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{Hill index $\alpha$:} estimated from the upper tail of $\tilde{w}$ (post-\simcal{} weights).
\item \textbf{QQ-plot:} compare weight quantiles to Pareto($\alpha$) reference.
\item \textbf{Max/median ratio:} if $> 100$, likely heavy-tailed.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} $\alpha \ge 3$ (well-behaved tails).
\item \textbf{Warn:} $\alpha \in [2, 3)$ (finite variance, but higher moments unstable).
\item \textbf{Fail:} $\alpha < 2$ (infinite variance; estimates unstable).
\item \textbf{Critical:} $\alpha < 1$ (infinite mean; nonsensical).
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Apply \simcal{} with a strict variance cap $\rho < 0.95$.
\item Cohort restriction to high-overlap prompts.
\item Check for provider drift (temperature, frequency penalty) that might cause unexpected tail behavior.
\item If $\alpha < 2$ persists, switch to \dr{} or regenerate.
\end{itemize}

\subsection{Diagnostic 7: DR orthogonality score}

\paragraph{What it measures.} The empirical moment
\begin{equation}
\text{OrthoScore} = \frac{1}{n} \sum_{i=1}^n \tilde{w}_i \cdot (R_i - \hat{g}(S_i)),
\end{equation}
with a 95\% CI, where $\tilde{w}_i$ are the stabilized, mean-one weights after \simcal. Under ideal conditions (mean-one weights and well-specified outcome model), this should be near zero.

\paragraph{Why it matters.} Orthogonality is the key to doubly robust inference. If the orthogonality score is far from zero, either the weights or the outcome model (or both) are poor, and \dr{} loses its efficiency and robustness guarantees.

\paragraph{Artifacts to inspect:}
\begin{itemize}
\item \textbf{Orthogonality score with CI:} point estimate $\pm 1.96 \cdot \SE$.
\item \textbf{Residual plot:} $(R_i - \hat{g}(S_i))$ vs.\ $\tilde{w}_i$ to spot patterns.
\item \textbf{Outcome model performance:} OOF $R^2$ or MAE on fresh draws.
\end{itemize}

\paragraph{Thresholds:}
\begin{itemize}
\item \textbf{Pass:} 95\% CI contains zero, $|$OrthoScore$| < 0.05$.
\item \textbf{Warn:} CI contains zero, but $|$OrthoScore$| \in [0.05, 0.10]$.
\item \textbf{Fail:} CI does not contain zero or $|$OrthoScore$| > 0.10$.
\end{itemize}

\paragraph{Fixes:}
\begin{itemize}
\item Improve the outcome model: add regularization, use a more flexible model (e.g., two-stage isotonic), or increase fresh draw sample size.
\item Revisit \simcal{} settings or overlap diagnostics (poor weights can break orthogonality).
\item Check cross-fitting: ensure folds are balanced and representative.
\item If orthogonality fails persistently, fall back to \ips{} or regenerate for \dm.
\end{itemize}

\subsection{Traffic-light summary and refusal gates}

CJE estimators can optionally enforce \textbf{refusal gates}: if a diagnostic crosses a critical threshold, the estimator returns \texttt{NaN} instead of a potentially misleading number.

\paragraph{Unified diagnostic thresholds.}

Table~\ref{tab:thresholds} summarizes all diagnostic thresholds in one place. Use this as the single source of truth for interpreting diagnostic outputs.

\begin{table}[h]
\centering
\caption{Unified diagnostic thresholds for CJE}
\label{tab:thresholds}
\begin{tabular}{lccc|c}
\toprule
\textbf{Diagnostic} & \textbf{PASS} & \textbf{WARN} & \textbf{FAIL} & \textbf{REFUSE} \\
\midrule
ESS / $n$ (post-\simcal) & $\ge 30\%$ & $[10\%, 30\%)$ & $< 10\%$ & $< 1\%$ \\
Hill index $\alpha$ & $\ge 3$ & $[2, 3)$ & $< 2$ & $< 1$ \\
Score coverage & $\ge 95\%$ & $[85\%, 95\%)$ & $< 85\%$ & โ \\
Calibration OOF MAE & $< 0.05$ & $[0.05, 0.10]$ & $> 0.10$ & โ \\
Mean preservation & $< 0.02$ & $[0.02, 0.05]$ & $> 0.05$ & โ \\
DR orthogonality $|$score$|$ & $< 0.05$ & $[0.05, 0.10]$ & $> 0.10$ & โ \\
\quad (CI must contain 0) & \checkmark & \checkmark & $\times$ & โ \\
OUA share & $< 20\%$ & $[20\%, 50\%]$ & $> 50\%$ & โ \\
Judge stability $\tau$ & $\ge 0.90$ & $[0.75, 0.90)$ & $< 0.75$ & โ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Default refusal gates:}
\begin{itemize}
\item ESS $< 1\%$ $\to$ refuse to estimate (off-policy only).
\item Hill index $\alpha < 2$ $\to$ refuse (off-policy only).
\item Coverage $< 50\%$ $\to$ warn strongly (all modes).
\end{itemize}

Operators can adjust these gates or disable them for exploratory analysis, but the default is to fail loudly rather than produce garbage.

\subsection{Diagnostic workflow (flowchart)}

\paragraph{Core diagnostics (all modes):}
\begin{enumerate}
\item \textbf{Check score coverage.} If $< 85\%$, add labels or narrow scope. If pass, continue.
\item \textbf{Check calibration reliability.} If regional error is high, enable two-stage \autocal{} or add labels. If pass, continue.
\item \textbf{Check judge stability.} If $\tau < 0.75$, freeze judge config and refresh oracle. If pass, continue.
\item \textbf{Check OUA share.} If $> 50\%$, add labels. If pass, you're done (for Direct Mode).
\end{enumerate}

\paragraph{Additional checks for off-policy (IPS/DR):}
\begin{enumerate}[resume]
\item \textbf{Check ESS.} If $< 30\%$ (WARN), review diagnostics and consider \dr. If $< 10\%$ (FAIL), use \simcal{} or switch to \dr. If $< 1\%$ (REFUSE), regenerate. If pass, continue.
\item \textbf{Check tail index.} If $\alpha < 2$, apply stricter \simcal{} or cohort restriction. If pass, continue.
\item \textbf{(DR only) Check orthogonality.} If CI excludes zero, improve outcome model or weights. If pass, trust the estimate.
\end{enumerate}

If any diagnostic fails and the fix is not immediately available, report the failure and do not ship the estimate.

\subsection{Visualization gallery (artifacts)}

For full audit trails, CJE produces:
\begin{itemize}
\item \textbf{Calibration curve:} $f(S)$ vs.\ $Y$ with OOF fit and mean-preservation line.
\item \textbf{Coverage histogram:} oracle $S$ vs.\ evaluation $S$ distributions.
\item \textbf{Weight distribution:} before/after \simcal, with ESS and Hill index annotations.
\item \textbf{Orthogonality residual plot:} $(R - \hat{g}(S))$ vs.\ $\tilde{w}$ with CI band.
\item \textbf{OUA variance decomposition:} stacked bar showing $\Var_{\text{main}}$ vs.\ $\Var_{\oua}$.
\end{itemize}

These plots are auto-generated by the CJE visualization module and can be included in reports or dashboards.

\subsection{Example: interpreting a diagnostic report}

\begin{lstlisting}
=== CJE Diagnostic Report ===
Mode: Calibrated DR
Estimator: stacked-dr
Target policy: gpt-4-mini
Baseline policy: gpt-3.5-turbo

== Core Diagnostics ==
[1] Score Coverage: 92% (PASS)
    Oracle S range: [2.1, 9.8]
    Eval S range: [1.9, 9.9]
    Boundary slopes: moderate

[2] Calibration Reliability: OOF MAE = 0.04 (PASS)
    Mean preservation: -0.01 (excellent)
    Regional MAE: low=0.03, mid=0.04, high=0.05 (balanced)

[3] Judge Stability: tau = 0.93 (PASS)
    Temporal batches: 5
    No drift detected across time periods

[4] OUA Share: 15% (PASS)
    Labels sufficient; variance dominated by prompt variation

== Off-Policy Diagnostics ==
[5] ESS: 342 / 1000 = 34% (WARN)
    Raw ESS: 18%
    Post-SIMCal ESS: 34%
    Recommendation: acceptable for DR; consider cohort restriction if CI too wide

[6] Tail Index: alpha = 2.8 (PASS)
    Max/median weight ratio: 12.3
    Tail behavior: well-behaved

[7] DR Orthogonality: 0.03 [-0.02, 0.08] (PASS)
    CI contains zero
    Outcome model OOF R^2: 0.67

Overall: PASS (with ESS warning)
Recommendation: Estimate is reliable. Consider adding fresh draws or cohort restriction for tighter CIs.
\end{lstlisting}

\subsection{Summary}

The six diagnostics are organized into two groups:

\textbf{Core diagnostics (all modes):} Coverage (ยง4.1), calibration reliability (ยง4.2), judge stability (ยง4.3), and OUA share (ยง4.4) apply to all evaluation modes. These catch fundamental validity issues like extrapolation, miscalibration, judge drift, and label scarcity.

\textbf{Off-policy diagnostics (IPS/DR only):} ESS (ยง4.5), tail heaviness (ยง4.6), and DR orthogonality (ยง4.7) only apply when reusing logged data. These diagnose overlap, weight behavior, and critic quality.

Each diagnostic produces a compact artifact, a traffic-light signal, and a concrete fix. Always inspect diagnostics before trusting estimates, and always report them alongside results.
