\section{Off-Policy Evaluation: Calibrated IPS and DR}

\subsection{What off-policy evaluation solves}

Off-policy evaluation answers: ``What KPI would we see if we deployed policy $\pi'$ instead of the logging policy $\pi_0$?'' It lets you assess multiple candidate policies by reusing a single judged log---no need to generate fresh outputs for each candidate. The core challenge is adjusting for the fact that the logged responses came from $\pi_0$, not from $\pi'$.

\begin{quickref}
\textbf{Off-policy in one minute (operator view)}
\begin{enumerate}
\item Collect a judged log under $\pi_0$: $(X_i, A_i, S_i)$ with logprobs for $\pi_0$ and candidate $\pi'$.
\item Calibrate scores to outcome-scale rewards: $R_i = f(S_i)$ via \autocal.
\item Compute importance weights: $W_i = \pi'(A_i \mid X_i) / \pi_0(A_i \mid X_i)$.
\item Stabilize weights with \simcal{} (monotone projection + variance cap).
\item Estimate value via \ips{} (weights only) or \dr{} (weights + critic).
\end{enumerate}
\end{quickref}

\subsection{When to use off-policy methods}

\begin{itemize}
\item You have a judged log from $\pi_0$ and want to assess multiple candidates $\pi'$ without regenerating.
\item You have per-sequence logprobs for both $\pi_0$ and candidate policies.
\item Your main risks are poor overlap (use diagnostics: ESS, tail index) and judge drift (check stability).
\item For \dr, you also need fresh draws to train a critic (outcome model).
\end{itemize}

\subsection{Calibrated IPS: reweight with stabilized weights}

The basic \ips{} estimator reweights calibrated rewards by the likelihood ratio:
\begin{equation}
\est{V}_{\text{IPS}}(\pi') = \frac{1}{n} \sum_{i=1}^n W_i \cdot R_i,
\end{equation}
where $W_i = \pi'(A_i \mid X_i) / \pi_0(A_i \mid X_i)$ and $R_i = f(S_i)$.

\paragraph{The variance problem.} When policies differ significantly, $W_i$ can have extreme values (heavy tails), leading to high variance and unstable estimates. A single large weight can dominate the sum.

\subsection{SIMCal: stabilize weights via monotone projection}

\textbf{\simcal{} (Score-Indexed Monotone Calibration)} stabilizes weights by projecting them onto monotone functions of the judge score $S$, then applying a variance cap:

\begin{enumerate}
\item \textbf{Fit three candidates} on a fold-out basis: baseline ($W$), monotone increasing ($W^\uparrow$), monotone decreasing ($W^\downarrow$).
\item \textbf{Blend} via cross-validated stacking to minimize variance.
\item \textbf{Variance cap:} if $\Var(W_{\text{blend}}) > \rho \cdot \Var(W)$, reproject to enforce $\Var \le \rho \cdot \Var(W)$.
\end{enumerate}

\textbf{Why it works:} Monotone projections weakly reduce variance by majorization, and the cap $\rho$ (default 0.95) ensures strict variance reduction. Calibration preserves $\E[W] = 1$, so the estimate remains unbiased.

\paragraph{Operator artifacts to check:}
\begin{itemize}
\item \textbf{ESS (Effective Sample Size):} $({\textstyle\sum} W)^2 / ({\textstyle\sum} W^2)$. Aim for ESS $\ge 10\%$ of $n$ (typical) or at least $\ge 1\%$ (critical).
\item \textbf{Tail heaviness:} Hill index $\alpha \ge 2$ (finite variance). If $\alpha < 2$, weights are catastrophic.
\item \textbf{Weight summary:} min, median, max, 95th percentile before/after \simcal.
\end{itemize}

\subsection{Calibrated DR: add a critic for robustness}

Doubly robust (DR) combines \ips{} with an outcome model (critic) to gain efficiency and robustness. The estimator is:
\begin{equation}
\est{V}_{\text{DR}}(\pi') = \frac{1}{n} \sum_{i=1}^n \left[ W_i \cdot (R_i - \hat{g}(X_i)) + \hat{g}(X_i) \right],
\end{equation}
where $\hat{g}(X_i)$ is a cross-fitted prediction of $R$ given $X$ (trained on fresh draws from $\pi'$).

\paragraph{Double robustness:} The estimate is consistent if \emph{either} the weights are correct \emph{or} the outcome model is correct (or both). When both are decent, DR achieves the efficiency bound.

\subsection{Inputs \& setup (off-policy)}

\begin{description}
\item[Judged log:] Responses from $\pi_0$ with judge scores $S$ and logprobs for $\pi_0$ and each candidate $\pi'$.
\item[Oracle slice:] A small random subsample with ground truth $Y$ for \autocal{} (same as DM).
\item[Fresh draws (DR only):] For each candidate $\pi'$, generate fresh responses on the same prompts to train the critic $\hat{g}(X)$.
\item[Judge:] Same fixed rubric/config; check stability over time (Kendall $\tau$ on an anchor set).
\end{description}

\subsection{Teacher forcing and fresh draws}

For \dr, the critic $\hat{g}(X)$ must predict what $\pi'$ would achieve. Train it on \textbf{fresh draws}: responses generated by $\pi'$ on the same prompts, with judge scores. This is called \textbf{teacher forcing}---forcing the model to generate its own outputs, not reusing $\pi_0$'s.

Without fresh draws, you cannot build a valid critic, and \ips{} is your only option.

\subsection{Estimator and inference}

With calibrated rewards $R_i = f(S_i)$ and stabilized weights $W_i$ (via \simcal), compute:
\begin{align}
\est{V}_{\text{IPS}}(\pi') &= \frac{1}{n} \sum_{i=1}^n W_i \cdot R_i, \\
\est{V}_{\text{DR}}(\pi') &= \frac{1}{n} \sum_{i=1}^n \left[ W_i \cdot (R_i - \hat{g}(X_i)) + \hat{g}(X_i) \right].
\end{align}

Standard errors are computed via influence functions (per-sample contributions), which account for calibration, cross-fitting, and oracle uncertainty (via \oua). CIs are formed using the normal approximation.

\subsection{Diagnostics (highest leverage) \& quick fixes}

\begin{enumerate}[label=(\alph*)]
\item \textbf{ESS (Effective Sample Size)} after \simcal.

\emph{Fix:} If ESS $< 10\%$, try cohort restriction (focus on prompts with better overlap), tune \simcal{} variance cap $\rho$, or switch to \dr{} with a stronger critic. If ESS $< 1\%$, the estimate is unreliable---consider regenerating or narrowing scope.

\item \textbf{Tail heaviness} (Hill index $\alpha$).

\emph{Fix:} If $\alpha < 2$, weights have infinite variance. Use \simcal{} aggressively, restrict to high-overlap cohorts, or switch to \dr. Check for provider temperature/frequency-penalty drift.

\item \textbf{Weight stability} (compare raw vs.\ \simcal-adjusted distributions).

\emph{Fix:} Large changes indicate strong reliance on monotonicity assumption (J2-M). Validate by checking that $\E[Y \mid S]$ and $\E[W \mid S]$ are indeed monotone on the oracle slice.

\item \textbf{DR orthogonality score} (empirical moment with CI).

\emph{Fix:} If the orthogonality CI does not contain zero, either the critic or the weights are poor. Improve critic regularization, add fresh draws, or revisit \simcal{} / overlap diagnostics.

\item \textbf{Oracle uncertainty share} (fraction of total variance from \oua).

\emph{Fix:} If large, add labels targeting uncovered $S$ regions or high-variance prompt families.
\end{enumerate}

\subsection{Reporting template (off-policy)}

For each candidate policy, report:

\begin{itemize}
\item Calibrated mean $\est{V}(\pi')$ with 95\% CI (\ips{} or \dr).
\item ESS (absolute and as \% of $n$), Hill index $\alpha$.
\item Weight summary: min, median, max, 95th percentile (raw and post-\simcal).
\item For \dr, orthogonality score with CI.
\item \oua{} share; note if oracle slice coverage is thin.
\item Any judge stability checks (e.g., Kendall $\tau$ on anchor set).
\end{itemize}

\subsection{Sample size \& label planner (off-policy)}

Let $\hat{\sigma}_W^2$ be the variance of $W_i \cdot R_i$ for \ips, or the variance of the DR influence function. Then a rough CI half-width is
\begin{equation}
\text{HalfWidth} \approx 1.96 \sqrt{\frac{\hat{\sigma}_W^2}{n} + \Var_{\oua}}.
\end{equation}

For \ips, more data helps only if ESS is not the bottleneck; if ESS is low, improving overlap (via cohort restriction or stronger \simcal) is more effective than adding samples. For \dr, more fresh draws improve the critic and tighten intervals.

\subsection{When to use IPS vs.\ DR}

\begin{itemize}
\item \textbf{Use \ips{}} when you cannot generate fresh draws, or when overlap is excellent (ESS $> 50\%$) and variance is low.
\item \textbf{Use \dr{}} when you can afford fresh draws and overlap is moderate to poor (ESS $< 50\%$). \dr{} is more robust and typically yields tighter CIs.
\item \textbf{Use stacked-DR} (ensemble of DR variants) when you want the best of all worlds: robustness, efficiency, and automatic selection among multiple critics.
\end{itemize}

\subsection{Common pitfalls (and how to avoid them)}

\begin{itemize}
\item \textbf{Ignoring ESS.} A single large weight can dominate. Always check ESS; if $< 10\%$, investigate or restrict scope.

\item \textbf{Assuming weights have finite variance.} Check the Hill index $\alpha$; if $< 2$, estimates are unstable. Use \simcal{} or cohort restriction.

\item \textbf{Reusing logged responses for the critic.} The critic must be trained on \emph{fresh draws} from $\pi'$, not on $\pi_0$'s responses. Otherwise \dr{} fails.

\item \textbf{Ignoring judge drift.} If the judge's scoring changes over time, scores from the log are not comparable to fresh scores. Check stability via an anchor set.

\item \textbf{Thin oracle coverage.} If labeled $S$ does not cover the range of evaluated $S$, \autocal{} extrapolates poorly. Target labels to uncovered bins or report the coverage badge.
\end{itemize}

\subsection{Minimal recipe (pseudocode: IPS)}

\begin{lstlisting}[language=Python,caption=Calibrated IPS Recipe]
# Inputs: judged log from pi_0 with (X, A, S, logprob_pi0, logprob_pi_prime)
#         oracle slice {(S, Y)}
# Output: calibrated mean, ESS, CI with OUA

# 1) Fit AutoCal-R on oracle (cross-fitted)
f = fit_autocal_r(oracle_S, oracle_Y, K=5)

# 2) Calibrate rewards and compute raw weights
for i in range(n):
    R[i] = f(S[i])
    W[i] = exp(logprob_pi_prime[i] - logprob_pi0[i])

# 3) Stabilize weights with SIMCal (OOF stacking + variance cap)
W_calibrated = simcal(W, S, rho=0.95, K=5)

# 4) Estimate value
V_hat_ips = np.mean(W_calibrated * R)

# 5) Compute ESS and diagnostics
ESS = (np.sum(W_calibrated)**2) / np.sum(W_calibrated**2)
alpha_hill = estimate_hill_index(W_calibrated)

# 6) OUA: refit AutoCal-R across K folds and add oracle variance
for k in range(K):
    f_k = fit_autocal_r(oracle_minus_fold_k)
    # ... recompute V_hat with f_k ...
SE_total_squared = SE_main**2 + Var_OUA

# 7) Report V_hat, 95% CI, ESS, Hill index, OUA share
\end{lstlisting}

\subsection{Minimal recipe (pseudocode: DR)}

\begin{lstlisting}[language=Python,caption=Calibrated DR Recipe]
# Inputs: judged log from pi_0, oracle slice, fresh draws from pi_prime
# Output: calibrated mean, orthogonality score, CI with OUA

# 1) Fit AutoCal-R on oracle
f = fit_autocal_r(oracle_S, oracle_Y, K=5)

# 2) Calibrate rewards and compute stabilized weights (as in IPS)
for i in range(n):
    R[i] = f(S[i])
    W[i] = exp(logprob_pi_prime[i] - logprob_pi0[i])
W_calibrated = simcal(W, S, rho=0.95, K=5)

# 3) Train critic g(X) on fresh draws (cross-fitted)
for k in range(K):
    train_prompts = fresh_draws_minus_fold_k
    g_k = fit_critic(train_prompts, R_fresh)
    # Predict on fold k:
    for i in fold_k:
        g_hat[i] = g_k(X[i])

# 4) Compute DR estimate
V_hat_dr = np.mean(W_calibrated * (R - g_hat) + g_hat)

# 5) Orthogonality check (empirical moment)
ortho_moment = np.mean((W_calibrated - 1) * (R - g_hat))
ortho_se = np.std((W_calibrated - 1) * (R - g_hat)) / np.sqrt(n)
ortho_ci = [ortho_moment - 1.96*ortho_se, ortho_moment + 1.96*ortho_se]

# 6) OUA and CI (as in IPS)
# ... refit AutoCal-R across folds ...
SE_total_squared = SE_main**2 + Var_OUA

# 7) Report V_hat_dr, orthogonality CI, ESS, OUA share
\end{lstlisting}

\subsection{Scope notes}

Off-policy evaluation assumes that the logging policy $\pi_0$ had positive probability of generating any response that $\pi'$ might generate (overlap, assumption D2). If policies are too different (e.g., different model families with non-overlapping support), off-policy methods fail. In such cases, generate fresh outputs and use \dm{} instead.

Judge stability is also critical: if the judge's meaning of a score changes between the time the log was collected and the time fresh draws are evaluated, calibration breaks. Always check drift via an anchor set with stable prompts.
