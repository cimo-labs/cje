\section{Off-Policy Evaluation: Calibrated IPS and DR}

\begin{mdframed}[linecolor=cjegray, backgroundcolor=white, linewidth=1pt]
\textbf{Note for Direct Mode users:} This section covers advanced off-policy methods for reusing logged data and answering counterfactual questions (``What if we deployed policy X?''). If you're just comparing policies on a fresh eval set using Direct Mode, you can \textbf{skip to ยง6 (Playbook)} or continue reading ยง4.1--4.4 (core diagnostics). Return here later if you need to reuse logged data without regenerating outputs.
\end{mdframed}

\subsection{What off-policy evaluation solves}

Off-policy evaluation answers: ``What KPI would we see if we deployed policy $\pi'$ instead of the logging policy $\pi_0$?'' It lets you assess multiple candidate policies by reusing a single judged log---no need to generate fresh outputs for each candidate. The core challenge is adjusting for the fact that the logged responses came from $\pi_0$, not from $\pi'$.

\begin{quickref}
\textbf{Off-policy in one minute (operator view)}
\begin{enumerate}
\item Collect a judged log under $\pi_0$: $(X_i, A_i, S_i)$ with logprobs for $\pi_0$ and candidate $\pi'$.
\item Calibrate scores to outcome-scale rewards: $R_i = f(S_i)$ via \autocal.
\item Compute importance weights: $W_i = \pi'(A_i \mid X_i) / \pi_0(A_i \mid X_i)$.
\item Stabilize weights with \simcal{} (monotone projection + variance cap).
\item Estimate value via \ips{} (weights only) or \dr{} (weights + critic).
\end{enumerate}
\end{quickref}

\subsection{When to use off-policy methods}

\begin{itemize}
\item You have a judged log from $\pi_0$ and want to assess multiple candidates $\pi'$ without regenerating.
\item You have per-sequence logprobs for both $\pi_0$ and candidate policies.
\item Your main risks are poor overlap (use diagnostics: ESS, tail index) and judge drift (check stability).
\item For \dr, you also need fresh draws to train a critic (outcome model).
\end{itemize}

\subsection{Calibrated IPS: reweight with stabilized weights}

The basic \ips{} estimator reweights calibrated rewards by the likelihood ratio:
\begin{equation}
\est{V}_{\text{IPS}}(\pi') = \frac{1}{n} \sum_{i=1}^n W_i \cdot R_i,
\end{equation}
where $W_i = \pi'(A_i \mid X_i) / \pi_0(A_i \mid X_i)$ and $R_i = f(S_i)$.

\paragraph{The variance problem.} When policies differ significantly, $W_i$ can have extreme values (heavy tails), leading to high variance and unstable estimates. A single large weight can dominate the sum.

\subsection{SIMCal: stabilize weights via monotone projection}

\textbf{\simcal{} (Score-Indexed Monotone Calibration)} stabilizes weights by projecting them onto monotone functions of the judge score $S$, then applying a variance cap:

\begin{enumerate}
\item \textbf{Fit three candidates} on a fold-out basis: baseline ($W$), monotone increasing ($W^\uparrow$), monotone decreasing ($W^\downarrow$).
\item \textbf{Blend} via cross-validated stacking to minimize variance.
\item \textbf{Variance cap:} if $\Var(W_{\text{blend}}) > \rho \cdot \Var(W)$, reproject to enforce $\Var \le \rho \cdot \Var(W)$.
\end{enumerate}

\textbf{Why it works:} Monotone projections weakly reduce variance by majorization, and the cap $\rho$ (default 0.95) ensures strict variance reduction. \simcal{} explicitly normalizes weights to mean one and projects them as a monotone function of $S$ out-of-fold. Under our J2-M assumption (score-indexed sufficiency), this projection reduces variance and \emph{approximately} preserves the target; we surface trimming/cohort sensitivity and orthogonality to detect residual bias.

\paragraph{Operator artifacts to check:}
\begin{itemize}
\item \textbf{ESS (Effective Sample Size):} $({\textstyle\sum} \tilde{w})^2 / ({\textstyle\sum} \tilde{w}^2)$, where $\tilde{w}$ are post-\simcal{} weights. Aim for ESS $\ge 30\%$ of $n$ (PASS); 10--30\% is acceptable (WARN); $< 10\%$ is poor (FAIL); $< 1\%$ is catastrophic (REFUSE).
\item \textbf{Tail heaviness:} Hill index $\alpha \ge 2$ (finite variance). If $\alpha < 2$, weights are catastrophic.
\item \textbf{Weight summary:} min, median, max, 95th percentile before/after \simcal.
\end{itemize}

\subsection{Calibrated DR: add an outcome model for robustness}

Doubly robust (DR) combines \ips{} with an outcome model to gain efficiency and robustness. CJE supports two DR implementations, both valid under different modeling assumptions.

\paragraph{Two DR implementations.}

\emph{(A) Score-only AIPW (lightweight, default).} Fit $\hat{g}(S) \approx \E[R \mid S]$ on fresh draws from $\pi'$ using the judge score $S$ only. Then:
\begin{equation}
\est{V}_{\text{DR-S}}(\pi') = \frac{1}{n} \sum_{i=1}^n \left[ \tilde{w}_i \cdot (R_i - \hat{g}(S_i)) + \hat{g}(S_i) \right],
\end{equation}
where $\tilde{w}_i$ are the stabilized, mean-one weights after \simcal, and $\hat{g}$ is typically a monotone function of $S$ (isotonic regression). This is the default in CJE: low cost, works well when $S$ is a strong predictor.

\emph{(B) Action-conditioned DR (canonical, research).} Fit $\hat{q}(X,A) \approx \E[R \mid X, A]$ using prompt features $X$ and response $A$, then compute $\hat{g}_{\pi'}(X) = \E_{A \sim \pi'(\ \cdot \mid X)} \hat{q}(X, A)$ via Monte Carlo rollouts or analytic evaluation. Then:
\begin{equation}
\est{V}_{\text{DR}}(\pi') = \frac{1}{n} \sum_{i=1}^n \left[ \hat{g}_{\pi'}(X_i) + \tilde{w}_i \cdot (R_i - \hat{q}(X_i, A_{0i})) \right].
\end{equation}
This is the canonical DR estimator from the semiparametric literature; use it when you have rich features or need maximum robustness (e.g., when $S$ alone is insufficient).

\textbf{When to use which:}
\begin{itemize}
\item \textbf{DR-S (score-only):} When judge scores are informative and you want low implementation cost. Default for most LLM evaluations.
\item \textbf{DR (action-conditioned):} When you have structured prompt features (length, domain, difficulty), or when $S$ is a weak predictor and you need the full robustness guarantee.
\end{itemize}

\paragraph{Double robustness guarantee:} Both estimators are consistent if \emph{either} the stabilized weights $\tilde{w}(\pi')$ converge to the true density ratios (up to mean-one normalization) \emph{or} the outcome model ($\hat{g}$ or $\hat{q}$) is correctly specified; if both are good, DR attains the semiparametric efficiency bound.

\paragraph{Why isotonic regression for $\hat{g}(S)$?} The default outcome model uses isotonic regression because it enforces exactly the right structural prior: \emph{higher judge score $\Rightarrow$ no worse expected reward}. This minimal monotonicity assumption avoids misspecification risk from rigid parametric forms (sigmoid, beta), provides mean preservation by construction (critical for unbiased DR), and achieves strong stability with few fresh draws (5-10\% coverage often sufficient). Isotonic's step-function output also makes edge fragility visible in diagnostics, enabling targeted label collection. When monotonicity fails (e.g., systematic length bias at fixed $S$), \autocal{}'s two-stage variant learns a smooth transformation first.

\subsection{Inputs \& setup (off-policy)}

\begin{description}
\item[Judged log:] Responses from $\pi_0$ with judge scores $S$ and logprobs for $\pi_0$ and each candidate $\pi'$.
\item[Oracle slice:] A small random subsample with ground truth $Y$ for \autocal{} (same as DM).
\item[Fresh draws (DR only):] For each candidate $\pi'$, generate fresh responses on the same prompts to train the critic $\hat{g}(X)$.
\item[Judge:] Same fixed rubric/config; check stability over time (Kendall $\tau$ on an anchor set).
\end{description}

\subsection{Fresh draws for outcome modeling}

For \dr, the outcome model $\hat{g}(X)$ must predict what $\pi'$ would achieve. Train it on \textbf{fresh draws}: responses generated by $\pi'$ on the same prompts, with judge scores.

Without fresh draws, you cannot build a valid outcome model, and \ips{} is your only option.

\subsection{Estimator and inference}

With calibrated rewards $R_i = f(S_i)$ and stabilized, mean-one weights $\tilde{w}_i$ (via \simcal), compute:
\begin{align}
\est{V}_{\text{IPS}}(\pi') &= \frac{1}{n} \sum_{i=1}^n \tilde{w}_i \cdot R_i, \\
\est{V}_{\text{DR-S}}(\pi') &= \frac{1}{n} \sum_{i=1}^n \left[ \tilde{w}_i \cdot (R_i - \hat{g}(S_i)) + \hat{g}(S_i) \right].
\end{align}

Standard errors are computed via influence functions (per-sample contributions), which account for calibration, cross-fitting, and oracle uncertainty (via \oua). CIs are formed using the normal approximation.

\subsection{Diagnostics (highest leverage) \& quick fixes}

\begin{enumerate}[label=(\alph*)]
\item \textbf{ESS (Effective Sample Size)} after \simcal.

\emph{Fix:} If ESS $< 30\%$ (WARN), try cohort restriction (focus on prompts with better overlap) or tune \simcal{} variance cap $\rho$. If ESS $< 10\%$ (FAIL), switch to \dr{} with a stronger outcome model. If ESS $< 1\%$ (REFUSE), the estimate is unreliable---regenerate or narrow scope.

\item \textbf{Tail heaviness} (Hill index $\alpha$).

\emph{Fix:} If $\alpha < 2$, weights have infinite variance. Use \simcal{} aggressively, restrict to high-overlap cohorts, or switch to \dr. Check for provider temperature/frequency-penalty drift.

\item \textbf{Weight stability} (compare raw vs.\ \simcal-adjusted distributions).

\emph{Fix:} Large changes indicate strong reliance on monotonicity assumption (J2-M). Validate by checking that $\E[Y \mid S]$ and $\E[W \mid S]$ are indeed monotone on the oracle slice.

\item \textbf{DR orthogonality score} (empirical moment with CI).

\emph{Fix:} If the orthogonality CI does not contain zero, either the outcome model or the weights are poor. Improve outcome model regularization, add fresh draws, or revisit \simcal{} / overlap diagnostics.

\item \textbf{Oracle uncertainty share} (fraction of total variance from \oua).

\emph{Fix:} If large, add labels targeting uncovered $S$ regions or high-variance prompt families.
\end{enumerate}

\subsection{Reporting template (off-policy)}

For each candidate policy, report:

\begin{itemize}
\item Calibrated mean $\est{V}(\pi')$ with 95\% CI (\ips{} or \dr).
\item ESS (absolute and as \% of $n$), Hill index $\alpha$.
\item Weight summary: min, median, max, 95th percentile (raw and post-\simcal).
\item For \dr, orthogonality score with CI.
\item \oua{} share; note if oracle slice coverage is thin.
\item Any judge stability checks (e.g., Kendall $\tau$ on anchor set).
\end{itemize}

\subsection{Sample size \& label planner (off-policy)}

Let $\hat{\sigma}_W^2$ be the variance of $W_i \cdot R_i$ for \ips, or the variance of the DR influence function. Then a rough CI half-width is
\begin{equation}
\text{HalfWidth} \approx 1.96 \sqrt{\frac{\hat{\sigma}_W^2}{n} + \Var_{\oua}}.
\end{equation}

For \ips, more data helps only if ESS is not the bottleneck; if ESS is low, improving overlap (via cohort restriction or stronger \simcal) is more effective than adding samples. For \dr, more fresh draws improve the outcome model and tighten intervals.

\subsection{When to use IPS vs.\ DR}

\begin{itemize}
\item \textbf{Use \ips{}} when you cannot generate fresh draws, or when overlap is excellent (ESS $\ge 30\%$, PASS) and variance is low.
\item \textbf{Use \dr{}} when you can afford fresh draws and overlap is moderate to poor (ESS $< 30\%$, WARN/FAIL). \dr{} is more robust and typically yields tighter CIs.
\item \textbf{Use stacked-DR} (ensemble of DR variants) when you want the best of all worlds: robustness, efficiency, and automatic selection among multiple outcome models.
\end{itemize}

\subsection{Common pitfalls (and how to avoid them)}

\begin{itemize}
\item \textbf{Ignoring ESS.} A single large weight can dominate. Always check ESS; if $< 30\%$ (WARN), investigate via diagnostics; if $< 10\%$ (FAIL), restrict scope or switch to \dr.

\item \textbf{Assuming weights have finite variance.} Check the Hill index $\alpha$; if $< 2$, estimates are unstable. Use \simcal{} or cohort restriction.

\item \textbf{Reusing logged responses for the outcome model.} The outcome model must be trained on \emph{fresh draws} from $\pi'$, not on $\pi_0$'s responses. Otherwise \dr{} fails.

\item \textbf{Ignoring judge drift.} If the judge's scoring changes over time, scores from the log are not comparable to fresh scores. Check stability via an anchor set.

\item \textbf{Thin oracle coverage.} If labeled $S$ does not cover the range of evaluated $S$, \autocal{} extrapolates poorly. Target labels to uncovered bins or report the coverage badge.
\end{itemize}

\subsection{Minimal recipe (pseudocode: IPS)}

\begin{lstlisting}[language=Python,caption=Calibrated IPS Recipe]
# Inputs: judged log from pi_0 with (X, A, S, logprob_pi0, logprob_pi_prime)
#         oracle slice {(S, Y)}
# Output: calibrated mean, ESS, CI with OUA

# 1) Fit AutoCal-R on oracle (cross-fitted)
f = fit_autocal_r(oracle_S, oracle_Y, K=5)

# 2) Calibrate rewards and compute raw weights
for i in range(n):
    R[i] = f(S[i])
    W[i] = exp(logprob_pi_prime[i] - logprob_pi0[i])

# 3) Stabilize weights with SIMCal (OOF stacking + variance cap)
W_calibrated = simcal(W, S, rho=0.95, K=5)

# 4) Estimate value
V_hat_ips = np.mean(W_calibrated * R)

# 5) Compute ESS and diagnostics
ESS = (np.sum(W_calibrated)**2) / np.sum(W_calibrated**2)
alpha_hill = estimate_hill_index(W_calibrated)

# 6) OUA: refit AutoCal-R across K folds and add oracle variance
for k in range(K):
    f_k = fit_autocal_r(oracle_minus_fold_k)
    # ... recompute V_hat with f_k ...
SE_total_squared = SE_main**2 + Var_OUA

# 7) Report V_hat, 95% CI, ESS, Hill index, OUA share
\end{lstlisting}

\subsection{Minimal recipe (pseudocode: DR)}

\begin{lstlisting}[language=Python,caption=Calibrated DR Recipe]
# Inputs: judged log from pi_0, oracle slice, fresh draws from pi_prime
# Output: calibrated mean, orthogonality score, CI with OUA

# 1) Fit AutoCal-R on oracle
f = fit_autocal_r(oracle_S, oracle_Y, K=5)

# 2) Calibrate rewards and compute stabilized weights (as in IPS)
for i in range(n):
    R[i] = f(S[i])
    W[i] = exp(logprob_pi_prime[i] - logprob_pi0[i])
W_calibrated = simcal(W, S, rho=0.95, K=5)

# 3) Train outcome model g(S) on fresh draws (cross-fitted)
for k in range(K):
    train_scores = fresh_draws_scores_minus_fold_k
    g_k = fit_outcome_model(train_scores, R_fresh)  # isotonic regression
    # Predict on fold k:
    for i in fold_k:
        g_hat[i] = g_k(S[i])

# 4) Compute DR estimate
V_hat_dr = np.mean(W_calibrated * (R - g_hat) + g_hat)

# 5) Orthogonality check (empirical moment: E[W * (R - g_hat)])
ortho_moment = np.mean(W_calibrated * (R - g_hat))
ortho_se = np.std(W_calibrated * (R - g_hat)) / np.sqrt(n)
ortho_ci = [ortho_moment - 1.96*ortho_se, ortho_moment + 1.96*ortho_se]

# 6) OUA and CI (as in IPS)
# ... refit AutoCal-R across folds ...
SE_total_squared = SE_main**2 + Var_OUA

# 7) Report V_hat_dr, orthogonality CI, ESS, OUA share
\end{lstlisting}

\subsection{Scope notes}

Off-policy evaluation assumes that the logging policy $\pi_0$ had positive probability of generating any response that $\pi'$ might generate (overlap, assumption D2). If policies are too different (e.g., different model families with non-overlapping support), off-policy methods fail. In such cases, generate fresh outputs and use \dm{} instead.

Judge stability is also critical: if the judge's meaning of a score changes between the time the log was collected and the time fresh draws are evaluated, calibration breaks. Always check drift via an anchor set with stable prompts.
