\section{Operator Playbook: Start to Finish}

\subsection{Overview}

This section is a step-by-step guide for practitioners. It covers the full evaluation workflow from data collection to reporting, with decision trees, checklists, and troubleshooting tips.

\subsection{Decision tree: which mode to use?}

\begin{enumerate}
\item \textbf{Can you generate fresh outputs for each candidate policy?}
   \begin{itemize}
   \item \textbf{Yes} $\to$ Can you afford to label an oracle slice (50--200 examples)?
      \begin{itemize}
      \item \textbf{Yes} $\to$ Use \textbf{\dm} (on-policy, simplest). If you also have logged data with logprobs, consider \textbf{\dr} for tighter CIs.
      \item \textbf{No} $\to$ Use \textbf{Direct mode without calibration} (raw judge scores, rank-only). No KPI-scale estimates, but still useful for ranking.
      \end{itemize}
   \item \textbf{No} $\to$ Do you have a judged log with logprobs for $\pi_0$ and candidates $\pi'$?
      \begin{itemize}
      \item \textbf{Yes} $\to$ Can you label an oracle slice?
         \begin{itemize}
         \item \textbf{Yes} $\to$ Use \textbf{\ips} or \textbf{\dr} (if you can generate a few fresh draws for a critic). Check ESS; if $< 10\%$, \dr{} is strongly recommended.
         \item \textbf{No} $\to$ Use \textbf{IPS without calibration} (relative comparisons only, no KPI scale).
         \end{itemize}
      \item \textbf{No} $\to$ You cannot perform causal off-policy evaluation. Regenerate or use \dm.
      \end{itemize}
   \end{itemize}
\end{enumerate}

\subsection{Workflow 1: Direct Modeling (DM)}

\paragraph{Step 1: Design the evaluation set.}
\begin{itemize}
\item Choose $m$ prompts representative of deployment (e.g., 500--2000).
\item Ensure diversity: cover prompt families, lengths, difficulty levels.
\item Use stratified sampling if you have subgroups of interest.
\end{itemize}

\paragraph{Step 2: Generate outputs.}
\begin{itemize}
\item For each policy $\pi$, generate one output per prompt.
\item Use the same random seed (or paired draws) across policies for variance reduction.
\item Log all outputs, prompts, and metadata (model version, temperature, etc.).
\end{itemize}

\paragraph{Step 3: Judge all outputs.}
\begin{itemize}
\item Apply the judge to all $(X_i, A_{\pi i})$ pairs to get scores $S_{\pi i}$.
\item Use a fixed judge config (version, prompt, temperature).
\item Log judge metadata (timestamps, version, config).
\end{itemize}

\paragraph{Step 4: Label an oracle slice.}
\begin{itemize}
\item Sample 50--200 examples uniformly at random from the evaluation set.
\item Obtain ground-truth labels $Y$ (human ratings, gold KPI, etc.).
\item Ensure labels are unbiased and representative.
\end{itemize}

\paragraph{Step 4b (Optional): Use a dedicated calibration set.}
\begin{itemize}
\item If you have pre-existing curated oracle labels (e.g., from prior evaluation work), load them via \texttt{calibration\_data\_path} instead of sampling from the evaluation set.
\item CJE will auto-combine oracle labels from the calibration set with any labels in the evaluation data for maximum efficiency (priority: calibration $>$ fresh $>$ logged).
\item Inspect \texttt{results.metadata["oracle\_sources"]} for source breakdown and conflicts.
\end{itemize}

\paragraph{Step 5: Fit AutoCal-R.}
\begin{itemize}
\item Use 5-fold cross-fitting to learn $f: S \to [0, 1]$.
\item Check diagnostics: coverage (aim for $\ge 95\%$), reliability (OOF MAE $< 0.05$), mean preservation ($|\bar{f(S)} - \bar{Y}| < 0.02$).
\item If regional error is high, enable two-stage \autocal{} or add a coarse index.
\end{itemize}

\paragraph{Step 6: Compute estimates and CIs.}
\begin{itemize}
\item Calibrate rewards: $R_{\pi i} = f(S_{\pi i})$.
\item Compute $\est{V}_{\dm}(\pi) = \frac{1}{m} \sum_i R_{\pi i}$.
\item For pairwise contrasts, compute $\est{\Delta}(\pi, \pi') = \frac{1}{m} \sum_i (R_{\pi i} - R_{\pi' i})$ (paired SE).
\item Add \oua{} by refitting $f$ across oracle folds and computing $\Var_{\oua}$.
\item Report 95\% CIs: $\est{V} \pm 1.96 \cdot \SE_{\text{total}}$.
\end{itemize}

\paragraph{Step 7: Report.}
\begin{itemize}
\item For each policy: $\est{V}$ with CI, \oua{} share, calibration diagnostics, coverage badge.
\item For key contrasts: $\est{\Delta}$ with paired CI.
\item Include diagnostic plots (calibration curve, coverage histogram).
\end{itemize}

\subsection{Workflow 2: Off-Policy IPS}

\paragraph{Step 1: Collect a judged log.}
\begin{itemize}
\item Deploy $\pi_0$ and log $(X_i, A_i, S_i)$ for $n$ prompts.
\item Store logprobs: $\log \pi_0(A_i \mid X_i)$ and $\log \pi'(A_i \mid X_i)$ for each candidate $\pi'$.
\end{itemize}

\paragraph{Step 2: Label an oracle slice.}
\begin{itemize}
\item Sample 50--200 examples uniformly from the logged data.
\item Obtain ground-truth $Y$.
\end{itemize}

\paragraph{Step 2b (Optional): Use a dedicated calibration set.}
\begin{itemize}
\item If you have pre-existing curated oracle labels, load them via \texttt{calibration\_data\_path}.
\item CJE will auto-combine with logged data oracle labels (priority: calibration $>$ fresh $>$ logged).
\item Check \texttt{results.metadata["oracle\_sources"]} for distribution mismatch and temporal staleness warnings.
\end{itemize}

\paragraph{Step 3: Fit AutoCal-R.}
\begin{itemize}
\item Same as DM: 5-fold cross-fitting, check diagnostics.
\end{itemize}

\paragraph{Step 4: Compute and stabilize weights.}
\begin{itemize}
\item Raw weights: $W_i = \exp(\log \pi'(A_i \mid X_i) - \log \pi_0(A_i \mid X_i))$.
\item Apply \simcal{} (5-fold, variance cap $\rho = 0.95$) to get $W_i^{\text{stab}}$.
\item Check ESS: $({\textstyle\sum} W^{\text{stab}})^2 / ({\textstyle\sum} (W^{\text{stab}})^2)$. Aim for $\ge 10\%$.
\item Check Hill index $\alpha \ge 2$.
\end{itemize}

\paragraph{Step 5: Compute estimates and CIs.}
\begin{itemize}
\item Calibrate rewards: $R_i = f(S_i)$.
\item Compute $\est{V}_{\ips}(\pi') = \frac{1}{n} \sum_i W_i^{\text{stab}} \cdot R_i$.
\item Compute SE via influence functions, add \oua, form 95\% CI.
\end{itemize}

\paragraph{Step 6: Report.}
\begin{itemize}
\item $\est{V}$ with CI, ESS (absolute and \%), Hill index, weight summary (min, median, max, 95th pct).
\item \oua{} share, calibration diagnostics, judge stability check (if available).
\end{itemize}

\subsection{Workflow 3: Off-Policy DR}

\paragraph{Steps 1--4: Same as IPS.}

\paragraph{Step 5: Generate fresh draws for the critic.}
\begin{itemize}
\item For each candidate $\pi'$, generate fresh outputs on the same prompts used in the log.
\item Judge these fresh outputs to get $S'_{\pi i}$ and calibrate to $R'_{\pi i} = f(S'_{\pi i})$.
\item This is your training set for the outcome model $\hat{g}(X)$.
\end{itemize}

\paragraph{Step 6: Train the critic (cross-fitted).}
\begin{itemize}
\item Split fresh draws into 5 folds.
\item For each fold $k$, train $\hat{g}_k(X)$ on the other 4 folds to predict $R'_{\pi}$.
\item Predict on fold $k$ to get $\hat{g}(X_i)$ for logged prompts $X_i$.
\item Use a flexible model (e.g., gradient-boosted trees, neural net) with regularization.
\end{itemize}

\paragraph{Step 7: Compute DR estimate.}
\begin{itemize}
\item $\est{V}_{\dr}(\pi') = \frac{1}{n} \sum_i \left[ W_i^{\text{stab}} \cdot (R_i - \hat{g}(X_i)) + \hat{g}(X_i) \right]$.
\item Compute orthogonality score: $\frac{1}{n} \sum_i (W_i^{\text{stab}} - 1) \cdot (R_i - \hat{g}(X_i))$ with CI.
\item If CI excludes zero, improve the critic or weights.
\end{itemize}

\paragraph{Step 8: Report.}
\begin{itemize}
\item $\est{V}$ with CI, ESS, Hill index, weight summary.
\item Orthogonality score with CI, critic OOF $R^2$.
\item \oua{} share, calibration diagnostics.
\end{itemize}

\subsection{Checklist: before you ship an estimate}

\begin{enumerate}
\item[$\square$] Checked score coverage ($\ge 85\%$)?
\item[$\square$] Checked calibration reliability (OOF MAE $< 0.10$, mean preservation OK)?
\item[$\square$] (Off-policy) Checked ESS ($\ge 10\%$) and Hill index ($\alpha \ge 2$)?
\item[$\square$] (DR) Checked orthogonality (CI contains zero)?
\item[$\square$] Checked \oua{} share (if $> 50\%$, consider adding labels)?
\item[$\square$] Checked judge stability (if using old logs)?
\item[$\square$] Reported CIs that include \oua?
\item[$\square$] Included diagnostic plots and summary in the report?
\item[$\square$] Documented judge config, prompt set, oracle sampling procedure?
\item[$\square$] Ran sensitivity checks (cohort restriction, trimming, alternative calibrators)?
\end{enumerate}

If any box is unchecked, do not ship the estimate.

\subsection{Troubleshooting guide}

\paragraph{Problem: Low ESS ($< 10\%$).}
\begin{itemize}
\item \textbf{Cause:} Poor overlap; policies are too different.
\item \textbf{Fixes:} (1) Apply \simcal{} with stricter $\rho$ (e.g., 0.90). (2) Restrict to high-overlap cohort ($|\log W| < 2$). (3) Switch to \dr{} with a strong critic. (4) If ESS $< 1\%$, regenerate and use \dm.
\end{itemize}

\paragraph{Problem: Heavy tails ($\alpha < 2$).}
\begin{itemize}
\item \textbf{Cause:} Extreme weights due to rare events or provider drift.
\item \textbf{Fixes:} (1) Use \simcal{} aggressively. (2) Cohort restriction. (3) Check for provider temperature/frequency-penalty changes. (4) Switch to \dr{} or regenerate.
\end{itemize}

\paragraph{Problem: Poor calibration (OOF MAE $> 0.10$).}
\begin{itemize}
\item \textbf{Cause:} Judge scores are not monotone in outcome, or regional miscalibration.
\item \textbf{Fixes:} (1) Enable two-stage \autocal. (2) Add a coarse index (prompt family, length). (3) Add 10--30 labels in problematic $S$ regions. (4) Check for judge drift.
\end{itemize}

\paragraph{Problem: Thin coverage ($< 85\%$).}
\begin{itemize}
\item \textbf{Cause:} Oracle slice does not span the evaluation $S$-range.
\item \textbf{Fixes:} (1) Add 5--20 labels targeting uncovered $S$ bins. (2) Narrow the prompt set to the intended deployment slice. (3) If stress-testing edges, ensure oracle includes edge examples.
\end{itemize}

\paragraph{Problem: DR orthogonality CI excludes zero.}
\begin{itemize}
\item \textbf{Cause:} Critic or weights are poor.
\item \textbf{Fixes:} (1) Improve critic: add regularization, use a more flexible model, increase fresh draw sample size. (2) Revisit \simcal{} or overlap diagnostics. (3) Check cross-fitting folds. (4) Fall back to \ips{} if orthogonality fails persistently.
\end{itemize}

\paragraph{Problem: Large \oua{} share ($> 50\%$).}
\begin{itemize}
\item \textbf{Cause:} Label scarcity is the bottleneck, not prompt scarcity.
\item \textbf{Fixes:} (1) Add labels, prioritizing sparse $S$ regions or high-variance prompt families. (2) Adding more prompts yields diminishing returns; focus labeling budget on coverage and balance.
\end{itemize}

\subsection{Sample size planning}

\paragraph{Goal:} Achieve a target CI half-width $\delta$ (e.g., $\delta = 0.02$ for $\pm 2\%$ precision).

\paragraph{Prompt sample size (for DM or DR with good critic):}
\begin{equation}
m \approx \left( \frac{1.96 \cdot \sigma_R}{\delta} \right)^2,
\end{equation}
where $\sigma_R$ is the SD of calibrated rewards $R$ (estimate from a pilot).

\paragraph{Label sample size (oracle):}
The \oua{} variance scales as $1 / n_{\text{oracle}}$. For \oua{} share $< 20\%$, use:
\begin{equation}
n_{\text{oracle}} \approx 5 \cdot m^{1/2}.
\end{equation}
For $m = 1000$, this gives $n_{\text{oracle}} \approx 150$.

\paragraph{Fresh draws for DR critic:}
Use the same $m$ prompts as the logged data. More is better; diminishing returns after $m \approx 500$.

\subsection{Reporting template}

\begin{lstlisting}
=== CJE Evaluation Report ===
Date: 2025-10-08
Evaluator: Alice
Mode: Calibrated DR
Estimator: stacked-dr

## Policies Evaluated
- Baseline (pi_0): gpt-3.5-turbo (deployed 2025-09-01 to 2025-10-01)
- Candidate (pi'): gpt-4-mini (under consideration)

## Data
- Logged prompts: n = 1,000
- Oracle slice: 150 (15% of total)
- Fresh draws (for critic): 1,000 (same prompts)
- Judge: GPT-4-as-judge with rubric v2.3 (frozen)
- Outcome: Binary pass rate (Y in {0, 1})

## Estimates
pi_0 (baseline): 0.72 [0.69, 0.75] (95% CI)
pi' (gpt-4-mini): 0.81 [0.78, 0.84] (95% CI)
Delta (pi' - pi_0): +0.09 [0.05, 0.13] (95% CI, p < 0.001)

## Diagnostics
[1] Score Coverage: 94% (PASS)
[2] Calibration Reliability: OOF MAE = 0.04 (PASS)
[3] ESS: 34% (WARN, acceptable for DR)
[4] Hill Index: alpha = 2.8 (PASS)
[5] DR Orthogonality: 0.03 [-0.02, 0.08] (PASS, CI contains 0)
[6] OUA Share: 15% (PASS)

## Interpretation
Switching from gpt-3.5-turbo to gpt-4-mini is estimated to increase
the pass rate by 9 percentage points (95% CI: [5%, 13%]). The estimate
is reliable: all diagnostics pass, ESS is moderate (acceptable for DR),
and orthogonality is good.

## Recommendation
Proceed with deployment of gpt-4-mini. Expected uplift is substantial
and statistically significant.

## Attachments
- calibration_curve.pdf
- weight_distribution.pdf
- orthogonality_residuals.pdf
- data_and_code.zip (for reproducibility)
\end{lstlisting}

\subsection{Summary}

The operator playbook provides decision trees, workflows, checklists, and troubleshooting for the full CJE pipeline. Always check diagnostics before shipping, document everything, and report CIs with \oua. When in doubt, run sensitivity checks.
