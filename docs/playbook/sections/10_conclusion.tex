\section{Conclusion}

\subsection{Summary}

Causal Judge Evaluation (CJE) provides a principled, practitioner-first framework for turning LLM-as-judge scores into causally interpretable estimates with honest confidence intervals. This playbook has covered:

\begin{itemize}
\item \textbf{The problem:} Naive averaging of judge scores is a heuristic that breaks in predictable ways (wrong scale, hidden drift, no uncertainty, off-policy illusion).

\item \textbf{The solution:} Three analysis modes---\dm{} (on-policy), Calibrated \ips{} (off-policy with weights), and Calibrated \dr{} (off-policy with critic)---each tailored to common evaluation workflows.

\item \textbf{The core methods:} \autocal{} to map scores to outcomes, \simcal{} to stabilize weights, and \oua{} to keep CIs honest.

\item \textbf{The diagnostics:} Five high-leverage checks (coverage, reliability, ESS, tail index, orthogonality) that catch the most important failure modes and point to concrete fixes.

\item \textbf{The assumptions:} SUTVA, overlap, oracle randomness, and judge monotone sufficiency, all checkable via diagnostics.

\item \textbf{The playbook:} Decision trees, workflows, checklists, troubleshooting, and reporting templates for operators.

\item \textbf{The evidence:} Case studies showing DM, IPS, and DR in action, with realistic numbers and sensitivity checks.

\item \textbf{The limitations:} Judge quality, monotonicity, overlap, drift, and small oracle slices are the main constraints; most are addressable with targeted remediations.
\end{itemize}

\subsection{When to use CJE}

Use CJE when you need to:
\begin{itemize}
\item Rank candidate policies on a shared prompt set with statistical rigor.
\item Estimate policy values on a meaningful KPI scale (e.g., pass rate, satisfaction) with confidence intervals.
\item Reuse judged logs to assess multiple candidates without regenerating (off-policy evaluation).
\item Combine logged data with fresh draws for tighter CIs and robustness (\dr).
\item Audit and document evaluation assumptions, diagnostics, and sensitivity checks.
\end{itemize}

\subsection{What CJE is not}

CJE is not:
\begin{itemize}
\item A replacement for A/B testing. Use CJE for rapid offline iteration; validate winners with online tests when stakes are high.
\item A fix for poor judges. If the judge is biased or unreliable, calibration cannot save you.
\item A method for sequential or adaptive settings (multi-turn conversations with context dependence). Extensions are possible but beyond this playbook.
\item A universal solution. When assumptions fail and cannot be fixed, CJE may not apply. In such cases, report limitations transparently.
\end{itemize}

\subsection{Best practices}

\begin{enumerate}
\item \textbf{Design for paired comparisons.} Use the same prompts and seeds across policies to reduce variance.

\item \textbf{Invest in oracle quality.} A well-chosen, representative oracle slice (150--200 samples) is the foundation of reliable calibration.

\item \textbf{Check diagnostics before shipping.} Coverage, ESS, reliability, and \oua{} share catch most issues. Always inspect them.

\item \textbf{Report CIs with OUA.} Honest intervals that include calibrator uncertainty build trust and prevent overconfidence.

\item \textbf{Run sensitivity checks.} Cohort restriction, trimming, and alternative calibrators confirm robustness and flag fragility.

\item \textbf{Document everything.} Judge config, prompt sampling, oracle slice, estimator choice, and diagnostic results. Reproducibility matters.

\item \textbf{Iterate.} Use CJE diagnostics to guide data collection: add labels where coverage is thin, improve the judge where reliability is poor, collect fresh draws where ESS is low.
\end{enumerate}

\subsection{Future directions}

Ongoing and future work includes:
\begin{itemize}
\item \textbf{Sequential and adaptive evaluation:} Extensions for multi-turn conversations and adaptive prompting.
\item \textbf{Multi-objective optimization:} Methods for evaluating trade-offs across multiple KPIs.
\item \textbf{Online integration:} Combining CJE with bandit algorithms for continuous policy improvement.
\item \textbf{Fairness and robustness:} Ensuring unbiased estimates across demographic groups and robust to adversarial prompts.
\item \textbf{Meta-learning for calibration:} Using historical evaluations to improve calibration on new domains.
\end{itemize}

\subsection{Acknowledgments}

CJE builds on decades of work in causal inference, importance sampling, doubly robust estimation, and calibration. Key intellectual debts include:
\begin{itemize}
\item Doubly robust methods (Robins, Rotnitzky, Scharfstein; Bang, Robins; Chernozhukov et al.).
\item Importance sampling and overlap diagnostics (Hirano, Imbens, Ridder; Li, SÃ¤vje, Sussman).
\item Isotonic regression and calibration (Zadrozny, Elkan; Platt; Guo et al.).
\item Influence functions and semiparametric efficiency (van der Laan, Robins).
\end{itemize}

This playbook synthesizes these methods into a practitioner-first toolkit for LLM evaluation.

\subsection{Final thoughts}

LLM-as-judge evaluation is here to stay. As models improve, so do the stakes: small differences in policy performance can translate to large impacts at scale. CJE provides a rigorous, transparent, and practical framework for making these decisions with confidence.

The diagnostics are not gatekeepers---they are guides. When diagnostics pass, trust the estimate. When they fail, use the fixes. When fixes don't work, report the limitations and proceed with caution (or regenerate).

Evaluation is not a one-shot activity. It's a loop: design, collect, calibrate, diagnose, remediate, report, and iterate. CJE is designed to support that loop with minimal friction and maximum transparency.

We hope this playbook serves as a practical companion for teams building and deploying LLMs. For questions, contributions, or feedback, visit the CJE repository at \url{https://github.com/cimo-labs/cje}.

\vspace{1em}
\noindent\textbf{Happy evaluating!}
