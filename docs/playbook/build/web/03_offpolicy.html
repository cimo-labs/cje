<h2>Off-Policy Evaluation: Calibrated IPS and DR</h2>
<p>
[linecolor=cjegray, backgroundcolor=white, linewidth=1pt]
<strong>Note for Direct Mode users:</strong> This section covers advanced off-policy methods for reusing logged data and answering counterfactual questions ("What if we deployed policy X?"). If you're just comparing policies on a fresh eval set using Direct Mode, you can <strong>skip to §6 (Playbook)</strong> or continue reading §4.1–4.4 (core diagnostics). Return here later if you need to reuse logged data without regenerating outputs.
</p>
<h3>What off-policy evaluation solves</h3>
<p>
Off-policy evaluation answers: "What KPI would we see if we deployed policy $\pi'$ instead of the logging policy $\pi_0$?" It lets you assess multiple candidate policies by reusing a single judged log—no need to generate fresh outputs for each candidate. The core challenge is adjusting for the fact that the logged responses came from $\pi_0$, not from $\pi'$.
</p>
<p>
<strong>Off-policy in one minute (operator view)</strong>
</p>
<ol>
</p>
<p>
<li>Collect a judged log under $\pi_0$: $(X_i, A_i, S_i)$ with logprobs for $\pi_0$ and candidate $\pi'$.
<li>Calibrate scores to outcome-scale rewards: $R_i = f(S_i)$ via AutoCal-R.
<li>Compute importance weights: $W_i = \pi'(A_i \mid X_i) / \pi_0(A_i \mid X_i)$.
<li>Stabilize weights with SIMCal{} (monotone projection + variance cap).
<li>Estimate value via IPS{} (weights only) or DR{} (weights + critic).
</p>
<p>
</ol>
<h3>When to use off-policy methods</h3>
<ul>
</p>
<p>
<li>You have a judged log from $\pi_0$ and want to assess multiple candidates $\pi'$ without regenerating.
<li>You have per-sequence logprobs for both $\pi_0$ and candidate policies.
<li>Your main risks are poor overlap (use diagnostics: ESS, tail index) and judge drift (check stability).
<li>For DR, you also need fresh draws to train a critic (outcome model).
</p>
<p>
</ul>
<h3>Calibrated IPS: reweight with stabilized weights</h3>
<p>
The basic IPS{} estimator reweights calibrated rewards by the likelihood ratio:
$$
\est{V}_{\text{IPS}}(\pi') = \frac{1}{n} \sum_{i=1}^n W_i \cdot R_i,
$$
where $W_i = \pi'(A_i \mid X_i) / \pi_0(A_i \mid X_i)$ and $R_i = f(S_i)$.
</p>
<h5>The variance problem.</h5>
 When policies differ significantly, $W_i$ can have extreme values (heavy tails), leading to high variance and unstable estimates. A single large weight can dominate the sum.
</p>
<h3>SIMCal: stabilize weights via monotone projection</h3>
<p>
<strong>SIMCal{</strong> (Score-Indexed Monotone Calibration)} stabilizes weights by projecting them onto monotone functions of the judge score $S$, then applying a variance cap:
</p>
<ol>
</p>
<p>
<li><strong>Fit three candidates</strong> on a fold-out basis: baseline ($W$), monotone increasing ($W^\uparrow$), monotone decreasing ($W^\downarrow$).
<li><strong>Blend</strong> via cross-validated stacking to minimize variance.
<li><strong>Variance cap:</strong> if $\Var(W_{\text{blend}}) > \rho \cdot \Var(W)$, reproject to enforce $\Var \le \rho \cdot \Var(W)$.
</p>
<p>
</ol>
<p>
<strong>Why it works:</strong> Monotone projections weakly reduce variance by majorization, and the cap $\rho$ (default 0.95) ensures strict variance reduction. SIMCal{} explicitly normalizes weights to mean one and projects them as a monotone function of $S$ out-of-fold. Under our J2-M assumption (score-indexed sufficiency), this projection reduces variance and <em>approximately</em> preserves the target; we surface trimming/cohort sensitivity and orthogonality to detect residual bias.
</p>
<h5>Operator artifacts to check:</h5>
<ul>
</p>
<p>
<li><strong>ESS (Effective Sample Size):</strong> $({\textstyle\sum} \tilde{w})^2 / ({\textstyle\sum} \tilde{w}^2)$, where $\tilde{w}$ are post-SIMCal{} weights. Aim for ESS $\ge 30\%$ of $n$ (PASS); 10–30\
<li><strong>Tail heaviness:</strong> Hill index $\alpha \ge 2$ (finite variance). If $\alpha < 2$, weights are catastrophic.
<li><strong>Weight summary:</strong> min, median, max, 95th percentile before/after SIMCal.
</p>
<p>
</ul>
<h3>Calibrated DR: add an outcome model for robustness</h3>
<p>
Doubly robust (DR) combines IPS{} with an outcome model to gain efficiency and robustness. CJE supports two DR implementations, both valid under different modeling assumptions.
</p>
<h5>Two DR implementations.</h5>
<p>
<em>(A) Score-only AIPW (lightweight, default).</em> Fit $\hat{g}(S) \approx \E[R \mid S]$ on fresh draws from $\pi'$ using the judge score $S$ only. Then:
$$
\est{V}_{\text{DR-S}}(\pi') = \frac{1}{n} \sum_{i=1}^n \left[ \tilde{w}_i \cdot (R_i - \hat{g}(S_i)) + \hat{g}(S_i) \right],
$$
where $\tilde{w}_i$ are the stabilized, mean-one weights after SIMCal, and $\hat{g}$ is typically a monotone function of $S$ (isotonic regression). This is the default in CJE: low cost, works well when $S$ is a strong predictor.
</p>
<p>
<em>(B) Action-conditioned DR (canonical, research).</em> Fit $\hat{q}(X,A) \approx \E[R \mid X, A]$ using prompt features $X$ and response $A$, then compute $\hat{g}_{\pi'}(X) = \E_{A \sim \pi'(\ \cdot \mid X)} \hat{q}(X, A)$ via Monte Carlo rollouts or analytic evaluation. Then:
$$
\est{V}_{\text{DR}}(\pi') = \frac{1}{n} \sum_{i=1}^n \left[ \hat{g}_{\pi'}(X_i) + \tilde{w}_i \cdot (R_i - \hat{q}(X_i, A_{0i})) \right].
$$
This is the canonical DR estimator from the semiparametric literature; use it when you have rich features or need maximum robustness (e.g., when $S$ alone is insufficient).
</p>
<p>
<strong>When to use which:</strong>
</p>
<ul>
</p>
<p>
<li><strong>DR-S (score-only):</strong> When judge scores are informative and you want low implementation cost. Default for most LLM evaluations.
<li><strong>DR (action-conditioned):</strong> When you have structured prompt features (length, domain, difficulty), or when $S$ is a weak predictor and you need the full robustness guarantee.
</p>
<p>
</ul>
<h5>Double robustness guarantee:</h5>
 Both estimators are consistent if <em>either</em> the stabilized weights $\tilde{w}(\pi')$ converge to the true density ratios (up to mean-one normalization) <em>or</em> the outcome model ($\hat{g}$ or $\hat{q}$) is correctly specified; if both are good, DR attains the semiparametric efficiency bound.
</p>
<h5>Why isotonic regression for $\hat{g}(S)$?</h5>
 The default outcome model uses isotonic regression because it enforces exactly the right structural prior: <em>higher judge score $\Rightarrow$ no worse expected reward</em>. This minimal monotonicity assumption avoids misspecification risk from rigid parametric forms (sigmoid, beta), provides mean preservation by construction (critical for unbiased DR), and achieves strong stability with few fresh draws (5-10\
</p>
<h3>Inputs & setup (off-policy)</h3>
<p>
<li>Responses from $\pi_0$ with judge scores $S$ and logprobs for $\pi_0$ and each candidate $\pi'$.
<li>A small random subsample with ground truth $Y$ for AutoCal-R{} (same as DM).
<li>For each candidate $\pi'$, generate fresh responses on the same prompts to train the critic $\hat{g}(X)$.
<li>Same fixed rubric/config; check stability over time (Kendall $\tau$ on an anchor set).
</p>
<h3>Fresh draws for outcome modeling</h3>
<p>
For DR, the outcome model $\hat{g}(X)$ must predict what $\pi'$ would achieve. Train it on <strong>fresh draws</strong>: responses generated by $\pi'$ on the same prompts, with judge scores.
</p>
<p>
Without fresh draws, you cannot build a valid outcome model, and IPS{} is your only option.
</p>
<h3>Estimator and inference</h3>
<p>
With calibrated rewards $R_i = f(S_i)$ and stabilized, mean-one weights $\tilde{w}_i$ (via SIMCal), compute:
$$
\est{V}_{\text{IPS}}(\pi') &= \frac{1}{n} \sum_{i=1}^n \tilde{w}_i \cdot R_i, \\
\est{V}_{\text{DR-S}}(\pi') &= \frac{1}{n} \sum_{i=1}^n \left[ \tilde{w}_i \cdot (R_i - \hat{g}(S_i)) + \hat{g}(S_i) \right].
$$
</p>
<p>
Standard errors are computed via influence functions (per-sample contributions), which account for calibration, cross-fitting, and oracle uncertainty (via OUA). Calibrated IPS uses <em>outer cross-validation</em> for SIMCal{} selection by default, so weight-learning uncertainty is reflected in the SEs. CIs are formed using the normal approximation for large samples or t-critical with Satterthwaite df when clusters/oracle folds are small (see §sec:uncertainty).
</p>
<h3>Diagnostics (highest leverage) & quick fixes</h3>
<ol>
[label=(*)]
<li><strong>ESS (Effective Sample Size)</strong> after SIMCal.
</p>
<p>
<em>Fix:</em> If ESS $< 30\%$ (WARN), try cohort restriction (focus on prompts with better overlap) or tune SIMCal{} variance cap $\rho$. If ESS $< 10\%$ (FAIL), switch to DR{} with a stronger outcome model. If ESS $< 1\%$ (REFUSE), the estimate is unreliable—regenerate or narrow scope.
</p>
<p>
<li><strong>Tail heaviness</strong> (Hill index $\alpha$).
</p>
<p>
<em>Fix:</em> If $\alpha < 2$, weights have infinite variance. Use SIMCal{} aggressively, restrict to high-overlap cohorts, or switch to DR. Check for provider temperature/frequency-penalty drift.
</p>
<p>
<li><strong>Weight stability</strong> (compare raw vs.\ SIMCal-adjusted distributions).
</p>
<p>
<em>Fix:</em> Large changes indicate strong reliance on monotonicity assumption (J2-M). Validate by checking that $\E[Y \mid S]$ and $\E[W \mid S]$ are indeed monotone on the oracle slice.
</p>
<p>
<li><strong>DR orthogonality score</strong> (empirical moment with CI).
</p>
<p>
<em>Fix:</em> If the orthogonality CI does not contain zero, either the outcome model or the weights are poor. Improve outcome model regularization, add fresh draws, or revisit SIMCal{} / overlap diagnostics.
</p>
<p>
<li><strong>Oracle uncertainty share</strong> (fraction of total variance from OUA).
</p>
<p>
<em>Fix:</em> If large, add labels targeting uncovered $S$ regions or high-variance prompt families.
</p>
<p>
</ol>
<h3>Reporting template (off-policy)</h3>
<p>
For each candidate policy, report:
</p>
<ul>
</p>
<p>
<li>Calibrated mean $\est{V}(\pi')$ with 95\
<li>ESS (absolute and as \
<li>Weight summary: min, median, max, 95th percentile (raw and post-SIMCal).
<li>For DR, orthogonality score with CI.
<li>OUA{} share; note if oracle slice coverage is thin.
<li>Any judge stability checks (e.g., Kendall $\tau$ on anchor set).
</p>
<p>
</ul>
<h3>Sample size & label planner (off-policy)</h3>
<p>
Let $\hat{\sigma}_W^2$ be the variance of $W_i \cdot R_i$ for IPS, or the variance of the DR influence function. Then a rough CI half-width is
$$
\text{HalfWidth} \approx 1.96 \sqrt{\frac{\hat{\sigma}_W^2}{n} + \Var_{\oua}}.
$$
</p>
<p>
For IPS, more data helps only if ESS is not the bottleneck; if ESS is low, improving overlap (via cohort restriction or stronger SIMCal) is more effective than adding samples. For DR, more fresh draws improve the outcome model and tighten intervals.
</p>
<h3>When to use IPS vs.\ DR</h3>
<ul>
</p>
<p>
<li><strong>Use IPS{</strong>} when you cannot generate fresh draws, or when overlap is excellent (ESS $\ge 30\%$, PASS) and variance is low.
<li><strong>Use DR{</strong>} when you can afford fresh draws and overlap is moderate to poor (ESS $< 30\%$, WARN/FAIL). DR{} is more robust and typically yields tighter CIs.
<li><strong>Use stacked-DR</strong> (ensemble of DR variants) when you want the best of all worlds: robustness, efficiency, and automatic selection among multiple outcome models.
</p>
<p>
</ul>
<h3>Common pitfalls (and how to avoid them)</h3>
<ul>
</p>
<p>
<li><strong>Ignoring ESS.</strong> A single large weight can dominate. Always check ESS; if $< 30\%$ (WARN), investigate via diagnostics; if $< 10\%$ (FAIL), restrict scope or switch to DR.
</p>
<p>
<li><strong>Assuming weights have finite variance.</strong> Check the Hill index $\alpha$; if $< 2$, estimates are unstable. Use SIMCal{} or cohort restriction.
</p>
<p>
<li><strong>Reusing logged responses for the outcome model.</strong> The outcome model must be trained on <em>fresh draws</em> from $\pi'$, not on $\pi_0$'s responses. Otherwise DR{} fails.
</p>
<p>
<li><strong>Ignoring judge drift.</strong> If the judge's scoring changes over time, scores from the log are not comparable to fresh scores. Check stability via an anchor set.
</p>
<p>
<li><strong>Thin oracle coverage.</strong> If labeled $S$ does not cover the range of evaluated $S$, AutoCal-R{} extrapolates poorly. Target labels to uncovered bins or report the coverage badge.
</p>
<p>
</ul>
<h3>Minimal recipe (pseudocode: IPS)</h3>
<pre><code># Inputs: judged log from pi_0 with (X, A, S, logprob_pi0, logprob_pi_prime)
#         oracle slice {(S, Y)}
# Output: calibrated mean, ESS, CI with OUA
</p>
<p>
# 1) Fit AutoCal-R on oracle (cross-fitted)
f = fit_autocal_r(oracle_S, oracle_Y, K=5)
</p>
<p>
# 2) Calibrate rewards and compute raw weights
for i in range(n):
    R[i] = f(S[i])
    W[i] = exp(logprob_pi_prime[i] - logprob_pi0[i])
</p>
<p>
# 3) Stabilize weights with SIMCal (OOF stacking + variance cap)
W_calibrated = simcal(W, S, rho=0.95, K=5)
</p>
<p>
# 4) Estimate value
V_hat_ips = np.mean(W_calibrated * R)
</p>
<p>
# 5) Compute ESS and diagnostics
ESS = (np.sum(W_calibrated)**2) / np.sum(W_calibrated**2)
alpha_hill = estimate_hill_index(W_calibrated)
</p>
<p>
# 6) OUA: refit AutoCal-R across K folds and add oracle variance
for k in range(K):
    f_k = fit_autocal_r(oracle_minus_fold_k)
    # ... recompute V_hat with f_k ...
SE_total_squared = SE_main**2 + Var_OUA
</p>
<p>
# 7) Report V_hat, 95
</code></pre>
<h3>Minimal recipe (pseudocode: DR)</h3>
<pre><code># Inputs: judged log from pi_0, oracle slice, fresh draws from pi_prime
# Output: calibrated mean, orthogonality score, CI with OUA
</p>
<p>
# 1) Fit AutoCal-R on oracle
f = fit_autocal_r(oracle_S, oracle_Y, K=5)
</p>
<p>
# 2) Calibrate rewards and compute stabilized weights (as in IPS)
for i in range(n):
    R[i] = f(S[i])
    W[i] = exp(logprob_pi_prime[i] - logprob_pi0[i])
W_calibrated = simcal(W, S, rho=0.95, K=5)
</p>
<p>
# 3) Train outcome model g(S) on fresh draws (cross-fitted)
for k in range(K):
    train_scores = fresh_draws_scores_minus_fold_k
    g_k = fit_outcome_model(train_scores, R_fresh)  # isotonic regression
    # Predict on fold k:
    for i in fold_k:
        g_hat[i] = g_k(S[i])
</p>
<p>
# 4) Compute DR estimate
V_hat_dr = np.mean(W_calibrated * (R - g_hat) + g_hat)
</p>
<p>
# 5) Orthogonality check (empirical moment: E[W * (R - g_hat)])
ortho_moment = np.mean(W_calibrated * (R - g_hat))
ortho_se = np.std(W_calibrated * (R - g_hat)) / np.sqrt(n)
ortho_ci = [ortho_moment - 1.96*ortho_se, ortho_moment + 1.96*ortho_se]
</p>
<p>
# 6) OUA and CI (as in IPS)
# ... refit AutoCal-R across folds ...
SE_total_squared = SE_main**2 + Var_OUA
</p>
<p>
# 7) Report V_hat_dr, orthogonality CI, ESS, OUA share
</code></pre>
<h3>Scope notes</h3>
<p>
Off-policy evaluation assumes that the logging policy $\pi_0$ had positive probability of generating any response that $\pi'$ might generate (overlap, assumption D2). If policies are too different (e.g., different model families with non-overlapping support), off-policy methods fail. In such cases, generate fresh outputs and use DM{} instead.
</p>
<p>
Judge stability is also critical: if the judge's meaning of a score changes between the time the log was collected and the time fresh draws are evaluated, calibration breaks. Always check drift via an anchor set with stable prompts.

</p>