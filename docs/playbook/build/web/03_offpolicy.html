<section id="off-policy-evaluation-calibrated-ips-and-dr"
class="level1">
<h1>Off-Policy Evaluation: Calibrated IPS and DR</h1>
<div class="mdframed">
<p><strong>Note for Direct Mode users:</strong> This section covers
advanced off-policy methods for reusing logged data and answering
counterfactual questions (“What if we deployed policy X?”). If you’re
just comparing policies on a fresh eval set using Direct Mode, you can
<strong>skip to §6 (Playbook)</strong> or continue reading §4.1–4.4
(core diagnostics). Return here later if you need to reuse logged data
without regenerating outputs.</p>
</div>
<section id="what-off-policy-evaluation-solves" class="level2">
<h2>What off-policy evaluation solves</h2>
<p>Off-policy evaluation answers: “What KPI would we see if we deployed
policy <span class="math inline">\(\pi&#39;\)</span> instead of the
logging policy <span class="math inline">\(\pi_0\)</span>?” It lets you
assess multiple candidate policies by reusing a single judged log—no
need to generate fresh outputs for each candidate. The core challenge is
adjusting for the fact that the logged responses came from <span
class="math inline">\(\pi_0\)</span>, not from <span
class="math inline">\(\pi&#39;\)</span>.</p>
<div class="quickref">
<p><strong>Off-policy in one minute (operator view)</strong></p>
<ol>
<li><p>Collect a judged log under <span
class="math inline">\(\pi_0\)</span>: <span class="math inline">\((X_i,
A_i, S_i)\)</span> with logprobs for <span
class="math inline">\(\pi_0\)</span> and candidate <span
class="math inline">\(\pi&#39;\)</span>.</p></li>
<li><p>Calibrate scores to outcome-scale rewards: <span
class="math inline">\(R_i = f(S_i)\)</span> via AutoCal-R.</p></li>
<li><p>Compute importance weights: <span class="math inline">\(W_i =
\pi&#39;(A_i \mid X_i) / \pi_0(A_i \mid X_i)\)</span>.</p></li>
<li><p>Stabilize weights with SIMCal (monotone projection + variance
cap).</p></li>
<li><p>Estimate value via IPS (weights only) or DR (weights +
critic).</p></li>
</ol>
</div>
</section>
<section id="when-to-use-off-policy-methods" class="level2">
<h2>When to use off-policy methods</h2>
<ul>
<li><p>You have a judged log from <span
class="math inline">\(\pi_0\)</span> and want to assess multiple
candidates <span class="math inline">\(\pi&#39;\)</span> without
regenerating.</p></li>
<li><p>You have per-sequence logprobs for both <span
class="math inline">\(\pi_0\)</span> and candidate policies.</p></li>
<li><p>Your main risks are poor overlap (use diagnostics: ESS, tail
index) and judge drift (check stability).</p></li>
<li><p>For DR, you also need fresh draws to train a critic (outcome
model).</p></li>
</ul>
</section>
<section id="calibrated-ips-reweight-with-stabilized-weights"
class="level2">
<h2>Calibrated IPS: reweight with stabilized weights</h2>
<p>The basic IPS estimator reweights calibrated rewards by the
likelihood ratio: <span
class="math display">\[\est{V}_{\text{IPS}}(\pi&#39;) = \frac{1}{n}
\sum_{i=1}^n W_i \cdot R_i,\]</span> where <span
class="math inline">\(W_i = \pi&#39;(A_i \mid X_i) / \pi_0(A_i \mid
X_i)\)</span> and <span class="math inline">\(R_i = f(S_i)\)</span>.</p>
<section id="the-variance-problem." class="level4">
<h4>The variance problem.</h4>
<p>When policies differ significantly, <span
class="math inline">\(W_i\)</span> can have extreme values (heavy
tails), leading to high variance and unstable estimates. A single large
weight can dominate the sum.</p>
</section>
</section>
<section id="simcal-stabilize-weights-via-monotone-projection"
class="level2">
<h2>SIMCal: stabilize weights via monotone projection</h2>
<p><strong>SIMCal (Score-Indexed Monotone Calibration)</strong>
stabilizes weights by projecting them onto monotone functions of the
judge score <span class="math inline">\(S\)</span>, then applying a
variance cap:</p>
<ol>
<li><p><strong>Fit three candidates</strong> on a fold-out basis:
baseline (<span class="math inline">\(W\)</span>), monotone increasing
(<span class="math inline">\(W^\uparrow\)</span>), monotone decreasing
(<span class="math inline">\(W^\downarrow\)</span>).</p></li>
<li><p><strong>Blend</strong> via cross-validated stacking to minimize
variance.</p></li>
<li><p><strong>Variance cap:</strong> if <span
class="math inline">\(\Var(W_{\text{blend}}) &gt; \rho \cdot
\Var(W)\)</span>, reproject to enforce <span class="math inline">\(\Var
\le \rho \cdot \Var(W)\)</span>.</p></li>
</ol>
<p><strong>Why it works:</strong> Monotone projections weakly reduce
variance by majorization, and the cap <span
class="math inline">\(\rho\)</span> (default 0.95) ensures strict
variance reduction. SIMCal explicitly normalizes weights to mean one (so
stabilized weights average to 1) and projects them as a monotone
function of <span class="math inline">\(S\)</span> out-of-fold. Under
our J2-M/J2-MX assumptions (score-indexed monotone sufficiency), this
projection <em>stabilizes variance with approximately unbiased mean-one
normalization</em>. We surface cohort and trimming sensitivities in
diagnostics to reveal any residual bias; if J2-M fails or overlap is
poor, these sensitivity checks will flag the issue.</p>
<section id="operator-artifacts-to-check" class="level4">
<h4>Operator artifacts to check:</h4>
<ul>
<li><p><strong>ESS (Effective Sample Size):</strong> <span
class="math inline">\(({\textstyle\sum} \tilde{w})^2 / ({\textstyle\sum}
\tilde{w}^2)\)</span>, where <span
class="math inline">\(\tilde{w}\)</span> are post-SIMCal weights. Aim
for ESS <span class="math inline">\(\ge 30\%\)</span> of <span
class="math inline">\(n\)</span> (PASS); 10–30% is acceptable (WARN);
<span class="math inline">\(&lt; 10\%\)</span> is poor (FAIL); <span
class="math inline">\(&lt; 1\%\)</span> is catastrophic
(REFUSE).</p></li>
<li><p><strong>Tail heaviness:</strong> Hill index <span
class="math inline">\(\alpha \ge 2\)</span> (finite variance). If <span
class="math inline">\(\alpha &lt; 2\)</span>, weights are
catastrophic.</p></li>
<li><p><strong>Weight summary:</strong> min, median, max, 95th
percentile before/after SIMCal.</p></li>
</ul>
</section>
</section>
<section id="calibrated-dr-add-an-outcome-model-for-robustness"
class="level2">
<h2>Calibrated DR: add an outcome model for robustness</h2>
<p>Doubly robust (DR) combines IPS with an outcome model to gain
efficiency and robustness. CJE supports two DR implementations, both
valid under different modeling assumptions.</p>
<section id="two-dr-implementations." class="level4">
<h4>Two DR implementations.</h4>
<p><em>(A) Score-only AIPW (lightweight, default).</em> Fit <span
class="math inline">\(\hat{g}(S) \approx \E[R \mid S]\)</span> on fresh
draws from <span class="math inline">\(\pi&#39;\)</span> using the judge
score <span class="math inline">\(S\)</span> only. Then: <span
class="math display">\[\est{V}_{\text{DR-S}}(\pi&#39;) = \frac{1}{n}
\sum_{i=1}^n \left[ \tilde{w}_i \cdot (R_i - \hat{g}(S_i)) +
\hat{g}(S_i) \right],\]</span> where <span
class="math inline">\(\tilde{w}_i\)</span> are the stabilized, mean-one
weights after SIMCal, and <span class="math inline">\(\hat{g}\)</span>
is typically a monotone function of <span
class="math inline">\(S\)</span> (isotonic regression). This is the
default in CJE: low cost, works well when <span
class="math inline">\(S\)</span> is a strong predictor.</p>
<p><em>(B) Action-conditioned DR (canonical, research).</em> Fit <span
class="math inline">\(\hat{q}(X,A) \approx \E[R \mid X, A]\)</span>
using prompt features <span class="math inline">\(X\)</span> and
response <span class="math inline">\(A\)</span>, then compute <span
class="math inline">\(\hat{g}_{\pi&#39;}(X) = \E_{A \sim \pi&#39;(\
\cdot \mid X)} \hat{q}(X, A)\)</span> via Monte Carlo rollouts or
analytic evaluation. Then: <span
class="math display">\[\est{V}_{\text{DR}}(\pi&#39;) = \frac{1}{n}
\sum_{i=1}^n \left[ \hat{g}_{\pi&#39;}(X_i) + \tilde{w}_i \cdot (R_i -
\hat{q}(X_i, A_{0i})) \right].\]</span> This is the canonical DR
estimator from the semiparametric literature; use it when you have rich
features or need maximum robustness (e.g., when <span
class="math inline">\(S\)</span> alone is insufficient).</p>
<p><strong>When to use which:</strong></p>
<ul>
<li><p><strong>DR-S (score-only):</strong> When judge scores are
informative and you want low implementation cost. Default for most LLM
evaluations.</p></li>
<li><p><strong>DR (action-conditioned):</strong> When you have
structured prompt features (length, domain, difficulty), or when <span
class="math inline">\(S\)</span> is a weak predictor and you need the
full robustness guarantee.</p></li>
</ul>
</section>
<section id="double-robustness-guarantee" class="level4">
<h4>Double robustness guarantee:</h4>
<p>Both estimators are consistent if <em>either</em> the stabilized
weights <span class="math inline">\(\tilde{w}(\pi&#39;)\)</span>
converge to the true density ratios (up to mean-one normalization)
<em>or</em> the outcome model (<span
class="math inline">\(\hat{g}\)</span> or <span
class="math inline">\(\hat{q}\)</span>) is correctly specified; if both
are good, DR attains the semiparametric efficiency bound.</p>
</section>
<section id="why-isotonic-regression-for-hatgs" class="level4">
<h4>Why isotonic regression for <span
class="math inline">\(\hat{g}(S)\)</span>?</h4>
<p>The default outcome model uses isotonic regression because it
enforces exactly the right structural prior: <em>higher judge score
<span class="math inline">\(\Rightarrow\)</span> no worse expected
reward</em>. This minimal monotonicity assumption avoids
misspecification risk from rigid parametric forms (sigmoid, beta),
provides mean preservation by construction (critical for unbiased DR),
and achieves strong stability with few fresh draws (5-10% coverage often
sufficient). Isotonic’s step-function output also makes edge fragility
visible in diagnostics, enabling targeted label collection. When
monotonicity fails (e.g., systematic length bias at fixed <span
class="math inline">\(S\)</span>), AutoCal-R’s two-stage variant learns
a smooth transformation first.</p>
</section>
</section>
<section id="inputs-setup-off-policy" class="level2">
<h2>Inputs &amp; setup (off-policy)</h2>
<dl>
<dt>Judged log:</dt>
<dd>
<p>Responses from <span class="math inline">\(\pi_0\)</span> with judge
scores <span class="math inline">\(S\)</span> and logprobs for <span
class="math inline">\(\pi_0\)</span> and each candidate <span
class="math inline">\(\pi&#39;\)</span>.</p>
</dd>
<dt>Oracle slice:</dt>
<dd>
<p>A small random subsample with ground truth <span
class="math inline">\(Y\)</span> for AutoCal-R (same as DM).</p>
</dd>
<dt>Fresh draws (DR only):</dt>
<dd>
<p>For each candidate <span class="math inline">\(\pi&#39;\)</span>,
generate fresh responses on the same prompts to train the critic <span
class="math inline">\(\hat{g}(X)\)</span>.</p>
</dd>
<dt>Judge:</dt>
<dd>
<p>Same fixed rubric/config; check stability over time (Kendall <span
class="math inline">\(\tau\)</span> on an anchor set).</p>
</dd>
</dl>
</section>
<section id="fresh-draws-for-outcome-modeling" class="level2">
<h2>Fresh draws for outcome modeling</h2>
<p>For DR, the outcome model <span
class="math inline">\(\hat{g}(X)\)</span> must predict what <span
class="math inline">\(\pi&#39;\)</span> would achieve. Train it on
<strong>fresh draws</strong>: responses generated by <span
class="math inline">\(\pi&#39;\)</span> on the same prompts, with judge
scores.</p>
<p>Without fresh draws, you cannot build a valid outcome model, and IPS
is your only option.</p>
</section>
<section id="estimator-and-inference" class="level2">
<h2>Estimator and inference</h2>
<p>With calibrated rewards <span class="math inline">\(R_i =
f(S_i)\)</span> and stabilized, mean-one weights <span
class="math inline">\(\tilde{w}_i\)</span> (via SIMCal), compute: <span
class="math display">\[\begin{aligned}
\est{V}_{\text{IPS}}(\pi&#39;) &amp;= \frac{1}{n} \sum_{i=1}^n
\tilde{w}_i \cdot R_i, \\
\est{V}_{\text{DR-S}}(\pi&#39;) &amp;= \frac{1}{n} \sum_{i=1}^n \left[
\tilde{w}_i \cdot (R_i - \hat{g}(S_i)) + \hat{g}(S_i) \right].
\end{aligned}\]</span></p>
<p>Standard errors are computed via influence functions (per-sample
contributions), which account for calibration, cross-fitting, and oracle
uncertainty (via OUA). Calibrated IPS uses <em>outer
cross-validation</em> for SIMCal selection by default, so
weight-learning uncertainty is reflected in the SEs. CIs are formed
using the normal approximation for large samples or t-critical with
Satterthwaite df when clusters/oracle folds are small (see §<a
href="#sec:uncertainty" data-reference-type="ref"
data-reference="sec:uncertainty">[sec:uncertainty]</a>).</p>
</section>
<section id="diagnostics-highest-leverage-quick-fixes" class="level2">
<h2>Diagnostics (highest leverage) &amp; quick fixes</h2>
<ol>
<li><p><strong>ESS (Effective Sample Size)</strong> after SIMCal.</p>
<p><em>Fix:</em> If ESS <span class="math inline">\(&lt; 30\%\)</span>
(WARN), try cohort restriction (focus on prompts with better overlap) or
tune SIMCal variance cap <span class="math inline">\(\rho\)</span>. If
ESS <span class="math inline">\(&lt; 10\%\)</span> (FAIL), switch to DR
with a stronger outcome model. If ESS <span class="math inline">\(&lt;
1\%\)</span> (REFUSE), the estimate is unreliable—regenerate or narrow
scope.</p></li>
<li><p><strong>Tail heaviness</strong> (Hill index <span
class="math inline">\(\alpha\)</span>).</p>
<p><em>Fix:</em> If <span class="math inline">\(\alpha &lt; 2\)</span>,
weights have infinite variance. Use SIMCal aggressively, restrict to
high-overlap cohorts, or switch to DR. Check for provider
temperature/frequency-penalty drift.</p></li>
<li><p><strong>Weight stability</strong> (compare raw
vs. SIMCal-adjusted distributions).</p>
<p><em>Fix:</em> Large changes indicate strong reliance on monotonicity
assumption (J2-M). Validate by checking that <span
class="math inline">\(\E[Y \mid S]\)</span> and <span
class="math inline">\(\E[W \mid S]\)</span> are indeed monotone on the
oracle slice.</p></li>
<li><p><strong>DR orthogonality score</strong> (empirical moment with
CI).</p>
<p><em>Fix:</em> If the orthogonality CI does not contain zero, either
the outcome model or the weights are poor. Improve outcome model
regularization, add fresh draws, or revisit SIMCal / overlap
diagnostics.</p></li>
<li><p><strong>Oracle uncertainty share</strong> (fraction of total
variance from OUA).</p>
<p><em>Fix:</em> If large, add labels targeting uncovered <span
class="math inline">\(S\)</span> regions or high-variance prompt
families.</p></li>
</ol>
</section>
<section id="reporting-template-off-policy" class="level2">
<h2>Reporting template (off-policy)</h2>
<p>For each candidate policy, report:</p>
<ul>
<li><p>Calibrated mean <span
class="math inline">\(\est{V}(\pi&#39;)\)</span> with 95% CI (IPS or
DR).</p></li>
<li><p>ESS (absolute and as % of <span
class="math inline">\(n\)</span>), Hill index <span
class="math inline">\(\alpha\)</span>.</p></li>
<li><p>Weight summary: min, median, max, 95th percentile (raw and
post-SIMCal).</p></li>
<li><p>For DR, orthogonality score with CI.</p></li>
<li><p>OUA share; note if oracle slice coverage is thin.</p></li>
<li><p>Any judge stability checks (e.g., Kendall <span
class="math inline">\(\tau\)</span> on anchor set).</p></li>
</ul>
</section>
<section id="sample-size-label-planner-off-policy" class="level2">
<h2>Sample size &amp; label planner (off-policy)</h2>
<p>Let <span class="math inline">\(\hat{\sigma}_W^2\)</span> be the
variance of <span class="math inline">\(W_i \cdot R_i\)</span> for IPS,
or the variance of the DR influence function. Then a rough CI half-width
is <span class="math display">\[\text{HalfWidth} \approx 1.96
\sqrt{\frac{\hat{\sigma}_W^2}{n} + \Var_{OUA}}.\]</span></p>
<p>For IPS, more data helps only if ESS is not the bottleneck; if ESS is
low, improving overlap (via cohort restriction or stronger SIMCal) is
more effective than adding samples. For DR, more fresh draws improve the
outcome model and tighten intervals.</p>
</section>
<section id="when-to-use-ips-vs.-dr" class="level2">
<h2>When to use IPS vs. DR</h2>
<ul>
<li><p><strong>Use IPS</strong> when you cannot generate fresh draws, or
when overlap is excellent (ESS <span class="math inline">\(\ge
30\%\)</span>, PASS) and variance is low.</p></li>
<li><p><strong>Use DR</strong> when you can afford fresh draws and
overlap is moderate to poor (ESS <span class="math inline">\(&lt;
30\%\)</span>, WARN/FAIL). DR is more robust and typically yields
tighter CIs.</p></li>
<li><p><strong>Use stacked-DR</strong> (ensemble of DR variants) when
you want the best of all worlds: robustness, efficiency, and automatic
selection among multiple outcome models.</p></li>
</ul>
</section>
<section id="common-pitfalls-and-how-to-avoid-them" class="level2">
<h2>Common pitfalls (and how to avoid them)</h2>
<ul>
<li><p><strong>Ignoring ESS.</strong> A single large weight can
dominate. Always check ESS; if <span class="math inline">\(&lt;
30\%\)</span> (WARN), investigate via diagnostics; if <span
class="math inline">\(&lt; 10\%\)</span> (FAIL), restrict scope or
switch to DR.</p></li>
<li><p><strong>Assuming weights have finite variance.</strong> Check the
Hill index <span class="math inline">\(\alpha\)</span>; if <span
class="math inline">\(&lt; 2\)</span>, estimates are unstable. Use
SIMCal or cohort restriction.</p></li>
<li><p><strong>Reusing logged responses for the outcome model.</strong>
The outcome model must be trained on <em>fresh draws</em> from <span
class="math inline">\(\pi&#39;\)</span>, not on <span
class="math inline">\(\pi_0\)</span>’s responses. Otherwise DR
fails.</p></li>
<li><p><strong>Ignoring judge drift.</strong> If the judge’s scoring
changes over time, scores from the log are not comparable to fresh
scores. Check stability via an anchor set.</p></li>
<li><p><strong>Thin oracle coverage.</strong> If labeled <span
class="math inline">\(S\)</span> does not cover the range of evaluated
<span class="math inline">\(S\)</span>, AutoCal-R extrapolates poorly.
Target labels to uncovered bins or report the coverage badge.</p></li>
</ul>
</section>
<section id="minimal-recipe-pseudocode-ips" class="level2">
<h2>Minimal recipe (pseudocode: IPS)</h2>
<pre class="python" data-language="Python"
data-caption="Calibrated IPS Recipe"><code># Inputs: judged log from pi_0 with (X, A, S, logprob_pi0, logprob_pi_prime)
#         oracle slice {(S, Y)}
# Output: calibrated mean, ESS, CI with OUA

# 1) Fit AutoCal-R on oracle (cross-fitted)
f = fit_autocal_r(oracle_S, oracle_Y, K=5)

# 2) Calibrate rewards and compute raw weights
for i in range(n):
    R[i] = f(S[i])
    W[i] = exp(logprob_pi_prime[i] - logprob_pi0[i])

# 3) Stabilize weights with SIMCal (OOF stacking + variance cap)
W_calibrated = simcal(W, S, rho=0.95, K=5)

# 4) Estimate value
V_hat_ips = np.mean(W_calibrated * R)

# 5) Compute ESS and diagnostics
ESS = (np.sum(W_calibrated)**2) / np.sum(W_calibrated**2)
alpha_hill = estimate_hill_index(W_calibrated)

# 6) OUA: refit AutoCal-R across K folds and add oracle variance
for k in range(K):
    f_k = fit_autocal_r(oracle_minus_fold_k)
    # ... recompute V_hat with f_k ...
SE_total_squared = SE_main**2 + Var_OUA

# 7) Report V_hat, 95% CI, ESS, Hill index, OUA share</code></pre>
</section>
<section id="minimal-recipe-pseudocode-dr" class="level2">
<h2>Minimal recipe (pseudocode: DR)</h2>
<pre class="python" data-language="Python"
data-caption="Calibrated DR Recipe"><code># Inputs: judged log from pi_0, oracle slice, fresh draws from pi_prime
# Output: calibrated mean, orthogonality score, CI with OUA

# 1) Fit AutoCal-R on oracle
f = fit_autocal_r(oracle_S, oracle_Y, K=5)

# 2) Calibrate rewards and compute stabilized weights (as in IPS)
for i in range(n):
    R[i] = f(S[i])
    W[i] = exp(logprob_pi_prime[i] - logprob_pi0[i])
W_calibrated = simcal(W, S, rho=0.95, K=5)

# 3) Train outcome model g(S) on fresh draws (cross-fitted)
for k in range(K):
    train_scores = fresh_draws_scores_minus_fold_k
    g_k = fit_outcome_model(train_scores, R_fresh)  # isotonic regression
    # Predict on fold k:
    for i in fold_k:
        g_hat[i] = g_k(S[i])

# 4) Compute DR estimate
V_hat_dr = np.mean(W_calibrated * (R - g_hat) + g_hat)

# 5) Orthogonality check (empirical moment: E[W * (R - g_hat)])
ortho_moment = np.mean(W_calibrated * (R - g_hat))
ortho_se = np.std(W_calibrated * (R - g_hat)) / np.sqrt(n)
ortho_ci = [ortho_moment - 1.96*ortho_se, ortho_moment + 1.96*ortho_se]

# 6) OUA and CI (as in IPS)
# ... refit AutoCal-R across folds ...
SE_total_squared = SE_main**2 + Var_OUA

# 7) Report V_hat_dr, orthogonality CI, ESS, OUA share</code></pre>
</section>
<section id="scope-notes" class="level2">
<h2>Scope notes</h2>
<p>Off-policy evaluation assumes that the logging policy <span
class="math inline">\(\pi_0\)</span> had positive probability of
generating any response that <span
class="math inline">\(\pi&#39;\)</span> might generate (overlap,
assumption D2). If policies are too different (e.g., different model
families with non-overlapping support), off-policy methods fail. In such
cases, generate fresh outputs and use DM instead.</p>
<p>Judge stability is also critical: if the judge’s meaning of a score
changes between the time the log was collected and the time fresh draws
are evaluated, calibration breaks. Always check drift via an anchor set
with stable prompts.</p>
</section>
</section>
