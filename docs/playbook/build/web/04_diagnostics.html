<section id="diagnostics-the-eight-high-leverage-checks" class="level1">
<h1>Diagnostics: The Eight High-Leverage Checks</h1>
<section id="overview" class="level2">
<h2>Overview</h2>
<p>CJE’s diagnostic dashboard is designed to catch the most important
failure modes with minimal operator overhead. Each diagnostic points
directly to a concrete fix. This section details the eight checks, their
interpretation, thresholds, and remediations.</p>
</section>
<section id="the-diagnostic-checklist" class="level2">
<h2>The diagnostic checklist</h2>
<section id="core-diagnostics-all-modes" class="level4">
<h4>Core diagnostics (all modes):</h4>
<ol>
<li><p><strong>Score coverage</strong>: Does the oracle slice span the
evaluation <span class="math inline">\(S\)</span>-range?</p></li>
<li><p><strong>Calibration reliability</strong>: Is <span
class="math inline">\(f(S)\)</span> accurate across the <span
class="math inline">\(S\)</span> spectrum?</p></li>
<li><p><strong>Judge stability</strong>: Is the judge’s scoring
consistent over time or across batches?</p></li>
<li><p><strong>Oracle uncertainty share</strong>: Is label scarcity the
bottleneck for precision?</p></li>
<li><p><strong>Transportability audit</strong>: Does the calibrator
transport to target policies/time windows?</p></li>
</ol>
</section>
<section id="off-policy-diagnostics-for-only" class="level4">
<h4>Off-policy diagnostics (for / only):</h4>
<ol>
<li><p><strong>Effective sample size (ESS)</strong>: Do we have enough
effective samples after reweighting?</p></li>
<li><p><strong>Tail heaviness</strong>: Are importance weights
catastrophically heavy-tailed?</p></li>
<li><p><strong>DR orthogonality</strong> (for only): Is the critic good
enough for doubly robust guarantees?</p></li>
</ol>
<p>Each diagnostic produces a compact artifact (a number, a CI, or a
plot) and a traffic-light signal (pass / warn / fail).</p>
<p><strong>If you’re using Direct Mode only:</strong> Focus on
diagnostics 1–5. Skip diagnostics 6–8 unless you later need
counterfactual inference.</p>
</section>
</section>
<section id="diagnostic-1-score-coverage" class="level2">
<h2>Diagnostic 1: Score coverage</h2>
<section id="what-it-measures." class="level4">
<h4>What it measures.</h4>
<p>The fraction of evaluated scores <span
class="math inline">\(S\)</span> that fall within the range of oracle
scores used to train . Also checks boundary slopes for extrapolation
risk.</p>
</section>
<section id="why-it-matters." class="level4">
<h4>Why it matters.</h4>
<p>If must extrapolate beyond the labeled <span
class="math inline">\(S\)</span>-range, calibrated rewards <span
class="math inline">\(R = f(S)\)</span> can be wildly wrong. This is the
single most common source of large errors.</p>
</section>
<section id="artifacts-to-inspect" class="level4">
<h4>Artifacts to inspect:</h4>
<ul>
<li><p><strong>Coverage fraction:</strong> <span
class="math inline">\(\#\{i : S_{\min}^{\text{oracle}} \le S_i \le
S_{\max}^{\text{oracle}}\} / n\)</span>.</p></li>
<li><p><strong>Boundary slopes:</strong> slope of <span
class="math inline">\(f\)</span> near <span
class="math inline">\(S_{\min}\)</span> and <span
class="math inline">\(S_{\max}\)</span>. Steep slopes indicate sensitive
extrapolation (small <span class="math inline">\(S\)</span> changes
yield large <span class="math inline">\(R\)</span> changes); flat slopes
suggest poor discrimination at edges.</p></li>
<li><p><strong>Histogram overlay:</strong> oracle <span
class="math inline">\(S\)</span> distribution vs. evaluation <span
class="math inline">\(S\)</span> distribution.</p></li>
</ul>
</section>
<section id="thresholds" class="level4">
<h4>Thresholds:</h4>
<ul>
<li><p><strong>Pass:</strong> Coverage <span class="math inline">\(\ge
95\%\)</span>, boundary slopes moderate.</p></li>
<li><p><strong>Warn:</strong> Coverage <span class="math inline">\(\in
[85\%, 95\%)\)</span> or steep boundary slopes.</p></li>
<li><p><strong>Fail:</strong> Coverage <span class="math inline">\(&lt;
85\%\)</span> or nearly-flat/nearly-vertical boundary slopes.</p></li>
</ul>
</section>
<section id="fixes" class="level4">
<h4>Fixes:</h4>
<ul>
<li><p>Add a small number of labels (5–20) targeting the uncovered <span
class="math inline">\(S\)</span> bins.</p></li>
<li><p>Narrow the prompt set to the intended deployment slice.</p></li>
<li><p>If the evaluation set is meant to stress-test edge cases, ensure
the oracle slice includes examples from those edges.</p></li>
</ul>
</section>
</section>
<section id="diagnostic-2-calibration-reliability" class="level2">
<h2>Diagnostic 2: Calibration reliability</h2>
<section id="what-it-measures.-1" class="level4">
<h4>What it measures.</h4>
<p>Out-of-fold (OOF) calibration accuracy: how well <span
class="math inline">\(f(S)\)</span> predicts <span
class="math inline">\(Y\)</span> on held-out oracle data. Includes
overall error and regional error (low/mid/high <span
class="math inline">\(S\)</span>).</p>
</section>
<section id="why-it-matters.-1" class="level4">
<h4>Why it matters.</h4>
<p>Even if coverage is good, can be miscalibrated in specific regions
(e.g., <span class="math inline">\(f(S)\)</span> is too optimistic for
high <span class="math inline">\(S\)</span>). Regional miscalibration
biases estimates and breaks mean preservation.</p>
</section>
<section id="artifacts-to-inspect-1" class="level4">
<h4>Artifacts to inspect:</h4>
<ul>
<li><p><strong>OOF calibration curve:</strong> scatter plot of <span
class="math inline">\(f(S)\)</span> vs. <span
class="math inline">\(Y\)</span> on held-out folds, with diagonal
reference.</p></li>
<li><p><strong>Mean preservation check:</strong> <span
class="math inline">\(|\E_{\text{oracle}}[f(S)] -
\E_{\text{oracle}}[Y]|\)</span> (should be <span
class="math inline">\(\approx 0\)</span>).</p></li>
<li><p><strong>Regional error:</strong> mean absolute error (MAE) in
low, mid, high <span class="math inline">\(S\)</span> terciles.</p></li>
</ul>
</section>
<section id="thresholds-1" class="level4">
<h4>Thresholds:</h4>
<ul>
<li><p><strong>Pass:</strong> OOF MAE <span class="math inline">\(&lt;
0.05\)</span>, regional MAE balanced, mean preservation within <span
class="math inline">\(\pm 0.02\)</span>.</p></li>
<li><p><strong>Warn:</strong> OOF MAE <span class="math inline">\(\in
[0.05, 0.10]\)</span> or one region has <span
class="math inline">\(2\times\)</span> the error of others.</p></li>
<li><p><strong>Fail:</strong> OOF MAE <span class="math inline">\(&gt;
0.10\)</span> or severe regional imbalance or mean drift <span
class="math inline">\(&gt; 0.05\)</span>.</p></li>
</ul>
</section>
<section id="fixes-1" class="level4">
<h4>Fixes:</h4>
<ul>
<li><p>Enable two-stage (spline index <span
class="math inline">\(\to\)</span> isotonic) for regional
adaptivity.</p></li>
<li><p>Add a coarse index (prompt family, length bin) to allow
family-specific calibration.</p></li>
<li><p>Collect 10–30 additional labels in the problematic <span
class="math inline">\(S\)</span> region.</p></li>
<li><p>Check if judge drift occurred (see §4.3).</p></li>
</ul>
</section>
</section>
<section id="diagnostic-2b-covariate-aware-reliability-two-stage"
class="level2">
<h2>Diagnostic 2b: Covariate-aware reliability (two-stage)</h2>
<section id="what-it-measures.-2" class="level4">
<h4>What it measures.</h4>
<p>Whether the two-stage index <span
class="math inline">\(T=g(S,X_{\mathrm{cov}})\)</span> resolves regional
miscalibration when plain J2-M fails.</p>
</section>
<section id="why-it-matters.-2" class="level4">
<h4>Why it matters.</h4>
<p>When slice effects (e.g., length or domain bias at fixed <span
class="math inline">\(S\)</span>) cause regional miscalibration,
two-stage with covariates can capture heterogeneity while preserving
monotonicity. This diagnostic validates that the covariate-augmented
mapping improves accuracy.</p>
</section>
<section id="artifacts-to-inspect-2" class="level4">
<h4>Artifacts to inspect:</h4>
<ul>
<li><p><strong>Reliability by <span
class="math inline">\(T\)</span>-quantiles (OOF):</strong> calibration
curve using the risk index <span class="math inline">\(T\)</span>
instead of raw <span class="math inline">\(S\)</span>.</p></li>
<li><p><strong>Residual MAE by slices of key covariates:</strong> e.g.,
short vs. long responses, or domain groups.</p></li>
<li><p><strong>Before/after regional MAE:</strong> compare plain
isotonic on <span class="math inline">\(S\)</span> vs. two-stage with
covariates.</p></li>
</ul>
</section>
<section id="thresholds-2" class="level4">
<h4>Thresholds:</h4>
<p>Same as Diagnostic 2 (OOF MAE <span class="math inline">\(&lt;
0.05\)</span> pass, <span class="math inline">\(&lt; 0.10\)</span> warn,
<span class="math inline">\(&gt; 0.10\)</span> fail), applied on <span
class="math inline">\(T\)</span>-quantiles. Additionally flag if any
covariate slice has <span class="math inline">\(&gt; 2\times\)</span>
the global MAE.</p>
</section>
<section id="fixes-2" class="level4">
<h4>Fixes:</h4>
<ul>
<li><p>Add/modify covariates (e.g., include response length if not
already used).</p></li>
<li><p>Increase labels in failing slices (e.g., long responses if they
show high residual error).</p></li>
<li><p>Tune first-stage capacity: more knots for splines or shallow
trees (avoid overfitting with small oracle slices).</p></li>
</ul>
</section>
</section>
<section id="diagnostic-3-judge-stability" class="level2">
<h2>Diagnostic 3: Judge stability</h2>
<section id="what-it-measures.-3" class="level4">
<h4>What it measures.</h4>
<p>Rank stability of judge scores on a small anchor set (20–50 fixed
prompts) over time or across batches, measured via Kendall <span
class="math inline">\(\tau\)</span> or Spearman <span
class="math inline">\(\rho\)</span>.</p>
</section>
<section id="why-it-matters.-3" class="level4">
<h4>Why it matters.</h4>
<p>If the judge’s interpretation of scores drifts between the time the
log was collected and the time fresh draws are evaluated, calibration
breaks. Judge drift affects <em>all</em> evaluation modes (DM, IPS, DR)
because it invalidates the assumption that <span class="math inline">\(S
= s(X, A)\)</span> is stable. This is a fundamental validity check.</p>
</section>
<section id="artifacts-to-inspect-3" class="level4">
<h4>Artifacts to inspect:</h4>
<ul>
<li><p><strong>Rank correlation:</strong> Kendall <span
class="math inline">\(\tau\)</span> between scores at time <span
class="math inline">\(t_0\)</span> (logging) and <span
class="math inline">\(t_1\)</span> (evaluation).</p></li>
<li><p><strong>Score drift plot:</strong> scatter of <span
class="math inline">\(S_{t_0}\)</span> vs. <span
class="math inline">\(S_{t_1}\)</span> for anchor prompts.</p></li>
<li><p><strong>Temporal batch correlation:</strong> sequential <span
class="math inline">\(\tau\)</span> across time-ordered batches (if
timestamps available).</p></li>
</ul>
</section>
<section id="thresholds-3" class="level4">
<h4>Thresholds:</h4>
<ul>
<li><p><strong>Pass:</strong> <span class="math inline">\(\tau \ge
0.90\)</span> (strong stability).</p></li>
<li><p><strong>Warn:</strong> <span class="math inline">\(\tau \in
[0.75, 0.90)\)</span> (moderate drift; check config).</p></li>
<li><p><strong>Fail:</strong> <span class="math inline">\(\tau &lt;
0.75\)</span> (severe drift; refresh oracle and re-calibrate).</p></li>
</ul>
</section>
<section id="fixes-3" class="level4">
<h4>Fixes:</h4>
<ul>
<li><p>Freeze judge version and config during the evaluation
window.</p></li>
<li><p>If drift is detected, refresh the oracle slice with new labels
and re-fit .</p></li>
<li><p>Consider using a judging protocol with locked
temperature/sampling to reduce stochasticity.</p></li>
<li><p>Use automated temporal drift detection: set
<code>check_drift=True</code> and <code>timestamp_field</code> to
monitor stability across batches.</p></li>
</ul>
</section>
<section id="automated-detection." class="level4">
<h4>Automated detection.</h4>
<p>When evaluation data includes timestamps, CJE can automatically
detect drift over time using <code>timestamp_field</code> and
<code>check_drift=True</code>. This sorts samples by timestamp, computes
sequential Kendall <span class="math inline">\(\tau\)</span>
correlations across time batches, and flags drift points. This automates
the anchor-set workflow for datasets with temporal metadata.</p>
<p><em>Transport trigger:</em> If anchor <span
class="math inline">\(\tau &lt; 0.90\)</span> or the Transportability
Audit (Diagnostic 5) fails, treat the mapping as
<em>non-transportable</em>: gather a fresh calibration slice and refit;
do not rely on old <span class="math inline">\(f\)</span> without
adjustment.</p>
</section>
</section>
<section id="diagnostic-4-oracle-uncertainty-oua-share" class="level2">
<h2>Diagnostic 4: Oracle uncertainty (OUA) share</h2>
<section id="what-it-measures.-4" class="level4">
<h4>What it measures.</h4>
<p>The fraction of total variance attributable to oracle uncertainty
(calibrator noise): <span class="math display">\[\text{OUA share} =
\frac{\Var_{\oua}}{\SE^2_{\text{total}}}.\]</span></p>
</section>
<section id="why-it-matters.-4" class="level4">
<h4>Why it matters.</h4>
<p>High OUA share means label scarcity is the bottleneck, not prompt
scarcity. Adding more prompts won’t help; you need more labels.</p>
</section>
<section id="artifacts-to-inspect-4" class="level4">
<h4>Artifacts to inspect:</h4>
<ul>
<li><p><strong>OUA share:</strong> reported as a percentage.</p></li>
<li><p><strong>Variance decomposition:</strong> <span
class="math inline">\(\SE^2_{\text{total}} = \Var_{\text{main}} +
\Var_{\oua}\)</span>.</p></li>
</ul>
</section>
<section id="thresholds-4" class="level4">
<h4>Thresholds:</h4>
<ul>
<li><p><strong>Pass:</strong> OUA share <span class="math inline">\(&lt;
20\%\)</span> (labels sufficient).</p></li>
<li><p><strong>Warn:</strong> OUA share <span class="math inline">\(\in
[20\%, 50\%]\)</span> (labels becoming a bottleneck).</p></li>
<li><p><strong>Fail:</strong> OUA share <span class="math inline">\(&gt;
50\%\)</span> (labels are the main source of uncertainty).</p></li>
</ul>
</section>
<section id="fixes-4" class="level4">
<h4>Fixes:</h4>
<ul>
<li><p>Add labels, prioritizing regions where <span
class="math inline">\(S\)</span> is sparse or where policy comparisons
are sensitive.</p></li>
<li><p>For high OUA share, adding prompts yields diminishing returns;
focus labeling budget on coverage and regional balance.</p></li>
</ul>
</section>
</section>
<section id="diag:transport" class="level2">
<h2>Diagnostic 5: Transportability Audit (policy / time)</h2>
<section id="goal." class="level4">
<h4>Goal.</h4>
<p>Test whether the source calibrator <span
class="math inline">\(f\)</span> transports to a target policy or time
window.</p>
</section>
<section id="probe-protocol-cheap." class="level4">
<h4>Probe protocol (cheap).</h4>
<p>For each target stratum <span class="math inline">\(G\)</span>:
collect a small <em>probe</em> of <span
class="math inline">\(n_{\text{probe}}=40\text{--}60\)</span> oracle
labels, compute residuals <span class="math inline">\(e_i = Y_i -
f(S_i,X_{i,\mathrm{cov}})\)</span>, and evaluate:</p>
<ol>
<li><p><strong>Global residual mean:</strong> <span
class="math inline">\(\hat\delta_G = \frac{1}{n_{\text{probe}}}\sum
e_i\)</span> with <span class="math inline">\(\mathrm{SE}(\hat\delta_G)
= \hat\sigma_e/\sqrt{n_{\text{probe}}}\)</span>; 95% CI should contain
<span class="math inline">\(0\)</span>.</p></li>
<li><p><strong>Regional residuals:</strong> bin by deciles of the
two-stage index <span
class="math inline">\(T=g(S,X_{\mathrm{cov}})\)</span> (or <span
class="math inline">\(S\)</span> if single-stage), compute mean <span
class="math inline">\(e\)</span> per bin; require all bins within <span
class="math inline">\(\pm 0.05\)</span> and no monotone
pattern.</p></li>
<li><p><strong>Score/Index coverage:</strong> coverage of target <span
class="math inline">\((S\)</span> or <span
class="math inline">\(U=\mathrm{ECDF}(T))\)</span> within the oracle
training range <span class="math inline">\(\ge 95\%\)</span>; inspect
boundary slopes.</p></li>
</ol>
</section>
<section id="traffic-light-thresholds." class="level4">
<h4>Traffic-light thresholds.</h4>
<ul>
<li><p><strong>PASS:</strong> <span class="math inline">\(0 \in
\text{CI}(\hat\delta_G)\)</span> and all decile residual means <span
class="math inline">\(\in [-0.05,0.05]\)</span> and coverage <span
class="math inline">\(\ge 95\%\)</span>.</p></li>
<li><p><strong>WARN:</strong> <span class="math inline">\(|\hat\delta_G|
\in [0.02,0.05]\)</span> or one/two bins exceed 0.05 in magnitude, or
coverage <span class="math inline">\(\in [85\%,95\%)\)</span>.</p></li>
<li><p><strong>FAIL:</strong> <span class="math inline">\(0 \notin
\text{CI}(\hat\delta_G)\)</span> or 3+ bins exceed 0.05, or coverage
<span class="math inline">\(&lt;85\%\)</span>.</p></li>
</ul>
</section>
<section id="fixes-by-failure-mode." class="level4">
<h4>Fixes by failure mode.</h4>
<ul>
<li><p><strong>Uniform mean shift:</strong> apply a per-group intercept
(“mean anchoring”) <span class="math inline">\(\tilde
f_G(S,X)=\mathrm{clip}(f(S,X)+\hat\delta_G)\)</span>; re-check regional
residuals.</p></li>
<li><p><strong>Regional pattern:</strong> enable two-stage with
covariates that explain the divergence (e.g., response length, domain);
collect +20–40 labels concentrated in failing deciles; refit.</p></li>
<li><p><strong>Coverage failure:</strong> add targeted labels in
uncovered <span class="math inline">\(S\)</span>/<span
class="math inline">\(U\)</span> bins or narrow scope to the intended
deployment slice.</p></li>
<li><p><strong>Judge drift (over time):</strong> treat as a new era;
gather a fresh calibration slice and refit (see §<a
href="#sec:calib-refresh" data-reference-type="ref"
data-reference="sec:calib-refresh">[sec:calib-refresh]</a>).</p></li>
</ul>
</section>
</section>
<section id="off-policy-diagnostics-ipsdr-only"
class="level2 unnumbered">
<h2 class="unnumbered">Off-Policy Diagnostics (IPS/DR Only)</h2>
<p>The following diagnostics only apply when reusing logged data.
<strong>If you’re using Direct Mode, you can skip to §4.9.</strong></p>
</section>
<section id="diagnostic-6-effective-sample-size-ess" class="level2">
<h2>Diagnostic 6: Effective sample size (ESS)</h2>
<section id="what-it-measures.-5" class="level4">
<h4>What it measures.</h4>
<p>The number of “effective” independent samples after importance
weighting: <span class="math display">\[\text{ESS} =
\frac{\big(\sum_{i=1}^n \tilde{w}_i\big)^2}{\sum_{i=1}^n
\tilde{w}_i^2},\]</span> where <span
class="math inline">\(\tilde{w}_i\)</span> are the stabilized, mean-one
weights after . ESS ranges from 1 (one weight dominates) to <span
class="math inline">\(n\)</span> (all weights equal).</p>
</section>
<section id="why-it-matters.-5" class="level4">
<h4>Why it matters.</h4>
<p>Low ESS means a few samples dominate the estimate, leading to high
variance and unreliable CIs. ESS is the primary diagnostic for
off-policy viability.</p>
</section>
<section id="artifacts-to-inspect-5" class="level4">
<h4>Artifacts to inspect:</h4>
<ul>
<li><p><strong>ESS (absolute and %)</strong>: report both raw ESS and
ESS<span class="math inline">\(/n\)</span>.</p></li>
<li><p><strong>Weight distribution:</strong> histogram or quantiles
(min, median, 95th percentile, max) before and after .</p></li>
<li><p><strong>Weight vs. <span class="math inline">\(S\)</span>
scatter:</strong> check if weights are monotone in <span
class="math inline">\(S\)</span> (validates J2-M assumption).</p></li>
</ul>
</section>
<section id="thresholds-5" class="level4">
<h4>Thresholds:</h4>
<ul>
<li><p><strong>Pass:</strong> ESS <span class="math inline">\(\ge
30\%\)</span> of <span class="math inline">\(n\)</span> (excellent
overlap).</p></li>
<li><p><strong>Warn:</strong> ESS <span class="math inline">\(\in [10\%,
30\%)\)</span> (moderate overlap; recommended).</p></li>
<li><p><strong>Fail:</strong> ESS <span class="math inline">\(&lt;
10\%\)</span> (poor overlap; estimates unreliable unless with strong
outcome model).</p></li>
<li><p><strong>Refuse:</strong> ESS <span class="math inline">\(&lt;
1\%\)</span> (catastrophic; do not trust any estimate).</p></li>
</ul>
</section>
<section id="fixes-5" class="level4">
<h4>Fixes:</h4>
<ul>
<li><p>Use with aggressive variance cap <span
class="math inline">\(\rho\)</span> (e.g., 0.90).</p></li>
<li><p>Restrict to a high-overlap cohort (e.g., prompts where <span
class="math inline">\(|\log \tilde{w}_i| &lt; 2\)</span>).</p></li>
<li><p>Switch from to with a strong outcome model.</p></li>
<li><p>If ESS <span class="math inline">\(&lt; 1\%\)</span>, regenerate
fresh outputs for the candidate policy and use instead.</p></li>
</ul>
</section>
</section>
<section id="diagnostic-7-tail-heaviness-hill-index" class="level2">
<h2>Diagnostic 7: Tail heaviness (Hill index)</h2>
<section id="what-it-measures.-6" class="level4">
<h4>What it measures.</h4>
<p>The tail index <span class="math inline">\(\alpha\)</span> of the
weight distribution, estimated via the Hill estimator. If <span
class="math inline">\(\alpha &lt; 2\)</span>, the weights have infinite
variance; if <span class="math inline">\(\alpha &lt; 1\)</span>,
infinite mean.</p>
</section>
<section id="why-it-matters.-6" class="level4">
<h4>Why it matters.</h4>
<p>Heavy-tailed weights break standard inference. Even if ESS looks
reasonable, <span class="math inline">\(\alpha &lt; 2\)</span> means
variance estimates are unreliable and CIs can be arbitrarily wrong.</p>
</section>
<section id="artifacts-to-inspect-6" class="level4">
<h4>Artifacts to inspect:</h4>
<ul>
<li><p><strong>Hill index <span
class="math inline">\(\alpha\)</span>:</strong> estimated from the upper
tail of <span class="math inline">\(\tilde{w}\)</span> (post-
weights).</p></li>
<li><p><strong>QQ-plot:</strong> compare weight quantiles to
Pareto(<span class="math inline">\(\alpha\)</span>) reference.</p></li>
<li><p><strong>Max/median ratio:</strong> if <span
class="math inline">\(&gt; 100\)</span>, likely heavy-tailed.</p></li>
</ul>
</section>
<section id="thresholds-6" class="level4">
<h4>Thresholds:</h4>
<ul>
<li><p><strong>Pass:</strong> <span class="math inline">\(\alpha \ge
3\)</span> (well-behaved tails).</p></li>
<li><p><strong>Warn:</strong> <span class="math inline">\(\alpha \in [2,
3)\)</span> (finite variance, but higher moments unstable).</p></li>
<li><p><strong>Fail:</strong> <span class="math inline">\(\alpha &lt;
2\)</span> (infinite variance; estimates unstable).</p></li>
<li><p><strong>Critical:</strong> <span class="math inline">\(\alpha
&lt; 1\)</span> (infinite mean; nonsensical).</p></li>
</ul>
</section>
<section id="fixes-6" class="level4">
<h4>Fixes:</h4>
<ul>
<li><p>Apply with a strict variance cap <span class="math inline">\(\rho
&lt; 0.95\)</span>.</p></li>
<li><p>Cohort restriction to high-overlap prompts.</p></li>
<li><p>Check for provider drift (temperature, frequency penalty) that
might cause unexpected tail behavior.</p></li>
<li><p>If <span class="math inline">\(\alpha &lt; 2\)</span> persists,
switch to or regenerate.</p></li>
</ul>
</section>
</section>
<section id="diagnostic-8-dr-orthogonality-score" class="level2">
<h2>Diagnostic 8: DR orthogonality score</h2>
<section id="what-it-measures.-7" class="level4">
<h4>What it measures.</h4>
<p>The empirical moment <span class="math display">\[\text{OrthoScore} =
\frac{1}{n} \sum_{i=1}^n \tilde{w}_i \cdot (R_i -
\hat{g}(S_i)),\]</span> with a 95% CI, where <span
class="math inline">\(\tilde{w}_i\)</span> are the stabilized, mean-one
weights after . Under ideal conditions (mean-one weights and
well-specified outcome model), this should be near zero.</p>
</section>
<section id="why-it-matters.-7" class="level4">
<h4>Why it matters.</h4>
<p>Orthogonality is the key to doubly robust inference. If the
orthogonality score is far from zero, either the weights or the outcome
model (or both) are poor, and loses its efficiency and robustness
guarantees.</p>
</section>
<section id="artifacts-to-inspect-7" class="level4">
<h4>Artifacts to inspect:</h4>
<ul>
<li><p><strong>Orthogonality score with CI:</strong> point estimate
<span class="math inline">\(\pm 1.96 \cdot \SE\)</span>.</p></li>
<li><p><strong>Residual plot:</strong> <span class="math inline">\((R_i
- \hat{g}(S_i))\)</span> vs. <span
class="math inline">\(\tilde{w}_i\)</span> to spot patterns.</p></li>
<li><p><strong>Outcome model performance:</strong> OOF <span
class="math inline">\(R^2\)</span> or MAE on fresh draws.</p></li>
</ul>
</section>
<section id="thresholds-7" class="level4">
<h4>Thresholds:</h4>
<ul>
<li><p><strong>Pass:</strong> 95% CI contains zero, <span
class="math inline">\(|\)</span>OrthoScore<span class="math inline">\(|
&lt; 0.05\)</span>.</p></li>
<li><p><strong>Warn:</strong> CI contains zero, but <span
class="math inline">\(|\)</span>OrthoScore<span class="math inline">\(|
\in [0.05, 0.10]\)</span>.</p></li>
<li><p><strong>Fail:</strong> CI does not contain zero or <span
class="math inline">\(|\)</span>OrthoScore<span class="math inline">\(|
&gt; 0.10\)</span>.</p></li>
</ul>
</section>
<section id="fixes-7" class="level4">
<h4>Fixes:</h4>
<ul>
<li><p>Improve the outcome model: add regularization, use a more
flexible model (e.g., two-stage isotonic), or increase fresh draw sample
size.</p></li>
<li><p>Revisit settings or overlap diagnostics (poor weights can break
orthogonality).</p></li>
<li><p>Check cross-fitting: ensure folds are balanced and
representative.</p></li>
<li><p>If orthogonality fails persistently, fall back to or regenerate
for .</p></li>
</ul>
</section>
</section>
<section id="traffic-light-summary-and-refusal-gates" class="level2">
<h2>Traffic-light summary and refusal gates</h2>
<p>CJE estimators can optionally enforce <strong>refusal gates</strong>:
if a diagnostic crosses a critical threshold, the estimator returns
<code>NaN</code> instead of a potentially misleading number.</p>
<section id="unified-diagnostic-thresholds." class="level4">
<h4>Unified diagnostic thresholds.</h4>
<p>Table <a href="#tab:thresholds" data-reference-type="ref"
data-reference="tab:thresholds">1</a> summarizes all diagnostic
thresholds in one place. Use this as the single source of truth for
interpreting diagnostic outputs.</p>
<div id="tab:thresholds">
<table>
<caption>Unified diagnostic thresholds for CJE</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Diagnostic</strong></th>
<th style="text-align: center;"><strong>PASS</strong></th>
<th style="text-align: center;"><strong>WARN</strong></th>
<th style="text-align: center;"><strong>FAIL</strong></th>
<th style="text-align: center;"><strong>REFUSE</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">ESS / <span
class="math inline">\(n\)</span> (post-)</td>
<td style="text-align: center;"><span class="math inline">\(\ge
30\%\)</span></td>
<td style="text-align: center;"><span class="math inline">\([10\%,
30\%)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&lt;
10\%\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&lt;
1\%\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Hill index <span
class="math inline">\(\alpha\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\ge
3\)</span></td>
<td style="text-align: center;"><span class="math inline">\([2,
3)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&lt;
2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&lt;
1\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Score coverage</td>
<td style="text-align: center;"><span class="math inline">\(\ge
95\%\)</span></td>
<td style="text-align: center;"><span class="math inline">\([85\%,
95\%)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&lt;
85\%\)</span></td>
<td style="text-align: center;">—</td>
</tr>
<tr class="even">
<td style="text-align: left;">Calibration OOF MAE</td>
<td style="text-align: center;"><span class="math inline">\(&lt;
0.05\)</span></td>
<td style="text-align: center;"><span class="math inline">\([0.05,
0.10]\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&gt;
0.10\)</span></td>
<td style="text-align: center;">—</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mean preservation</td>
<td style="text-align: center;"><span class="math inline">\(&lt;
0.02\)</span></td>
<td style="text-align: center;"><span class="math inline">\([0.02,
0.05]\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&gt;
0.05\)</span></td>
<td style="text-align: center;">—</td>
</tr>
<tr class="even">
<td style="text-align: left;">DR orthogonality <span
class="math inline">\(|\)</span>score<span
class="math inline">\(|\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&lt;
0.05\)</span></td>
<td style="text-align: center;"><span class="math inline">\([0.05,
0.10]\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&gt;
0.10\)</span></td>
<td style="text-align: center;">—</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(CI must contain 0)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span
class="math inline">\(\times\)</span></td>
<td style="text-align: center;">—</td>
</tr>
<tr class="even">
<td style="text-align: left;">OUA share</td>
<td style="text-align: center;"><span class="math inline">\(&lt;
20\%\)</span></td>
<td style="text-align: center;"><span class="math inline">\([20\%,
50\%]\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&gt;
50\%\)</span></td>
<td style="text-align: center;">—</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Judge stability <span
class="math inline">\(\tau\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\ge
0.90\)</span></td>
<td style="text-align: center;"><span class="math inline">\([0.75,
0.90)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(&lt;
0.75\)</span></td>
<td style="text-align: center;">—</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="default-refusal-gates" class="level4">
<h4>Default refusal gates:</h4>
<ul>
<li><p>ESS <span class="math inline">\(&lt; 1\%\)</span> <span
class="math inline">\(\to\)</span> refuse to estimate (off-policy
only).</p></li>
<li><p>Hill index <span class="math inline">\(\alpha &lt; 2\)</span>
<span class="math inline">\(\to\)</span> refuse (off-policy
only).</p></li>
<li><p>Coverage <span class="math inline">\(&lt; 50\%\)</span> <span
class="math inline">\(\to\)</span> warn strongly (all modes).</p></li>
</ul>
<p>Operators can adjust these gates or disable them for exploratory
analysis, but the default is to fail loudly rather than produce
garbage.</p>
</section>
</section>
<section id="diagnostic-workflow-flowchart" class="level2">
<h2>Diagnostic workflow (flowchart)</h2>
<section id="core-diagnostics-all-modes-1" class="level4">
<h4>Core diagnostics (all modes):</h4>
<ol>
<li><p><strong>Check score coverage.</strong> If <span
class="math inline">\(&lt; 85\%\)</span>, add labels or narrow scope. If
pass, continue.</p></li>
<li><p><strong>Check calibration reliability.</strong> If regional error
is high, enable two-stage or add labels. If pass, continue.</p></li>
<li><p><strong>Check judge stability.</strong> If <span
class="math inline">\(\tau &lt; 0.75\)</span>, freeze judge config and
refresh oracle. If pass, continue.</p></li>
<li><p><strong>Check OUA share.</strong> If <span
class="math inline">\(&gt; 50\%\)</span>, add labels. If pass, you’re
done (for Direct Mode).</p></li>
</ol>
</section>
<section id="additional-checks-for-off-policy-ipsdr" class="level4">
<h4>Additional checks for off-policy (IPS/DR):</h4>
<ol>
<li><p><strong>Check ESS.</strong> If <span class="math inline">\(&lt;
30\%\)</span> (WARN), review diagnostics and consider . If <span
class="math inline">\(&lt; 10\%\)</span> (FAIL), use or switch to . If
<span class="math inline">\(&lt; 1\%\)</span> (REFUSE), regenerate. If
pass, continue.</p></li>
<li><p><strong>Check tail index.</strong> If <span
class="math inline">\(\alpha &lt; 2\)</span>, apply stricter or cohort
restriction. If pass, continue.</p></li>
<li><p><strong>(DR only) Check orthogonality.</strong> If CI excludes
zero, improve outcome model or weights. If pass, trust the
estimate.</p></li>
</ol>
<p>If any diagnostic fails and the fix is not immediately available,
report the failure and do not ship the estimate.</p>
</section>
</section>
<section id="visualization-gallery-artifacts" class="level2">
<h2>Visualization gallery (artifacts)</h2>
<p>For full audit trails, CJE produces:</p>
<ul>
<li><p><strong>Calibration curve:</strong> <span
class="math inline">\(f(S)\)</span> vs. <span
class="math inline">\(Y\)</span> with OOF fit and mean-preservation
line.</p></li>
<li><p><strong>Coverage histogram:</strong> oracle <span
class="math inline">\(S\)</span> vs. evaluation <span
class="math inline">\(S\)</span> distributions.</p></li>
<li><p><strong>Weight distribution:</strong> before/after , with ESS and
Hill index annotations.</p></li>
<li><p><strong>Orthogonality residual plot:</strong> <span
class="math inline">\((R - \hat{g}(S))\)</span> vs. <span
class="math inline">\(\tilde{w}\)</span> with CI band.</p></li>
<li><p><strong>OUA variance decomposition:</strong> stacked bar showing
<span class="math inline">\(\Var_{\text{main}}\)</span> vs. <span
class="math inline">\(\Var_{\oua}\)</span>.</p></li>
</ul>
<p>These plots are auto-generated by the CJE visualization module and
can be included in reports or dashboards.</p>
</section>
<section id="example-interpreting-a-diagnostic-report" class="level2">
<h2>Example: interpreting a diagnostic report</h2>
<pre><code>=== CJE Diagnostic Report ===
Mode: Calibrated DR
Estimator: stacked-dr
Target policy: gpt-4-mini
Baseline policy: gpt-3.5-turbo

== Core Diagnostics ==
[1] Score Coverage: 92% (PASS)
    Oracle S range: [2.1, 9.8]
    Eval S range: [1.9, 9.9]
    Boundary slopes: moderate

[2] Calibration Reliability: OOF MAE = 0.04 (PASS)
    Mean preservation: -0.01 (excellent)
    Regional MAE: low=0.03, mid=0.04, high=0.05 (balanced)

[3] Judge Stability: tau = 0.93 (PASS)
    Temporal batches: 5
    No drift detected across time periods

[4] OUA Share: 15% (PASS)
    Labels sufficient; variance dominated by prompt variation

== Off-Policy Diagnostics ==
[5] ESS: 342 / 1000 = 34% (WARN)
    Raw ESS: 18%
    Post-SIMCal ESS: 34%
    Recommendation: acceptable for DR; consider cohort restriction if CI too wide

[6] Tail Index: alpha = 2.8 (PASS)
    Max/median weight ratio: 12.3
    Tail behavior: well-behaved

[7] DR Orthogonality: 0.03 [-0.02, 0.08] (PASS)
    CI contains zero
    Outcome model OOF R^2: 0.67

Overall: PASS (with ESS warning)
Recommendation: Estimate is reliable. Consider adding fresh draws or cohort restriction for tighter CIs.</code></pre>
</section>
<section id="uncertainty-quantification-cluster-robust-oua"
class="level2">
<h2>Uncertainty quantification: cluster-robust + OUA</h2>
<section id="two-component-variance-structure." class="level4">
<h4>Two-component variance structure.</h4>
<p>CJE’s confidence intervals properly account for <em>two independent
sources of uncertainty</em>:</p>
<ol>
<li><p><strong>Main sampling uncertainty</strong> (from the eval
log/prompts) <span class="math inline">\(\to\)</span>
<strong>cluster-robust SEs</strong>.</p></li>
<li><p><strong>Calibrator uncertainty</strong> (from the oracle slice)
<span class="math inline">\(\to\)</span>
<strong>jackknife</strong>.</p></li>
</ol>
<p>Under the product-sample setup (oracle and eval are independent
draws), these components are <strong>additive</strong>: <span
class="math display">\[\widehat{\Var}_{\text{total}} =
\widehat{\Var}_{\text{CR}} + \widehat{\Var}_{\oua}.\]</span></p>
</section>
<section id="why-it-matters.-8" class="level4">
<h4>Why it matters.</h4>
<p>Ignoring clustering on the eval side can cause severe undercoverage
(e.g., 86.9% instead of 95% observed empirically), while ignoring
calibrator uncertainty understates risk when oracle slices are small.
Both components are essential for honest inference.</p>
</section>
<section id="component-1-cluster-robust-variance-eval-side-dependence."
class="level4">
<h4>Component 1: Cluster-robust variance (eval-side dependence).</h4>
<p>Standard i.i.d. SEs assume independent samples. When evaluation rows
cluster—e.g., multiple prompts per user/session, time blocks,
conversation threads, or paired DM contrasts—the <em>effective</em>
sample size is closer to the number of clusters <span
class="math inline">\(G\)</span>, not the number of rows <span
class="math inline">\(n\)</span>.</p>
<p><strong>What to cluster:</strong> any dependence structure in your
eval data (user, session, time block, paired design).</p>
<p>All CJE estimators are means of per-row <strong>influence
contributions</strong> <span class="math inline">\(\psi_i\)</span>:
<span class="math display">\[\begin{aligned}
\text{DM level:} \quad &amp;\psi_i = f(S_i^\pi) - \hat{V}, \\
\text{DM paired contrast:} \quad &amp;\psi_i = \bigl[f(S_i^\pi) -
f(S_i^{\pi&#39;})\bigr] - \widehat{\Delta}, \\
\text{IPS:} \quad &amp;\psi_i = \tilde{w}_i R_i - \hat{V}, \\
\text{DR:} \quad &amp;\psi_i = \hat{g}_\pi(X_i) + \tilde{w}_i\bigl(R_i -
\hat{q}(X_i, A_i^0)\bigr) - \hat{V}.
\end{aligned}\]</span></p>
<p>Aggregate to cluster sums <span class="math inline">\(\Psi_g =
\sum_{i:\, c(i)=g} \psi_i\)</span>, then compute the
small-sample–corrected cluster-robust variance: <span
class="math display">\[\widehat{\Var}_{\text{CR}} = \frac{G}{G-1} \cdot
\frac{1}{n^2} \sum_{g=1}^G \Psi_g^2.\]</span></p>
<p><strong>Edge cases:</strong></p>
<ul>
<li><p><strong>Few clusters (<span class="math inline">\(G &lt;
15\)</span>):</strong> prefer wild-cluster bootstrap over CR1.</p></li>
<li><p><strong>Two-way dependence (e.g., user <span
class="math inline">\(\times\)</span> day):</strong> use two-way
clustering formula.</p></li>
<li><p><strong>Time series:</strong> use moving-block bootstrap with
block length <span class="math inline">\(\approx
n^{1/3}\)</span>.</p></li>
</ul>
</section>
<section id="component-2-oracle-uncertainty-aware-oua-jackknife."
class="level4">
<h4>Component 2: Oracle-uncertainty aware (OUA) jackknife.</h4>
<p>When the oracle slice is small (5–10% coverage is common), treating
the calibrator <span class="math inline">\(f\)</span> as fixed
drastically understates total uncertainty. properly accounts for
this:</p>
<ol>
<li><p>Split oracle labels into <span class="math inline">\(K\)</span>
folds (same folds used for cross-fitting).</p></li>
<li><p>For each fold <span class="math inline">\(k\)</span>:</p>
<ul>
<li><p>Refit calibrator <span class="math inline">\(f^{(-k)}\)</span> on
oracle <span class="math inline">\(\setminus\)</span> fold <span
class="math inline">\(k\)</span> (let auto mode selection re-decide mono
vs. two-stage).</p></li>
<li><p>Recompute calibrated rewards <span class="math inline">\(R^{(-k)}
= f^{(-k)}(S)\)</span>.</p></li>
<li><p><strong>For IPS/DR:</strong> re-select weights (selection depends
on <span class="math inline">\(R\)</span>).</p></li>
<li><p><strong>For DR:</strong> refit outcome model (depends on <span
class="math inline">\(R\)</span>).</p></li>
<li><p>Compute point estimate <span
class="math inline">\(\hat{\theta}^{(-k)}\)</span>.</p></li>
</ul></li>
<li><p>Jackknife variance: <span
class="math display">\[\widehat{\Var}_{\oua} = \frac{K-1}{K}
\sum_{k=1}^K \bigl(\hat{\theta}^{(-k)} - \bar{\theta}\bigr)^2, \quad
\bar{\theta} = \frac{1}{K}\sum_k \hat{\theta}^{(-k)}.\]</span></p></li>
</ol>
<p><strong>Key principle:</strong> captures <em>oracle-only</em>
uncertainty. It holds the eval log fixed and only refits components that
depend on calibrator outputs. This maintains independence with <span
class="math inline">\(\widehat{\Var}_{\text{CR}}\)</span>.</p>
</section>
<section id="combining-the-components-and-critical-values."
class="level4">
<h4>Combining the components and critical values.</h4>
<p>Because oracle and eval are independent, the cross-term vanishes
asymptotically: <span class="math display">\[\SE_{\text{total}} =
\sqrt{\widehat{\Var}_{\text{CR}} + \widehat{\Var}_{\oua}}.\]</span></p>
<p><strong>Critical value:</strong></p>
<ul>
<li><p><strong>Large samples (<span class="math inline">\(G \ge
30\)</span>, <span class="math inline">\(K \ge 5\)</span>):</strong> use
normal quantile (1.96 for 95% CI).</p></li>
<li><p><strong>Small clusters or oracle:</strong> use Satterthwaite
effective degrees of freedom: <span
class="math display">\[\text{df}_{\text{eff}} =
\frac{\bigl(\widehat{\Var}_{\text{CR}} + \widehat{\Var}_{\oua}\bigr)^2}
{\widehat{\Var}_{\text{CR}}^2/\text{df}_{\text{CR}} +
\widehat{\Var}_{\oua}^2/\text{df}_{\oua}}\]</span> where <span
class="math inline">\(\text{df}_{\text{CR}} = G-1\)</span> and <span
class="math inline">\(\text{df}_{\oua} = K-1\)</span>. Then use <span
class="math inline">\(t_{1-\alpha/2,
\text{df}_{\text{eff}}}\)</span>.</p></li>
</ul>
</section>
<section id="what-to-report." class="level4">
<h4>What to report.</h4>
<p>For complete transparency, always include:</p>
<ul>
<li><p>Point estimate with 95% CI using <span
class="math inline">\(\SE_{\text{total}}\)</span>.</p></li>
<li><p><strong>share</strong>: <span
class="math inline">\(\widehat{\Var}_{\oua}/\widehat{\Var}_{\text{total}}\)</span>
(shows which component dominates).</p></li>
<li><p>Cluster structure: number of clusters <span
class="math inline">\(G\)</span>, cluster size distribution.</p></li>
<li><p>Comparison: i.i.d. SE vs. cluster-robust SE (shows dependence
penalty).</p></li>
</ul>
</section>
<section id="quick-diagnostic." class="level4">
<h4>Quick diagnostic.</h4>
<p>If share <span class="math inline">\(&gt; 50\%\)</span>, labels are
the bottleneck—add more oracle labels. If cluster-robust SE is much
larger than i.i.d. SE (e.g., 30%+ inflation), dependence is
substantial—ensure you’ve identified the right cluster definition. If
you’re using paired DM contrasts, <em>always</em> use cluster-robust
SEs; pairing reduces variance but often induces strong within-prompt
correlation.</p>
</section>
</section>
<section id="summary" class="level2">
<h2>Summary</h2>
<p>The six diagnostics are organized into two groups:</p>
<p><strong>Core diagnostics (all modes):</strong> Coverage (§4.1),
calibration reliability (§4.2), judge stability (§4.3), and OUA share
(§4.4) apply to all evaluation modes. These catch fundamental validity
issues like extrapolation, miscalibration, judge drift, and label
scarcity.</p>
<p><strong>Off-policy diagnostics (IPS/DR only):</strong> ESS (§4.5),
tail heaviness (§4.6), and DR orthogonality (§4.7) only apply when
reusing logged data. These diagnose overlap, weight behavior, and critic
quality.</p>
<p>Each diagnostic produces a compact artifact, a traffic-light signal,
and a concrete fix. Always inspect diagnostics before trusting
estimates, and always report them alongside results.</p>
</section>
</section>
