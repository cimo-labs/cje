<section id="case-studies" class="level1">
<h1>Case Studies</h1>
<section id="overview" class="level2">
<h2>Overview</h2>
<p>This section presents three compact case studies illustrating CJE in
practice: (1) a clean DM comparison, (2) an off-policy IPS evaluation
with overlap challenges, and (3) a DR analysis combining logged data
with fresh draws.</p>
</section>
<section id="case-1-dm-for-model-selection-on-policy" class="level2">
<h2>Case 1: DM for model selection (on-policy)</h2>
<section id="setting." class="level4">
<h4>Setting.</h4>
<p>A team is choosing between three candidate models for a
code-generation task: GPT-3.5, GPT-4, and Claude-3. They have 1,000
diverse coding prompts and can generate one output per model per prompt.
The KPI is “correctness” (binary: code passes unit tests or not). They
use GPT-4-as-judge to score outputs on a 1–10 scale.</p>
</section>
<section id="data-collection." class="level4">
<h4>Data collection.</h4>
<ul>
<li><p>Generate outputs: 3 models <span
class="math inline">\(\times\)</span> 1,000 prompts <span
class="math inline">\(=\)</span> 3,000 responses.</p></li>
<li><p>Judge all 3,000 with GPT-4, get scores <span
class="math inline">\(S \in [1, 10]\)</span>.</p></li>
<li><p>Sample 150 responses uniformly, run unit tests to get ground
truth <span class="math inline">\(Y \in \{0, 1\}\)</span>.</p></li>
</ul>
</section>
<section id="calibration." class="level4">
<h4>Calibration.</h4>
<ul>
<li><p>Fit (5-fold isotonic regression) on the 150-sample oracle
slice.</p></li>
<li><p>Diagnostics: Coverage 96% (pass), OOF MAE 0.03 (pass), mean
preservation <span class="math inline">\(|\bar{f(S)} - \bar{Y}| =
0.01\)</span> (excellent).</p></li>
<li><p>Map all 3,000 scores to calibrated rewards <span
class="math inline">\(R = f(S) \in [0, 1]\)</span>.</p></li>
</ul>
</section>
<section id="estimates-with-." class="level4">
<h4>Estimates (with ).</h4>
<p><span class="math display">\[\begin{aligned}
\est{V}_{\dm}(\text{GPT-3.5}) &amp;= 0.62 \; [0.58, 0.66] \\
\est{V}_{\dm}(\text{GPT-4}) &amp;= 0.78 \; [0.75, 0.81] \\
\est{V}_{\dm}(\text{Claude-3}) &amp;= 0.81 \; [0.78, 0.84]
\end{aligned}\]</span> Paired contrasts (tighter CIs due to pairing):
<span class="math display">\[\begin{aligned}
\est{\Delta}(\text{GPT-4}, \text{GPT-3.5}) &amp;= +0.16 \; [0.11, 0.21]
\\
\est{\Delta}(\text{Claude-3}, \text{GPT-4}) &amp;= +0.03 \; [-0.01,
0.07]
\end{aligned}\]</span></p>
</section>
<section id="diagnostics." class="level4">
<h4>Diagnostics.</h4>
<ul>
<li><p>share: 12% (labels sufficient, variance dominated by prompt
variation).</p></li>
<li><p>All diagnostics pass.</p></li>
</ul>
</section>
<section id="conclusion." class="level4">
<h4>Conclusion.</h4>
<p>Claude-3 and GPT-4 are statistically indistinguishable; both
substantially outperform GPT-3.5. Team chooses Claude-3 for deployment
based on cost.</p>
</section>
</section>
<section id="case-2-ips-with-overlap-challenges-off-policy"
class="level2">
<h2>Case 2: IPS with overlap challenges (off-policy)</h2>
<section id="setting.-1" class="level4">
<h4>Setting.</h4>
<p>A team deployed GPT-3.5-turbo in production for a month, logging
10,000 prompts with responses, judge scores, and logprobs. They now want
to estimate what the KPI would have been if they had deployed
GPT-4-turbo instead, without regenerating.</p>
</section>
<section id="data." class="level4">
<h4>Data.</h4>
<ul>
<li><p>Logged data: <span class="math inline">\(n = 10{,}000\)</span>
with <span class="math inline">\((X_i, A_i, S_i, \log \pi_0(A_i \mid
X_i), \log \pi&#39;(A_i \mid X_i))\)</span>.</p></li>
<li><p>Oracle slice: 200 samples with ground truth <span
class="math inline">\(Y\)</span> (user satisfaction, binary).</p></li>
<li><p>No fresh draws (cannot generate for GPT-4-turbo on these old
prompts).</p></li>
</ul>
</section>
<section id="calibration.-1" class="level4">
<h4>Calibration.</h4>
<ul>
<li><p>Fit on 200-sample oracle.</p></li>
<li><p>Diagnostics: Coverage 89% (warn), OOF MAE 0.06 (pass), mean
preservation OK.</p></li>
<li><p>Action: Add 20 labels targeting low-<span
class="math inline">\(S\)</span> bins to improve coverage to
94%.</p></li>
</ul>
</section>
<section id="weights-and-stabilization." class="level4">
<h4>Weights and stabilization.</h4>
<ul>
<li><p>Raw weights: <span class="math inline">\(W_i = \exp(\log
\pi&#39;(A_i \mid X_i) - \log \pi_0(A_i \mid X_i))\)</span>.</p></li>
<li><p>Raw ESS: 8% (poor overlap; GPT-4-turbo is quite different from
GPT-3.5-turbo).</p></li>
<li><p>Apply with <span class="math inline">\(\rho = 0.95\)</span>:
stabilized ESS improves to 18%.</p></li>
<li><p>Hill index <span class="math inline">\(\alpha = 2.6\)</span>
(pass, finite variance).</p></li>
</ul>
</section>
<section id="estimate." class="level4">
<h4>Estimate.</h4>
<p><span class="math display">\[\begin{aligned}
\est{V}_{\ips}(\text{GPT-4-turbo}) &amp;= 0.74 \; [0.68, 0.80] \\
\text{(Baseline GPT-3.5-turbo)} &amp;= 0.65 \; [0.62, 0.68] \\
\est{\Delta} &amp;= +0.09 \; [0.02, 0.16]
\end{aligned}\]</span></p>
</section>
<section id="diagnostics.-1" class="level4">
<h4>Diagnostics.</h4>
<ul>
<li><p>ESS 18% (warn, but acceptable given no fresh draws
available).</p></li>
<li><p>share 22% (warn; labels are becoming a bottleneck).</p></li>
<li><p>Weight max/median ratio: 45 (moderate tail).</p></li>
</ul>
</section>
<section id="sensitivity-check." class="level4">
<h4>Sensitivity check.</h4>
<ul>
<li><p>Cohort restriction: Re-run on prompts with <span
class="math inline">\(|\log W| &lt; 2\)</span> (<span
class="math inline">\(n = 6{,}000\)</span>). ESS improves to 35%,
estimate is <span class="math inline">\(0.76 \; [0.71, 0.81]\)</span>
(consistent).</p></li>
<li><p>Trimming: Drop top 1% of weights. Estimate shifts to <span
class="math inline">\(0.73 \; [0.68, 0.78]\)</span> (minor change,
robust).</p></li>
</ul>
</section>
<section id="conclusion.-1" class="level4">
<h4>Conclusion.</h4>
<p>Switching to GPT-4-turbo would likely improve satisfaction by <span
class="math inline">\(\approx 9\)</span> percentage points. The estimate
is moderately reliable (ESS 18%, sensitivity checks agree). Team decides
to run a small A/B test to confirm before full deployment.</p>
</section>
</section>
<section id="case-3-dr-for-tight-cis-off-policy-fresh-draws"
class="level2">
<h2>Case 3: DR for tight CIs (off-policy + fresh draws)</h2>
<section id="setting.-2" class="level4">
<h4>Setting.</h4>
<p>Same as Case 2, but the team can now afford to generate 2,000 fresh
responses from GPT-4-turbo (20% of the logged data) to train a critic
for DR.</p>
</section>
<section id="data.-1" class="level4">
<h4>Data.</h4>
<ul>
<li><p>Logged data: <span class="math inline">\(n = 10{,}000\)</span>
(same as Case 2).</p></li>
<li><p>Oracle slice: 200 (same).</p></li>
<li><p>Fresh draws: 2,000 prompts sampled uniformly from the 10k, with
GPT-4-turbo outputs and judge scores.</p></li>
</ul>
</section>
<section id="weights-and-calibration." class="level4">
<h4>Weights and calibration.</h4>
<p>Same as Case 2 (ESS 18% after , <span class="math inline">\(\alpha =
2.6\)</span>).</p>
</section>
<section id="critic-training." class="level4">
<h4>Critic training.</h4>
<ul>
<li><p>Train a gradient-boosted tree <span
class="math inline">\(\hat{g}(X)\)</span> on the 2,000 fresh draws
(5-fold cross-fitted) to predict calibrated reward <span
class="math inline">\(R\)</span>.</p></li>
<li><p>OOF <span class="math inline">\(R^2 = 0.58\)</span> (moderate;
prompt features include length, first-token embedding, topic
cluster).</p></li>
</ul>
</section>
<section id="dr-estimate." class="level4">
<h4>DR estimate.</h4>
<p><span class="math display">\[\begin{aligned}
\est{V}_{\dr}(\text{GPT-4-turbo}) &amp;= 0.75 \; [0.71, 0.79]
\end{aligned}\]</span></p>
</section>
<section id="diagnostics.-2" class="level4">
<h4>Diagnostics.</h4>
<ul>
<li><p>Orthogonality score: <span class="math inline">\(0.02 \; [-0.03,
0.07]\)</span> (pass, CI contains zero).</p></li>
<li><p>ESS: still 18% (unchanged by DR; weights are the same).</p></li>
<li><p>share: 18% (slightly lower than IPS due to variance reduction
from the critic).</p></li>
<li><p>Compare to IPS: IPS gave <span class="math inline">\([0.68,
0.80]\)</span> (width 0.12); DR gives <span class="math inline">\([0.71,
0.79]\)</span> (width 0.08). DR is 33% tighter.</p></li>
</ul>
</section>
<section id="conclusion.-2" class="level4">
<h4>Conclusion.</h4>
<p>DR delivers a tighter CI and a more stable estimate. The point
estimate (0.75) is consistent with IPS (0.74) and the sensitivity
checks. The team is now confident enough to deploy GPT-4-turbo without
an A/B test.</p>
</section>
</section>
<section id="lessons-learned" class="level2">
<h2>Lessons learned</h2>
<ul>
<li><p><strong>DM is simplest when you can generate.</strong> Case 1
shows clean paired comparisons with tight CIs.</p></li>
<li><p><strong>IPS works when overlap is moderate.</strong> Case 2 shows
that ESS 18% is usable, especially with sensitivity checks.</p></li>
<li><p><strong>DR tightens CIs and boosts confidence.</strong> Case 3
shows a 33% CI reduction with only 2,000 fresh draws (20% of logged
data).</p></li>
<li><p><strong>Always check diagnostics.</strong> Coverage, ESS,
orthogonality, and share catch issues early.</p></li>
<li><p><strong>Sensitivity checks build trust.</strong> Cohort
restriction, trimming, and alternative calibrators confirm
robustness.</p></li>
</ul>
</section>
</section>
