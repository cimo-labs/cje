<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>07_cases</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/usr/share/javascript/mathjax/MathJax.js"
  type="text/javascript"></script>
</head>
<body>
<section id="case-studies" class="level1">
<h1>Case Studies</h1>
<section id="overview" class="level2">
<h2>Overview</h2>
<p>This section presents three compact case studies illustrating CJE in
practice: (1) a clean DM comparison, (2) an off-policy IPS evaluation
with overlap challenges, and (3) a DR analysis combining logged data
with fresh draws.</p>
</section>
<section id="case-1-dm-for-model-selection-on-policy" class="level2">
<h2>Case 1: DM for model selection (on-policy)</h2>
<section id="setting." class="level4">
<h4>Setting.</h4>
<p>A team is choosing between three candidate models for a
code-generation task: GPT-3.5, GPT-4, and Claude-3. They have 1,000
diverse coding prompts and can generate one output per model per prompt.
The KPI is “correctness” (binary: code passes unit tests or not). They
use GPT-4-as-judge to score outputs on a 1–10 scale.</p>
</section>
<section id="data-collection." class="level4">
<h4>Data collection.</h4>
<ul>
<li><p>Generate outputs: 3 models <span
class="math inline">\(\times\)</span> 1,000 prompts <span
class="math inline">\(=\)</span> 3,000 responses.</p></li>
<li><p>Judge all 3,000 with GPT-4, get scores <span
class="math inline">\(S \in [1, 10]\)</span>.</p></li>
<li><p>Sample 150 responses uniformly, run unit tests to get ground
truth <span class="math inline">\(Y \in \{0, 1\}\)</span>.</p></li>
</ul>
</section>
<section id="calibration." class="level4">
<h4>Calibration.</h4>
<ul>
<li><p>Fit (5-fold isotonic regression) on the 150-sample oracle
slice.</p></li>
<li><p>Diagnostics: Coverage 96% (pass), OOF MAE 0.03 (pass), mean
preservation <span class="math inline">\(|\bar{f(S)} - \bar{Y}| =
0.01\)</span> (excellent).</p></li>
<li><p>Map all 3,000 scores to calibrated rewards <span
class="math inline">\(R = f(S) \in [0, 1]\)</span>.</p></li>
</ul>
</section>
<section id="estimates-with-." class="level4">
<h4>Estimates (with ).</h4>
<p><span class="math display">\[\begin{aligned}
\est{V}_{\dm}(\text{GPT-3.5}) &amp;= 0.62 \; [0.58, 0.66] \\
\est{V}_{\dm}(\text{GPT-4}) &amp;= 0.78 \; [0.75, 0.81] \\
\est{V}_{\dm}(\text{Claude-3}) &amp;= 0.81 \; [0.78, 0.84]
\end{aligned}\]</span> Paired contrasts (tighter CIs due to pairing):
<span class="math display">\[\begin{aligned}
\est{\Delta}(\text{GPT-4}, \text{GPT-3.5}) &amp;= +0.16 \; [0.11, 0.21]
\\
\est{\Delta}(\text{Claude-3}, \text{GPT-4}) &amp;= +0.03 \; [-0.01,
0.07]
\end{aligned}\]</span></p>
</section>
<section id="diagnostics." class="level4">
<h4>Diagnostics.</h4>
<ul>
<li><p>share: 12% (labels sufficient, variance dominated by prompt
variation).</p></li>
<li><p>All diagnostics pass.</p></li>
</ul>
</section>
<section id="conclusion." class="level4">
<h4>Conclusion.</h4>
<p>Claude-3 and GPT-4 are statistically indistinguishable; both
substantially outperform GPT-3.5. Team chooses Claude-3 for deployment
based on cost.</p>
</section>
</section>
<section id="case-2-ips-with-overlap-challenges-off-policy"
class="level2">
<h2>Case 2: IPS with overlap challenges (off-policy)</h2>
<section id="setting.-1" class="level4">
<h4>Setting.</h4>
<p>A team deployed GPT-3.5-turbo in production for a month, logging
10,000 prompts with responses, judge scores, and logprobs. They now want
to estimate what the KPI would have been if they had deployed
GPT-4-turbo instead, without regenerating.</p>
</section>
<section id="data." class="level4">
<h4>Data.</h4>
<ul>
<li><p>Logged data: <span class="math inline">\(n = 10{,}000\)</span>
with <span class="math inline">\((X_i, A_i, S_i, \log \pi_0(A_i \mid
X_i), \log \pi&#39;(A_i \mid X_i))\)</span>.</p></li>
<li><p>Oracle slice: 200 samples with ground truth <span
class="math inline">\(Y\)</span> (user satisfaction, binary).</p></li>
<li><p>No fresh draws (cannot generate for GPT-4-turbo on these old
prompts).</p></li>
</ul>
</section>
<section id="calibration.-1" class="level4">
<h4>Calibration.</h4>
<ul>
<li><p>Fit on 200-sample oracle.</p></li>
<li><p>Diagnostics: Coverage 89% (warn), OOF MAE 0.06 (pass), mean
preservation OK.</p></li>
<li><p>Action: Add 20 labels targeting low-<span
class="math inline">\(S\)</span> bins to improve coverage to
94%.</p></li>
</ul>
</section>
<section id="weights-and-stabilization." class="level4">
<h4>Weights and stabilization.</h4>
<ul>
<li><p>Raw weights: <span class="math inline">\(W_i = \exp(\log
\pi&#39;(A_i \mid X_i) - \log \pi_0(A_i \mid X_i))\)</span>.</p></li>
<li><p>Raw ESS: 8% (poor overlap; GPT-4-turbo is quite different from
GPT-3.5-turbo).</p></li>
<li><p>Apply with <span class="math inline">\(\rho = 0.95\)</span>:
stabilized ESS improves to 18%.</p></li>
<li><p>Hill index <span class="math inline">\(\alpha = 2.6\)</span>
(pass, finite variance).</p></li>
</ul>
</section>
<section id="estimate." class="level4">
<h4>Estimate.</h4>
<p><span class="math display">\[\begin{aligned}
\est{V}_{\ips}(\text{GPT-4-turbo}) &amp;= 0.74 \; [0.68, 0.80] \\
\text{(Baseline GPT-3.5-turbo)} &amp;= 0.65 \; [0.62, 0.68] \\
\est{\Delta} &amp;= +0.09 \; [0.02, 0.16]
\end{aligned}\]</span></p>
</section>
<section id="diagnostics.-1" class="level4">
<h4>Diagnostics.</h4>
<ul>
<li><p>ESS 18% (warn, but acceptable given no fresh draws
available).</p></li>
<li><p>share 22% (warn; labels are becoming a bottleneck).</p></li>
<li><p>Weight max/median ratio: 45 (moderate tail).</p></li>
</ul>
</section>
<section id="sensitivity-check." class="level4">
<h4>Sensitivity check.</h4>
<ul>
<li><p>Cohort restriction: Re-run on prompts with <span
class="math inline">\(|\log W| &lt; 2\)</span> (<span
class="math inline">\(n = 6{,}000\)</span>). ESS improves to 35%,
estimate is <span class="math inline">\(0.76 \; [0.71, 0.81]\)</span>
(consistent).</p></li>
<li><p>Trimming: Drop top 1% of weights. Estimate shifts to <span
class="math inline">\(0.73 \; [0.68, 0.78]\)</span> (minor change,
robust).</p></li>
</ul>
</section>
<section id="conclusion.-1" class="level4">
<h4>Conclusion.</h4>
<p>Switching to GPT-4-turbo would likely improve satisfaction by <span
class="math inline">\(\approx 9\)</span> percentage points. The estimate
is moderately reliable (ESS 18%, sensitivity checks agree). Team decides
to run a small A/B test to confirm before full deployment.</p>
</section>
</section>
<section id="case-3-dr-for-tight-cis-off-policy-fresh-draws"
class="level2">
<h2>Case 3: DR for tight CIs (off-policy + fresh draws)</h2>
<section id="setting.-2" class="level4">
<h4>Setting.</h4>
<p>Same as Case 2, but the team can now afford to generate 2,000 fresh
responses from GPT-4-turbo (20% of the logged data) to train a critic
for DR.</p>
</section>
<section id="data.-1" class="level4">
<h4>Data.</h4>
<ul>
<li><p>Logged data: <span class="math inline">\(n = 10{,}000\)</span>
(same as Case 2).</p></li>
<li><p>Oracle slice: 200 (same).</p></li>
<li><p>Fresh draws: 2,000 prompts sampled uniformly from the 10k, with
GPT-4-turbo outputs and judge scores.</p></li>
</ul>
</section>
<section id="weights-and-calibration." class="level4">
<h4>Weights and calibration.</h4>
<p>Same as Case 2 (ESS 18% after , <span class="math inline">\(\alpha =
2.6\)</span>).</p>
</section>
<section id="critic-training." class="level4">
<h4>Critic training.</h4>
<ul>
<li><p>Train a gradient-boosted tree <span
class="math inline">\(\hat{g}(X)\)</span> on the 2,000 fresh draws
(5-fold cross-fitted) to predict calibrated reward <span
class="math inline">\(R\)</span>.</p></li>
<li><p>OOF <span class="math inline">\(R^2 = 0.58\)</span> (moderate;
prompt features include length, first-token embedding, topic
cluster).</p></li>
</ul>
</section>
<section id="dr-estimate." class="level4">
<h4>DR estimate.</h4>
<p><span class="math display">\[\begin{aligned}
\est{V}_{\dr}(\text{GPT-4-turbo}) &amp;= 0.75 \; [0.71, 0.79]
\end{aligned}\]</span></p>
</section>
<section id="diagnostics.-2" class="level4">
<h4>Diagnostics.</h4>
<ul>
<li><p>Orthogonality score: <span class="math inline">\(0.02 \; [-0.03,
0.07]\)</span> (pass, CI contains zero).</p></li>
<li><p>ESS: still 18% (unchanged by DR; weights are the same).</p></li>
<li><p>share: 18% (slightly lower than IPS due to variance reduction
from the critic).</p></li>
<li><p>Compare to IPS: IPS gave <span class="math inline">\([0.68,
0.80]\)</span> (width 0.12); DR gives <span class="math inline">\([0.71,
0.79]\)</span> (width 0.08). DR is 33% tighter.</p></li>
</ul>
</section>
<section id="conclusion.-2" class="level4">
<h4>Conclusion.</h4>
<p>DR delivers a tighter CI and a more stable estimate. The point
estimate (0.75) is consistent with IPS (0.74) and the sensitivity
checks. The team is now confident enough to deploy GPT-4-turbo without
an A/B test.</p>
</section>
</section>
<section id="lessons-learned" class="level2">
<h2>Lessons learned</h2>
<ul>
<li><p><strong>DM is simplest when you can generate.</strong> Case 1
shows clean paired comparisons with tight CIs.</p></li>
<li><p><strong>IPS works when overlap is moderate.</strong> Case 2 shows
that ESS 18% is usable, especially with sensitivity checks.</p></li>
<li><p><strong>DR tightens CIs and boosts confidence.</strong> Case 3
shows a 33% CI reduction with only 2,000 fresh draws (20% of logged
data).</p></li>
<li><p><strong>Always check diagnostics.</strong> Coverage, ESS,
orthogonality, and share catch issues early.</p></li>
<li><p><strong>Sensitivity checks build trust.</strong> Cohort
restriction, trimming, and alternative calibrators confirm
robustness.</p></li>
</ul>
</section>
</section>
</body>
</html>
