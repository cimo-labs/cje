<section id="assumptions-for-causal-interpretation" class="level1">
<h1>Assumptions for Causal Interpretation</h1>
<section id="overview" class="level2">
<h2>Overview</h2>
<p>CJE estimates answer causal questions: “What would happen if we
deployed <span class="math inline">\(\pi&#39;\)</span>?” To interpret
estimates causally, we need assumptions. This section states them in
plain English, explains when they matter, how to check them, and what
happens when they fail.</p>
</section>
<section id="assumption-hierarchy" class="level2">
<h2>Assumption hierarchy</h2>
<p>Not all assumptions are created equal. We group them into three
tiers:</p>
<dl>
<dt>Tier 1 (Critical):</dt>
<dd>
<p>Required for unbiased estimation; violations cause systematic errors.
These are <strong>non-negotiable</strong>.</p>
</dd>
<dt>Tier 2 (Important):</dt>
<dd>
<p>Required for valid inference (CIs); violations lead to underestimated
uncertainty or bias. Checkable via diagnostics.</p>
</dd>
<dt>Tier 3 (Efficiency):</dt>
<dd>
<p>Required for optimal variance; violations cost power but do not bias
point estimates.</p>
</dd>
</dl>
</section>
<section id="data-assumptions" class="level2">
<h2>Data assumptions</h2>
<div class="assumption">
<p><span id="assum:sutva" label="assum:sutva"></span> The outcome for
prompt <span class="math inline">\(X_i\)</span> under policy <span
class="math inline">\(\pi\)</span> does not depend on what policy was
applied to other prompts. Formally: no interference and no hidden
versions of treatment.</p>
</div>
<section id="plain-english." class="level4">
<h4>Plain English.</h4>
<p>Evaluating one prompt doesn’t affect another. This rules out settings
where responses are shown to users sequentially and influence each other
(e.g., conversational context bleeding across prompts).</p>
</section>
<section id="when-it-matters." class="level4">
<h4>When it matters.</h4>
<p>All modes (, , ). Violations are rare in offline batch evaluation but
common in online settings with statefulness.</p>
</section>
<section id="how-to-check." class="level4">
<h4>How to check.</h4>
<p>SUTVA is mostly a design question: ensure prompts are evaluated
independently. In practice, use fixed seeds, separate sessions, or
randomize order.</p>
</section>
<section id="what-happens-if-it-fails." class="level4">
<h4>What happens if it fails.</h4>
<p>Estimates are biased in unknown directions. The only fix is to
redefine the estimand to account for interference (beyond this
playbook’s scope).</p>
<div class="assumption">
<p><span id="assum:overlap" label="assum:overlap"></span> For all
prompts <span class="math inline">\(X\)</span> and responses <span
class="math inline">\(A\)</span> such that <span
class="math inline">\(\pi&#39;(A \mid X) &gt; 0\)</span>, we have <span
class="math inline">\(\pi_0(A \mid X) &gt; 0\)</span>. Informally: the
logging policy could have generated any response the target policy might
generate.</p>
</div>
</section>
<section id="plain-english.-1" class="level4">
<h4>Plain English.</h4>
<p>Off-policy evaluation requires that the logged data “covers” the
target policy’s behavior. If <span
class="math inline">\(\pi&#39;\)</span> generates responses that <span
class="math inline">\(\pi_0\)</span> would never produce, we have no
data to learn from.</p>
</section>
<section id="when-it-matters.-1" class="level4">
<h4>When it matters.</h4>
<p>Off-policy only (, ). Not needed for (you generate fresh
outputs).</p>
</section>
<section id="how-to-check.-1" class="level4">
<h4>How to check.</h4>
<p>ESS and tail diagnostics. Low ESS (<span class="math inline">\(&lt;
10\%\)</span>) or heavy tails (<span class="math inline">\(\alpha &lt;
2\)</span>) indicate poor overlap. Scatter plot of <span
class="math inline">\(\log \pi&#39;(A \mid X) / \pi_0(A \mid X)\)</span>
vs. <span class="math inline">\(X\)</span> can reveal systematic
gaps.</p>
</section>
<section id="what-happens-if-it-fails.-1" class="level4">
<h4>What happens if it fails.</h4>
<p>Importance weights explode, variance becomes infinite, estimates are
dominated by a few outliers. The only fix is to restrict to a
high-overlap cohort, switch to with a strong critic, or regenerate
().</p>
</section>
</section>
<section id="judge-and-calibration-assumptions" class="level2">
<h2>Judge and calibration assumptions</h2>
<div class="assumption">
<p><span id="assum:oracle" label="assum:oracle"></span> The labeled
oracle slice is a simple random sample from the evaluation distribution,
and labels <span class="math inline">\(Y\)</span> are unbiased
measurements of the true outcome.</p>
</div>
<section id="plain-english.-2" class="level4">
<h4>Plain English.</h4>
<p>The prompts you label should be representative of the prompts you
evaluate. Don’t cherry-pick easy prompts or focus only on one
domain.</p>
</section>
<section id="when-it-matters.-2" class="level4">
<h4>When it matters.</h4>
<p>All modes that use (, , ). Without this, calibration is biased.</p>
</section>
<section id="how-to-check.-2" class="level4">
<h4>How to check.</h4>
<p>Compare the distribution of <span class="math inline">\(S\)</span>
(and other covariates like prompt length, family) between the oracle
slice and the full evaluation set. Use a two-sample test (e.g.,
Kolmogorov-Smirnov) or visual inspection.</p>
</section>
<section id="what-happens-if-it-fails.-2" class="level4">
<h4>What happens if it fails.</h4>
<p>learns the wrong mapping, leading to biased <span
class="math inline">\(R = f(S)\)</span>. CIs are too narrow because they
don’t account for the selection bias. Fix: stratify labeling by prompt
family or <span class="math inline">\(S\)</span> bins; use
inverse-propensity weighting if the labeling mechanism is known.</p>
<div class="assumption">
<p><span id="assum:j2m" label="assum:j2m"></span> There exist monotone
functions <span class="math inline">\(h_Y\)</span> and <span
class="math inline">\(h_W\)</span> such that:</p>
<ul>
<li><p><span class="math inline">\(\E[Y \mid S] = h_Y(S)\)</span>
(outcome is monotone in score),</p></li>
<li><p><span class="math inline">\(\E[W \mid S] = h_W(S)\)</span>
(importance weight is monotone in score).</p></li>
</ul>
</div>
</section>
<section id="plain-english.-3" class="level4">
<h4>Plain English.</h4>
<p>Higher judge scores should correspond (on average) to better
outcomes, and the importance weight should vary smoothly with the score.
This is the key to and .</p>
</section>
<section id="when-it-matters.-3" class="level4">
<h4>When it matters.</h4>
<p>All modes for (outcome); off-policy for (weights).</p>
</section>
<section id="how-to-check.-3" class="level4">
<h4>How to check.</h4>
<ul>
<li><p>For <span class="math inline">\(h_Y\)</span>: Bin the oracle
slice by <span class="math inline">\(S\)</span> and check that mean
<span class="math inline">\(Y\)</span> increases (or decreases)
monotonically. Fit isotonic regression and inspect the fit.</p></li>
<li><p>For <span class="math inline">\(h_W\)</span>: Bin logged data by
<span class="math inline">\(S\)</span> and check that mean <span
class="math inline">\(W\)</span> is monotone. Scatter plot of <span
class="math inline">\(W\)</span> vs. <span
class="math inline">\(S\)</span> should show a trend.</p></li>
</ul>
</section>
<section id="what-happens-if-it-fails.-3" class="level4">
<h4>What happens if it fails.</h4>
<ul>
<li><p>If <span class="math inline">\(\E[Y \mid S]\)</span> is not
monotone, can be badly miscalibrated. Use two-stage (spline index <span
class="math inline">\(\to\)</span> isotonic) or add a coarse index
(prompt family).</p></li>
<li><p>If <span class="math inline">\(\E[W \mid S]\)</span> is not
monotone, may not help (or may hurt). Check the diagnostic; if
monotonicity fails, consider model-based weight calibration or switch to
.</p></li>
</ul>
<div class="assumption">
<p><span id="assump:j2mx" label="assump:j2mx"></span> There exists a
one-dimensional index <span
class="math inline">\(T=g(S,X_{\mathrm{cov}})\)</span> and a
nondecreasing <span class="math inline">\(\mu\)</span> such that <span
class="math inline">\(\E[Y\mid S,X_{\mathrm{cov}}]=\mu(T)\)</span>. For
off-policy weight calibration analogously assume <span
class="math inline">\(\E[W\mid S,X_{\mathrm{cov}}]=\nu(T)\)</span> with
nondecreasing <span class="math inline">\(\nu\)</span>.</p>
</div>
</section>
<section id="plain-english.-4" class="level4">
<h4>Plain English.</h4>
<p>When judge monotonicity fails at fixed <span
class="math inline">\(S\)</span> due to slice effects (e.g., length or
domain bias), we assume there’s still a one-dimensional summary <span
class="math inline">\(T\)</span> of <span class="math inline">\((S,
X_{\mathrm{cov}})\)</span> where monotonicity holds. This is weaker than
J2-M but still preserves the core structural belief in
rank-ordering.</p>
</section>
<section id="when-it-matters.-4" class="level4">
<h4>When it matters.</h4>
<p>All modes when using two-stage with covariates (§<a
href="#sec:two-stage-autocal" data-reference-type="ref"
data-reference="sec:two-stage-autocal">[sec:two-stage-autocal]</a>). If
plain J2-M holds, this is not needed.</p>
</section>
<section id="how-to-check.-4" class="level4">
<h4>How to check.</h4>
<p>Bin the oracle slice by quantiles of <span
class="math inline">\(\widehat T\)</span> and verify monotone <span
class="math inline">\(\bar Y\)</span> across bins; for IPS/DR, verify
monotone <span class="math inline">\(\bar W\)</span> across <span
class="math inline">\(\widehat T\)</span> bins. Compare regional error
before/after adding covariates.</p>
</section>
<section id="what-happens-if-it-fails.-4" class="level4">
<h4>What happens if it fails.</h4>
<p>Add/adjust <span class="math inline">\(X_{\mathrm{cov}}\)</span>
(e.g., response length, family), increase oracle labels in problematic
regions, or fall back to plain monotone on <span
class="math inline">\(S\)</span> with caveats.</p>
<div class="assumption">
<p><span id="assum:stability" label="assum:stability"></span> The
judge’s scoring function does not drift between the time the log is
collected and the time fresh draws are evaluated. Formally: <span
class="math inline">\(s_t(X, A) = s_{t&#39;}(X, A)\)</span> for all
<span class="math inline">\(t, t&#39;\)</span> in the evaluation
window.</p>
</div>
</section>
<section id="plain-english.-5" class="level4">
<h4>Plain English.</h4>
<p>The judge should assign the same score to the same <span
class="math inline">\((X, A)\)</span> pair regardless of when it’s
scored. Drift can come from model updates, prompt changes, or even
randomness in sampling.</p>
</section>
<section id="when-it-matters.-5" class="level4">
<h4>When it matters.</h4>
<p>Off-policy (, ) when comparing logged scores to fresh scores. Less
critical for if all scoring happens in one batch.</p>
</section>
<section id="how-to-check.-5" class="level4">
<h4>How to check.</h4>
<p>Anchor set with Kendall <span class="math inline">\(\tau\)</span>:
score a fixed set of 20–50 prompts at <span
class="math inline">\(t_0\)</span> and <span
class="math inline">\(t_1\)</span>, compute rank correlation. <span
class="math inline">\(\tau \ge 0.90\)</span> is good; <span
class="math inline">\(\tau &lt; 0.75\)</span> is a red flag.</p>
</section>
<section id="what-happens-if-it-fails.-5" class="level4">
<h4>What happens if it fails.</h4>
<p>Calibration breaks because the meaning of <span
class="math inline">\(S\)</span> has changed. Oracle labels from <span
class="math inline">\(t_0\)</span> don’t predict outcomes at <span
class="math inline">\(t_1\)</span>. Fix: refresh the oracle slice with
new labels from <span class="math inline">\(t_1\)</span> and re-fit . Or
freeze the judge version/config during the evaluation window.</p>
<div class="assumption">
<p><span id="assum:transport" label="assum:transport"></span> Let <span
class="math inline">\(f\)</span> be the judge<span
class="math inline">\(\to\)</span>oracle map learned on a source stratum
<span class="math inline">\(\mathcal{S}\)</span> (e.g., a policy or time
window). We say <span class="math inline">\(f\)</span> is
<em>transportable</em> to a target stratum <span
class="math inline">\(\mathcal{T}\)</span> if, for all <span
class="math inline">\((S, X_{\mathrm{cov}})\)</span> in the target
support, <span class="math display">\[\E\!\left[\,Y \,\middle|\, S,
X_{\mathrm{cov}}, G=\mathcal{T}\right]
\;=\;
\E\!\left[\,Y \,\middle|\, S, X_{\mathrm{cov}}, G=\mathcal{S}\right]
\;=\; f^\star(S, X_{\mathrm{cov}})\]</span> i.e., <span
class="math inline">\(Y \,\perp\, G \mid (S, X_{\mathrm{cov}})\)</span>,
where <span class="math inline">\(G\)</span> is a group indicator
(policy or time “era”).</p>
</div>
</section>
<section id="interpretation." class="level4">
<h4>Interpretation.</h4>
<p>Conditional on the score and any included covariates, the group
(policy/time) carries no extra information about the outcome.
Equivalently, <span class="math inline">\(f\)</span> is
<em>policy-invariant</em> and <em>time-invariant</em> once you condition
on <span class="math inline">\((S, X_{\mathrm{cov}})\)</span>.</p>
</section>
<section id="weaker-variants." class="level4">
<h4>Weaker variants.</h4>
<p>(i) <strong>Mean-shift only:</strong> <span
class="math inline">\(\E[Y\!\mid\!S,X_{\mathrm{cov}},G] =
f^\star(S,X_{\mathrm{cov}}) + c_G\)</span> (a constant vertical offset
per group). (ii) <strong>Regional shift:</strong> small bounded
deviations within quantiles of the two-stage index <span
class="math inline">\(T=g(S,X_{\mathrm{cov}})\)</span>.</p>
</section>
<section id="when-it-can-fail." class="level4">
<h4>When it can fail.</h4>
<p>If a new policy changes outcome at fixed score (judge bias
w.r.t. that policy), or if the judge’s semantics drift over time
(model/rubric changes), <span class="math inline">\(f\)</span> is not
transportable without adjustment.</p>
</section>
</section>
<section id="structural-assumptions" class="level2">
<h2>Structural assumptions</h2>
<div class="assumption">
<p><span id="assum:bounded" label="assum:bounded"></span> Outcomes <span
class="math inline">\(Y \in [0, 1]\)</span> are bounded. Judge scores
<span class="math inline">\(S\)</span> may be unbounded, but calibrated
rewards <span class="math inline">\(R = f(S) \in [0, 1]\)</span>.</p>
</div>
<section id="plain-english.-6" class="level4">
<h4>Plain English.</h4>
<p>The outcome you care about (pass rate, reward, satisfaction) has a
natural scale. maps scores onto <span class="math inline">\([0,
1]\)</span> to match that scale.</p>
</section>
<section id="when-it-matters.-6" class="level4">
<h4>When it matters.</h4>
<p>All modes. Boundedness is needed for mean preservation and honest
CIs.</p>
</section>
<section id="how-to-check.-6" class="level4">
<h4>How to check.</h4>
<p>Verify that oracle <span class="math inline">\(Y \in [0, 1]\)</span>.
If judge scores are on a different scale (e.g., 1–10), automatically
rescales.</p>
</section>
<section id="what-happens-if-it-fails.-6" class="level4">
<h4>What happens if it fails.</h4>
<p>If outcomes are unbounded (e.g., token count, latency), variance can
be huge and isotonic regression may overfit. Consider transforming to a
bounded scale (e.g., log-transform or winsorize) before calibration.</p>
</section>
</section>
<section id="rate-assumptions-for-dr" class="level2">
<h2>Rate assumptions (for DR)</h2>
<div class="assumption">
<p><span id="assum:rates" label="assum:rates"></span> For doubly robust
inference, either:</p>
<ul>
<li><p>The outcome model <span class="math inline">\(\hat{g}(X)\)</span>
converges at rate <span class="math inline">\(n^{-1/4}\)</span> or
faster, OR</p></li>
<li><p>The stabilized weights <span class="math inline">\(W\)</span>
converge to the true weights at rate <span
class="math inline">\(n^{-1/4}\)</span> or faster.</p></li>
</ul>
<p>If both converge at <span class="math inline">\(n^{-1/2}\)</span>,
achieves the efficiency bound.</p>
</div>
<section id="plain-english.-7" class="level4">
<h4>Plain English.</h4>
<p>For to work well, you need either a decent critic or decent weights
(or both). If both are terrible, loses its advantages.</p>
</section>
<section id="when-it-matters.-7" class="level4">
<h4>When it matters.</h4>
<p>only. Not needed for or .</p>
</section>
<section id="how-to-check.-7" class="level4">
<h4>How to check.</h4>
<p>Orthogonality diagnostic. If the orthogonality score CI contains
zero, you’re in good shape. If not, either the critic or the weights are
too far off.</p>
</section>
<section id="what-happens-if-it-fails.-7" class="level4">
<h4>What happens if it fails.</h4>
<p>is still consistent (unbiased) but loses efficiency and may have
higher variance than . The fix is to improve the critic (more fresh
draws, better model) or improve the weights (better , cohort
restriction).</p>
</section>
</section>
<section id="when-assumptions-can-be-relaxed" class="level2">
<h2>When assumptions can be relaxed</h2>
<ul>
<li><p><strong>Overlap (D2)</strong> can be relaxed if you use with a
strong critic. The critic can extrapolate to regions with poor
overlap.</p></li>
<li><p><strong>Monotone sufficiency (J2-M)</strong> can be relaxed with
two-stage or coarse indexing (prompt family).</p></li>
<li><p><strong>Judge stability (J3)</strong> can be relaxed if you
refresh the oracle slice frequently and re-calibrate.</p></li>
<li><p><strong>Rate assumptions (R3)</strong> are not needed for point
estimates, only for optimal inference. If you don’t care about
efficiency, you can ignore them.</p></li>
</ul>
</section>
<section id="sensitivity-analysis-and-robustness-checks" class="level2">
<h2>Sensitivity analysis and robustness checks</h2>
<p>When assumptions are questionable, perform sensitivity checks:</p>
<ul>
<li><p><strong>Cohort restriction:</strong> Re-run the analysis on a
subset with better overlap (e.g., <span class="math inline">\(|\log W|
&lt; 2\)</span>). If estimates are stable, you’re robust.</p></li>
<li><p><strong>Trimming:</strong> Drop the top 1–5% of weights and see
if results change. Large changes indicate tail sensitivity.</p></li>
<li><p><strong>Alternative calibrators:</strong> Compare isotonic to
spline or linear calibration. Agreement suggests robustness.</p></li>
<li><p><strong>Alternative critics:</strong> For , try multiple outcome
models (linear, random forest, boosting). If all agree, you’re in good
shape.</p></li>
<li><p><strong>Anchor set drift:</strong> Compute <span
class="math inline">\(\tau\)</span> on multiple anchor sets or at
multiple time points. Consistent high <span
class="math inline">\(\tau\)</span> indicates stability.</p></li>
</ul>
</section>
<section id="minimal-assumptions-by-mode" class="level2">
<h2>Minimal assumptions by mode</h2>
<table>
<caption>Assumptions required by mode. = required but less critical due
to robustness. — = not needed.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Assumption</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">SUTVA (D1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Overlap (D2)</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Oracle slice (J1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Monotone sufficiency (J2-M)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Judge stability (J3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Bounded outcomes (S1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Rate assumptions (R3)</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</section>
<section id="summary" class="level2">
<h2>Summary</h2>
<p>The key assumptions are SUTVA, overlap (for off-policy), oracle
randomness, and judge monotone sufficiency. Most are checkable via
diagnostics. When in doubt, run sensitivity checks and report them
alongside results. If assumptions fail and cannot be fixed, switch modes
or regenerate data.</p>
</section>
</section>
