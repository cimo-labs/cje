<section id="limitations-and-known-issues" class="level1">
<h1>Limitations and Known Issues</h1>
<section id="overview" class="level2">
<h2>Overview</h2>
<p>CJE is a practical framework for causal judge evaluation, but it has
limitations. This section documents known issues, settings where CJE may
not apply, and open challenges.</p>
</section>
<section id="limitation-1-reliance-on-judge-quality" class="level2">
<h2>Limitation 1: Reliance on judge quality</h2>
<section id="the-issue." class="level4">
<h4>The issue.</h4>
<p>CJE calibrates judge scores to outcome-scale rewards, but if the
judge is systematically biased or unreliable, calibration cannot fix it.
Garbage in, garbage out.</p>
</section>
<section id="when-it-matters." class="level4">
<h4>When it matters.</h4>
<p>All modes. If the judge is poorly correlated with the true outcome
(<span class="math inline">\(\rho &lt; 0.5\)</span>), even with perfect
calibration, estimates will be noisy or biased.</p>
</section>
<section id="mitigation." class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Use a high-quality judge (e.g., GPT-4, Claude-3 with a
well-designed rubric).</p></li>
<li><p>Validate judge-human agreement on a small pilot (aim for <span
class="math inline">\(\rho &gt; 0.7\)</span>).</p></li>
<li><p>Use ensemble judges (average scores from multiple judges) to
reduce variance.</p></li>
<li><p>Check calibration reliability: if OOF MAE <span
class="math inline">\(&gt; 0.10\)</span>, the judge may not be
sufficiently predictive.</p></li>
</ul>
</section>
</section>
<section id="limitation-2-monotonicity-assumption-j2-m" class="level2">
<h2>Limitation 2: Monotonicity assumption (J2-M)</h2>
<section id="the-issue.-1" class="level4">
<h4>The issue.</h4>
<p>AutoCal-R and SIMCal assume that outcomes and weights are monotone in
the judge score <span class="math inline">\(S\)</span>. If <span
class="math inline">\(\E[Y \mid S]\)</span> is non-monotone (e.g., <span
class="math inline">\(U\)</span>-shaped), isotonic regression can be
badly miscalibrated.</p>
</section>
<section id="when-it-matters.-1" class="level4">
<h4>When it matters.</h4>
<p>All modes for AutoCal-R; off-policy for SIMCal.</p>
</section>
<section id="mitigation.-1" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Check monotonicity on the oracle slice: bin by <span
class="math inline">\(S\)</span>, plot mean <span
class="math inline">\(Y\)</span>.</p></li>
<li><p>If non-monotone, use two-stage AutoCal-R (flexible index <span
class="math inline">\(\to\)</span> isotonic) or add a coarse index
(prompt family, length).</p></li>
<li><p>For SIMCal, check <span class="math inline">\(W\)</span>
vs. <span class="math inline">\(S\)</span> scatter; if clearly
non-monotone, consider model-based weight calibration (e.g., fit <span
class="math inline">\(\E[W \mid S]\)</span> with a flexible
model).</p></li>
</ul>
</section>
</section>
<section id="limitation-3-overlap-off-policy-only" class="level2">
<h2>Limitation 3: Overlap (off-policy only)</h2>
<section id="the-issue.-2" class="level4">
<h4>The issue.</h4>
<p>Off-policy evaluation requires that the logging policy <span
class="math inline">\(\pi_0\)</span> could have generated any response
the target policy <span class="math inline">\(\pi&#39;\)</span> might
generate. If overlap is poor, importance weights explode and estimates
are unreliable.</p>
</section>
<section id="when-it-matters.-2" class="level4">
<h4>When it matters.</h4>
<p>Off-policy only (IPS, DR). Not an issue for DM.</p>
</section>
<section id="mitigation.-2" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Check ESS; if <span class="math inline">\(&lt; 10\%\)</span>,
overlap is poor.</p></li>
<li><p>Use SIMCal to stabilize weights.</p></li>
<li><p>Restrict to high-overlap cohorts.</p></li>
<li><p>Switch to DR with a strong critic (critic can extrapolate to
low-overlap regions).</p></li>
<li><p>If overlap is catastrophic (ESS <span class="math inline">\(&lt;
1\%\)</span>), regenerate and use DM.</p></li>
</ul>
</section>
</section>
<section id="limitation-4-judge-drift" class="level2">
<h2>Limitation 4: Judge drift</h2>
<section id="the-issue.-3" class="level4">
<h4>The issue.</h4>
<p>If the judge’s interpretation of scores changes over time (due to
model updates, prompt changes, or even sampling randomness), calibration
breaks. Old oracle labels don’t predict new outcomes.</p>
</section>
<section id="when-it-matters.-3" class="level4">
<h4>When it matters.</h4>
<p>Off-policy when comparing old logs to new fresh draws; also DM if
scoring happens over a long time window.</p>
</section>
<section id="mitigation.-3" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Freeze judge version and config during the evaluation
window.</p></li>
<li><p>Check stability via an anchor set (Kendall <span
class="math inline">\(\tau \ge 0.90\)</span>).</p></li>
<li><p>If drift is detected, refresh the oracle slice with new labels
and re-fit AutoCal-R.</p></li>
<li><p>For long-running evaluations, re-calibrate periodically (e.g.,
monthly).</p></li>
</ul>
</section>
</section>
<section id="limitation-5-small-oracle-slices" class="level2">
<h2>Limitation 5: Small oracle slices</h2>
<section id="the-issue.-4" class="level4">
<h4>The issue.</h4>
<p>When the oracle slice is small (<span class="math inline">\(&lt;
50\)</span> samples), AutoCal-R has high variance, leading to large OUA
share and wide CIs. Calibration may also be unreliable
(overfitting).</p>
</section>
<section id="when-it-matters.-4" class="level4">
<h4>When it matters.</h4>
<p>All modes. Especially problematic when the evaluation <span
class="math inline">\(S\)</span>-range is wide or when regional
calibration is needed.</p>
</section>
<section id="mitigation.-4" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Aim for <span class="math inline">\(n_{\text{oracle}} \ge
100\)</span> (150–200 recommended).</p></li>
<li><p>Check OUA share; if <span class="math inline">\(&gt;
50\%\)</span>, add labels.</p></li>
<li><p>Prioritize labels in sparse <span
class="math inline">\(S\)</span> regions or high-variance prompt
families.</p></li>
<li><p>Use regularization (smoothed isotonic regression) to reduce
overfitting on small oracle slices.</p></li>
</ul>
</section>
</section>
<section id="limitation-6-extrapolation-beyond-labeled-range"
class="level2">
<h2>Limitation 6: Extrapolation beyond labeled range</h2>
<section id="the-issue.-5" class="level4">
<h4>The issue.</h4>
<p>If evaluation scores <span class="math inline">\(S\)</span> fall
outside the oracle <span class="math inline">\(S\)</span>-range,
AutoCal-R must extrapolate. Extrapolation is unreliable, especially at
the boundaries.</p>
</section>
<section id="when-it-matters.-5" class="level4">
<h4>When it matters.</h4>
<p>All modes. Coverage diagnostic flags this.</p>
</section>
<section id="mitigation.-5" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Add labels targeting uncovered <span
class="math inline">\(S\)</span> bins.</p></li>
<li><p>Narrow the prompt set to the intended deployment slice (avoid
stress-testing edges unless you label them).</p></li>
<li><p>Inspect boundary slopes; if steep or flat, extrapolation is
risky.</p></li>
<li><p>Report coverage badge; if <span class="math inline">\(&lt;
85\%\)</span>, flag the estimate as provisional.</p></li>
</ul>
</section>
</section>
<section id="limitation-7-high-dimensional-prompts-dr-critic"
class="level2">
<h2>Limitation 7: High-dimensional prompts (DR critic)</h2>
<section id="the-issue.-6" class="level4">
<h4>The issue.</h4>
<p>For DR, the critic <span class="math inline">\(\hat{g}(X)\)</span>
must predict calibrated rewards from prompt features. If prompts are
high-dimensional or unstructured (e.g., raw text), the critic may
overfit or fail to generalize.</p>
</section>
<section id="when-it-matters.-6" class="level4">
<h4>When it matters.</h4>
<p>DR only.</p>
</section>
<section id="mitigation.-6" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Use feature engineering: extract prompt length, topic embeddings,
difficulty scores.</p></li>
<li><p>Use flexible models with regularization (gradient-boosted trees,
neural nets with dropout).</p></li>
<li><p>Check orthogonality: if CI excludes zero, the critic is not good
enough; add features or fresh draws.</p></li>
<li><p>For very high-dimensional prompts, consider dimensionality
reduction (e.g., PCA on embeddings).</p></li>
</ul>
</section>
</section>
<section id="limitation-8-non-i.i.d.-data" class="level2">
<h2>Limitation 8: Non-i.i.d. data</h2>
<section id="the-issue.-7" class="level4">
<h4>The issue.</h4>
<p>CJE assumes samples are independent and identically distributed
(i.i.d.). If prompts are sequential or correlated (e.g., multi-turn
conversations), SUTVA fails and estimates are biased.</p>
</section>
<section id="when-it-matters.-7" class="level4">
<h4>When it matters.</h4>
<p>All modes. Especially problematic in online settings with user
sessions.</p>
</section>
<section id="mitigation.-7" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>For batch offline evaluation, ensure prompts are sampled
independently.</p></li>
<li><p>For sequential data, break into independent chunks or use
session-level aggregation.</p></li>
<li><p>If SUTVA is violated, redefine the estimand to account for
interference (beyond this playbook).</p></li>
</ul>
</section>
</section>
<section id="limitation-9-multiple-testing" class="level2">
<h2>Limitation 9: Multiple testing</h2>
<section id="the-issue.-8" class="level4">
<h4>The issue.</h4>
<p>If you evaluate many policies or run many subgroup analyses, some
will appear significant by chance (false positives). Standard CIs do not
account for multiple comparisons.</p>
</section>
<section id="when-it-matters.-8" class="level4">
<h4>When it matters.</h4>
<p>When evaluating <span class="math inline">\(&gt; 5\)</span> policies
or running many subgroup analyses.</p>
</section>
<section id="mitigation.-8" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Use Bonferroni or Benjamini-Hochberg correction for multiple
testing.</p></li>
<li><p>Pre-specify a small number of key comparisons; treat others as
exploratory.</p></li>
<li><p>Use a holdout set for final validation (train/eval/test
split).</p></li>
</ul>
</section>
</section>
<section id="limitation-10-generalization-to-deployment" class="level2">
<h2>Limitation 10: Generalization to deployment</h2>
<section id="the-issue.-9" class="level4">
<h4>The issue.</h4>
<p>CJE estimates are valid for the evaluation prompt distribution. If
deployment prompts differ materially, estimates may not generalize.</p>
</section>
<section id="when-it-matters.-9" class="level4">
<h4>When it matters.</h4>
<p>All modes. This is a distribution-shift problem, not a CJE limitation
per se.</p>
</section>
<section id="mitigation.-9" class="level4">
<h4>Mitigation.</h4>
<ul>
<li><p>Ensure evaluation prompts are representative of deployment
(stratified sampling by user segment, use case, etc.).</p></li>
<li><p>For new domains, collect a small deployment-representative slice
and re-calibrate.</p></li>
<li><p>Monitor online KPIs after deployment and compare to offline
estimates (meta-learning for future calibrations).</p></li>
</ul>
</section>
</section>
<section id="open-challenges" class="level2">
<h2>Open challenges</h2>
<ul>
<li><p><strong>Sequential and adaptive evaluation:</strong> How to
handle multi-turn conversations or adaptive prompting where responses
depend on prior context.</p></li>
<li><p><strong>Distribution shift:</strong> Principled methods for
extrapolating to new prompt distributions.</p></li>
<li><p><strong>Multi-objective optimization:</strong> Evaluating
trade-offs between multiple KPIs (e.g., correctness vs. safety
vs. latency).</p></li>
<li><p><strong>Online learning:</strong> Integrating CJE with bandit
algorithms for continuous policy improvement.</p></li>
<li><p><strong>Fairness and robustness:</strong> Ensuring estimates are
unbiased across demographic groups and robust to adversarial
prompts.</p></li>
</ul>
</section>
<section id="summary" class="level2">
<h2>Summary</h2>
<p>CJE’s main limitations are judge quality, monotonicity assumptions,
overlap (for off-policy), and oracle slice size. Most are checkable via
diagnostics and addressable with targeted fixes. When limitations cannot
be resolved, report them transparently and interpret estimates with
appropriate caution.</p>
</section>
</section>
