<h2>Direct Modeling (DM): Calibrated On-Policy Evaluation</h2>
<h3>What DM solves</h3>
<p>
Direct Modeling answers: "What KPI would we see on this prompt set if we shipped policy $\pi$?" It is the closest offline analog to an A/B test: generate outputs for each policy on the same prompts (paired design), map judge scores to an outcome scale with AutoCal-R, average the calibrated rewards, and attach confidence intervals that include calibration uncertainty (OUA).
</p>
<p>
<strong>DM in one minute (operator view)</strong>
</p>
<ol>
</p>
<p>
<li>Fix the prompt set $\mathcal{X} = \{X_i\}_{i=1}^m$ for evaluation.
<li>For each candidate policy $\pi$, generate one output $A_{\pi i}$ per prompt $X_i$ (use shared seeds where relevant).
<li>Score each $(X_i, A_{\pi i})$ with the judge to get $S_{\pi i}$.
<li>Calibrate scores to outcome-scale rewards via AutoCal-R: $R_{\pi i} = f(S_{\pi i})$.
<li>Estimate the value and (paired) differences with honest CIs (add OUA).
</p>
<p>
</ol>
<h3>When to use DM</h3>
<ul>
</p>
<p>
<li>You can safely generate fresh outputs for every policy on a shared prompt set.
<li>You want clean causal comparisons without dealing with propensities/overlap.
<li>Your main risks are calibration coverage, judge drift, and label scarcity (handled by AutoCal-R{} + diagnostics + OUA).
</p>
<p>
</ul>
<h3>Inputs & setup</h3>
<p>
<li>a fixed evaluation set; use the same $\mathcal{X}$ for all policies (paired design).
<li>a scalar scoring function that returns $S = s(X, A)$ with fixed rubric/config.
<li>a small, diverse set with ground-truth labels $Y$ to train AutoCal-R.
</p>
<h3>AutoCal-R: map judge score to outcome</h3>
<p>
AutoCal-R{} learns a mean-preserving mapping $f: \mathbb{R} \to [0,1]$ from score $S$ to calibrated reward $R = f(S)$. By default it fits a monotone mapping; when needed, it auto-switches to a light two-stage index (e.g., spline index $\to$ isotonic) if regional miscalibration is detected. When applying $f$ to other policies, verify Assumption~§assum:transport with the Transportability Audit (Diagnostic 5, §diag:transport).
</p>
<h5>Why monotone (isotonic) by default?</h5>
 Isotonic regression enforces the single structural belief you want: <em>higher judge score $\Rightarrow$ no worse expected outcome</em>. Unlike parametric links (sigmoid, beta) that impose a rigid shape and risk misspecification, isotonic adapts to the data while preserving ranking sanity. It is mean-preserving by construction (via projection onto the monotone cone), so your KPI stays on the right scale without post-hoc adjustments. With small oracle slices (5-10\
</p>
<h5>Operator artifacts to check:</h5>
<ul>
</p>
<p>
<li><strong>Reliability:</strong> out-of-fold calibration curve and regional error (low/mid/high $S$).
<li><strong>Mean preservation:</strong> oracle mean of $f(S)$ matches mean of $Y$.
<li><strong>Score coverage:</strong> fraction of evaluated $S$ that lies inside the oracle $S$-range; inspect boundary slopes.
</p>
<p>
</ul>
<h4>Two-Stage AutoCal-R{</h4>
 (with covariates)}
</p>
<p>
When $\E[Y\mid S]$ is only approximately monotone or exhibits slice effects (e.g., length or domain bias at fixed $S$), we use a light two-stage procedure that preserves the single structural belief we trust—<em>monotonicity in a one-dimensional risk index</em>—while allowing covariate adjustment.
</p>
<h5>Construction (cross-fitted).</h5>
 Let $X_{\mathrm{cov}}$ denote optional, low-cardinality covariates (e.g., response length, prompt family). For each oracle fold $k$:
</p>
<ol>
</p>
<p>
<li><strong>Risk index (OOF):</strong> fit a low-capacity $g_{\setminus k}$ on folds $\neq k$ to predict $Y$ from $(S, X_{\mathrm{cov}})$ and compute $T_i = g_{\setminus k}(S_i, X_{i,\mathrm{cov}})$ for $i \in k$.
<li><strong>Uniformize:</strong> $U_i = \widehat{\mathrm{ECDF}}_{\setminus k}(T_i) \in [0,1]$ using the training folds only.
<li><strong>Shape-enforce (isotonic):</strong> fit $h_{\setminus k} = \arg\min_{h \in \mathcal{M}} \sum_{j \notin k} (Y_j - h(U_j))^2$ with $\mathcal{M}$ the nondecreasing functions; set
$$
f_{\setminus k}(S_i, X_{i,\mathrm{cov}})=h_{\setminus k}(U_i).
$$
<li><strong>Mean-preserve:</strong> recenter so $\frac{1}{|{\text{train}}|}\sum_{j\notin k} h_{\setminus k}(U_j)=\frac{1}{|{\text{train}}|}\sum_{j\notin k} Y_j$.
</p>
<p>
</ol>
<p>
At inference time we use the OOF $f_{\setminus k}$ for oracle points and the pooled fit for evaluation points.
</p>
<h5>Assumption (single-index monotone).</h5>
 There exists $g^\star(S,X_{\mathrm{cov}})$ and nondecreasing $\mu^\star$ such that $\E[Y\mid S,X_{\mathrm{cov}}]=\mu^\star(g^\star(S,X_{\mathrm{cov}}))$. Then $f$ is $L^2$-consistent; see §assump:j2mx.
</p>
<h5>Why it helps.</h5>
 (i) Captures slice heterogeneity with one d.o.f.; (ii) rank–uniformization makes the mapping scale-free and density-stable; (iii) preserves interpretability and mean; (iv) plays well with OUA{} and regional diagnostics.
</p>
<h5>When to enable.</h5>
 Persistent regional error in reliability plots, slice effects at fixed $S$, or edge fragility (flat/steep boundary slopes).
</p>
<h5>API (code parity).</h5>
<pre><code>from cje import analyze_dataset
</p>
<p>
results = analyze_dataset(
    fresh_draws_dir="responses/",
    calibration_mode="auto",                 # "two_stage" to force
    calibration_covariates=["response_length","domain"],
    include_response_length=True             # auto-compute from response text
)
</code></pre>
<h3>Estimator and paired contrasts</h3>
<p>
With $R_{\pi i} = f(S_{\pi i})$ on shared prompts,
$$
\est{V}_{\dm}(\pi) &= \frac{1}{m} \sum_{i=1}^m R_{\pi i}, \\
\est{\Delta}(\pi, \pi') &= \frac{1}{m} \sum_{i=1}^m \left( R_{\pi i} - R_{\pi' i} \right).
$$
</p>
<p>
Use paired standard errors for $\est{\Delta}$ (they are tighter because prompt-level noise cancels).
</p>
<h3>Uncertainty that stays honest (OUA)</h3>
<p>
Treating $f$ as fixed understates uncertainty when the oracle slice is small. <strong>OUA{</strong> (oracle-uncertainty aware)} inference refits $f$ across label folds and adds the induced variance:
$$
\SE^2_{\text{total}} = \Var_{\text{main}} + \Var_{\oua},
$$
and forms 95\
</p>
<h3>Diagnostics (highest leverage) & quick fixes</h3>
<ol>
[label=(*)]
<li><strong>Score coverage</strong> (fraction of evaluated $S$ inside the labeled range; boundary slopes).
</p>
<p>
<em>Fix:</em> add a small number of labels targeting uncovered $S$ bins; consider narrowing the prompt set to the intended deployment slice.
</p>
<p>
<li><strong>Calibration reliability</strong> (OOF curve + regional error).
</p>
<p>
<em>Fix:</em> enable the two-stage AutoCal-R{} fallback; add a coarse index (prompt family / length); collect a handful of labels in problematic regions.
</p>
<p>
<li><strong>Judge stability / drift</strong> (rank stability on a small anchor set; unchanged judge config).
</p>
<p>
<em>Fix:</em> freeze judge version during the eval window; if drift appears, refresh the oracle slice and re-fit AutoCal-R.
</p>
<p>
<li><strong>OUA{</strong> share} (how much labels drive the CI).
</p>
<p>
<em>Fix:</em> if large, add labels (especially in sparse $S$ regions); otherwise more prompts help most.
</p>
<p>
</ol>
<h3>Reporting template (DM)</h3>
<p>
For each policy (and for key pairwise contrasts), report:
</p>
<ul>
</p>
<p>
<li>Calibrated mean $\est{V}_{\dm}(\pi)$ with 95\
<li>OUA{} share; note if paired design was used.
<li>AutoCal-R{} reliability snippet (with regional error) and the score-coverage badge.
<li>Any drift checks performed on the judge.
</p>
<p>
</ul>
<h3>Sample size & label planner (rules of thumb)</h3>
<p>
Let $\hat{\sigma}_R^2$ be the prompt-level variance of $R$ for a policy on the evaluation set. Then a rough CI half-width for one policy is
$$
\text{HalfWidth} \approx 1.96 \sqrt{\frac{\hat{\sigma}_R^2}{m} + \Var_{\oua}}.
$$
</p>
<p>
More prompts shrink the first term; more labels shrink the second (OUA). For pairwise contrasts, replace $\hat{\sigma}_R^2$ with the variance of $R_\pi - R_{\pi'}$ (often smaller due to pairing).
</p>
<h3>Common pitfalls (and how to avoid them)</h3>
<ul>
</p>
<p>
<li><strong>Different prompts across policies.</strong> Always evaluate on the same $\mathcal{X}$; match seeds if stochastic.
</p>
<p>
<li><strong>Thin coverage at the edges of $S$.</strong> Target labels to those bins or focus the prompt set; report the coverage badge.
</p>
<p>
<li><strong>Calibration curve looks good overall but bad in one region.</strong> Use two-stage AutoCal-R{} or add a coarse index and a few labels.
</p>
<p>
<li><strong>Large OUA{</strong> share.} Label more—prioritize regions where $S$ is sparse or where policy comparisons matter.
</p>
<p>
</ul>
<h3>Minimal recipe (pseudocode)</h3>
<pre><code># Inputs: prompts X, candidate policies Pi, judge s(.), oracle slice {(S, Y)}
# Output: calibrated means, paired contrasts, and CIs with OUA
</p>
<p>
# 1) Fit AutoCal-R on oracle (cross-fitted); persist reliability + coverage edges
f = fit_autocal_r(oracle_S, oracle_Y, K=5)
</p>
<p>
# 2) For each policy pi:
for pi in candidates:
    for i in range(m):
        A_pi[i] = generate(pi, X[i])     # same prompts for all
        S_pi[i] = judge_score(X[i], A_pi[i])
        R_pi[i] = f(S_pi[i])
</p>
<p>
    V_hat[pi] = np.mean(R_pi)
</p>
<p>
# 3) For contrasts (pi, pi'):
for (pi, pi_prime) in pairs:
    Delta_hat = np.mean(R_pi - R_pi_prime)    # paired difference
    SE_main = np.std(R_pi - R_pi_prime) / np.sqrt(m)
</p>
<p>
# 4) OUA: refit AutoCal-R across K folds of oracle; recompute step (2-3)
for k in range(K):
    f_k = fit_autocal_r(oracle_minus_fold_k)
    # ... recompute V_hat with f_k ...
SE_total_squared = SE_main**2 + Var_OUA
</p>
<p>
# 5) Report V_hat, Delta_hat, 95
#    reliability, coverage
</code></pre>
<h3>Scope notes</h3>
<p>
DM{} is on-policy for your chosen prompt distribution. If deployment prompts differ materially, treat that as a distribution-shift question: either adapt the prompt set to match deployment or build a small label slice representative of the target domain and re-fit AutoCal-R.

</p>