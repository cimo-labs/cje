<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>02_dm</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/usr/share/javascript/mathjax/MathJax.js"
  type="text/javascript"></script>
</head>
<body>
<section id="direct-modeling-dm-calibrated-on-policy-evaluation"
class="level1">
<h1>Direct Modeling (DM): Calibrated On-Policy Evaluation</h1>
<section id="what-dm-solves" class="level2">
<h2>What DM solves</h2>
<p>Direct Modeling answers: “What KPI would we see on this prompt set if
we shipped policy <span class="math inline">\(\pi\)</span>?” It is the
closest offline analog to an A/B test: generate outputs for each policy
on the same prompts (paired design), map judge scores to an outcome
scale with , average the calibrated rewards, and attach confidence
intervals that include calibration uncertainty ().</p>
<div class="quickref">
<p><strong>DM in one minute (operator view)</strong></p>
<ol>
<li><p>Fix the prompt set <span class="math inline">\(\mathcal{X} =
\{X_i\}_{i=1}^m\)</span> for evaluation.</p></li>
<li><p>For each candidate policy <span
class="math inline">\(\pi\)</span>, generate one output <span
class="math inline">\(A_{\pi i}\)</span> per prompt <span
class="math inline">\(X_i\)</span> (use shared seeds where
relevant).</p></li>
<li><p>Score each <span class="math inline">\((X_i, A_{\pi i})\)</span>
with the judge to get <span class="math inline">\(S_{\pi
i}\)</span>.</p></li>
<li><p>Calibrate scores to outcome-scale rewards via : <span
class="math inline">\(R_{\pi i} = f(S_{\pi i})\)</span>.</p></li>
<li><p>Estimate the value and (paired) differences with honest CIs (add
).</p></li>
</ol>
</div>
</section>
<section id="when-to-use-dm" class="level2">
<h2>When to use DM</h2>
<ul>
<li><p>You can safely generate fresh outputs for every policy on a
shared prompt set.</p></li>
<li><p>You want clean causal comparisons without dealing with
propensities/overlap.</p></li>
<li><p>Your main risks are calibration coverage, judge drift, and label
scarcity (handled by + diagnostics + ).</p></li>
</ul>
</section>
<section id="inputs-setup" class="level2">
<h2>Inputs &amp; setup</h2>
<dl>
<dt>Prompt set:</dt>
<dd>
<p>a fixed evaluation set; use the same <span
class="math inline">\(\mathcal{X}\)</span> for all policies (paired
design).</p>
</dd>
<dt>Judge:</dt>
<dd>
<p>a scalar scoring function that returns <span class="math inline">\(S
= s(X, A)\)</span> with fixed rubric/config.</p>
</dd>
<dt>Oracle slice:</dt>
<dd>
<p>a small, diverse set with ground-truth labels <span
class="math inline">\(Y\)</span> to train .</p>
</dd>
</dl>
</section>
<section id="autocal-r-map-judge-score-to-outcome" class="level2">
<h2>AutoCal-R: map judge score to outcome</h2>
<p>learns a mean-preserving mapping <span class="math inline">\(f:
\mathbb{R} \to [0,1]\)</span> from score <span
class="math inline">\(S\)</span> to calibrated reward <span
class="math inline">\(R = f(S)\)</span>. By default it fits a monotone
mapping; when needed, it auto-switches to a light two-stage index (e.g.,
spline index <span class="math inline">\(\to\)</span> isotonic) if
regional miscalibration is detected. When applying <span
class="math inline">\(f\)</span> to other policies, verify Assumption <a
href="#assum:transport" data-reference-type="ref"
data-reference="assum:transport">[assum:transport]</a> with the
Transportability Audit (Diagnostic 5, §<a href="#diag:transport"
data-reference-type="ref"
data-reference="diag:transport">[diag:transport]</a>).</p>
<section id="why-monotone-isotonic-by-default" class="level4">
<h4>Why monotone (isotonic) by default?</h4>
<p>Isotonic regression enforces the single structural belief you want:
<em>higher judge score <span class="math inline">\(\Rightarrow\)</span>
no worse expected outcome</em>. Unlike parametric links (sigmoid, beta)
that impose a rigid shape and risk misspecification, isotonic adapts to
the data while preserving ranking sanity. It is mean-preserving by
construction (via projection onto the monotone cone), so your KPI stays
on the right scale without post-hoc adjustments. With small oracle
slices (5-10% coverage is often sufficient), this shape constraint buys
strong stability—no spurious wiggles, no overfitting. The step-function
output also makes edge fragility immediately visible, enabling targeted
label collection. When monotonicity fails (e.g., length bias at fixed
<span class="math inline">\(S\)</span>), the two-stage variant learns a
smooth transformation first.</p>
</section>
<section id="operator-artifacts-to-check" class="level4">
<h4>Operator artifacts to check:</h4>
<ul>
<li><p><strong>Reliability:</strong> out-of-fold calibration curve and
regional error (low/mid/high <span
class="math inline">\(S\)</span>).</p></li>
<li><p><strong>Mean preservation:</strong> oracle mean of <span
class="math inline">\(f(S)\)</span> matches mean of <span
class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Score coverage:</strong> fraction of evaluated <span
class="math inline">\(S\)</span> that lies inside the oracle <span
class="math inline">\(S\)</span>-range; inspect boundary
slopes.</p></li>
</ul>
</section>
<section id="sec:two-stage-autocal" class="level3">
<h3>Two-Stage (with covariates)</h3>
<p>When <span class="math inline">\(\E[Y\mid S]\)</span> is only
approximately monotone or exhibits slice effects (e.g., length or domain
bias at fixed <span class="math inline">\(S\)</span>), we use a light
two-stage procedure that preserves the single structural belief we
trust—<em>monotonicity in a one-dimensional risk index</em>—while
allowing covariate adjustment.</p>
<section id="construction-cross-fitted." class="level4">
<h4>Construction (cross-fitted).</h4>
<p>Let <span class="math inline">\(X_{\mathrm{cov}}\)</span> denote
optional, low-cardinality covariates (e.g., response length, prompt
family). For each oracle fold <span
class="math inline">\(k\)</span>:</p>
<ol>
<li><p><strong>Risk index (OOF):</strong> fit a low-capacity <span
class="math inline">\(g_{\setminus k}\)</span> on folds <span
class="math inline">\(\neq k\)</span> to predict <span
class="math inline">\(Y\)</span> from <span class="math inline">\((S,
X_{\mathrm{cov}})\)</span> and compute <span class="math inline">\(T_i =
g_{\setminus k}(S_i, X_{i,\mathrm{cov}})\)</span> for <span
class="math inline">\(i \in k\)</span>.</p></li>
<li><p><strong>Uniformize:</strong> <span class="math inline">\(U_i =
\widehat{\mathrm{ECDF}}_{\setminus k}(T_i) \in [0,1]\)</span> using the
training folds only.</p></li>
<li><p><strong>Shape-enforce (isotonic):</strong> fit <span
class="math inline">\(h_{\setminus k} = \arg\min_{h \in \mathcal{M}}
\sum_{j \notin k} (Y_j - h(U_j))^2\)</span> with <span
class="math inline">\(\mathcal{M}\)</span> the nondecreasing functions;
set <span class="math display">\[f_{\setminus k}(S_i,
X_{i,\mathrm{cov}})=h_{\setminus k}(U_i).\]</span></p></li>
<li><p><strong>Mean-preserve:</strong> recenter so <span
class="math inline">\(\frac{1}{|{\text{train}}|}\sum_{j\notin k}
h_{\setminus k}(U_j)=\frac{1}{|{\text{train}}|}\sum_{j\notin k}
Y_j\)</span>.</p></li>
</ol>
<p>At inference time we use the OOF <span
class="math inline">\(f_{\setminus k}\)</span> for oracle points and the
pooled fit for evaluation points.</p>
</section>
<section id="assumption-single-index-monotone." class="level4">
<h4>Assumption (single-index monotone).</h4>
<p>There exists <span
class="math inline">\(g^\star(S,X_{\mathrm{cov}})\)</span> and
nondecreasing <span class="math inline">\(\mu^\star\)</span> such that
<span class="math inline">\(\E[Y\mid
S,X_{\mathrm{cov}}]=\mu^\star(g^\star(S,X_{\mathrm{cov}}))\)</span>.
Then <span class="math inline">\(f\)</span> is <span
class="math inline">\(L^2\)</span>-consistent; see §<a
href="#assump:j2mx" data-reference-type="ref"
data-reference="assump:j2mx">[assump:j2mx]</a>.</p>
</section>
<section id="why-it-helps." class="level4">
<h4>Why it helps.</h4>
<p>(i) Captures slice heterogeneity with one d.o.f.; (ii)
rank–uniformization makes the mapping scale-free and density-stable;
(iii) preserves interpretability and mean; (iv) plays well with and
regional diagnostics.</p>
</section>
<section id="when-to-enable." class="level4">
<h4>When to enable.</h4>
<p>Persistent regional error in reliability plots, slice effects at
fixed <span class="math inline">\(S\)</span>, or edge fragility
(flat/steep boundary slopes).</p>
</section>
<section id="api-code-parity." class="level4">
<h4>API (code parity).</h4>
<pre class="python" data-language="Python"><code>from cje import analyze_dataset

results = analyze_dataset(
    fresh_draws_dir=&quot;responses/&quot;,
    calibration_mode=&quot;auto&quot;,                 # &quot;two_stage&quot; to force
    calibration_covariates=[&quot;response_length&quot;,&quot;domain&quot;],
    include_response_length=True             # auto-compute from response text
)</code></pre>
</section>
</section>
</section>
<section id="estimator-and-paired-contrasts" class="level2">
<h2>Estimator and paired contrasts</h2>
<p>With <span class="math inline">\(R_{\pi i} = f(S_{\pi i})\)</span> on
shared prompts, <span class="math display">\[\begin{aligned}
\est{V}_{\dm}(\pi) &amp;= \frac{1}{m} \sum_{i=1}^m R_{\pi i}, \\
\est{\Delta}(\pi, \pi&#39;) &amp;= \frac{1}{m} \sum_{i=1}^m \left(
R_{\pi i} - R_{\pi&#39; i} \right).
\end{aligned}\]</span></p>
<p>Use paired standard errors for <span
class="math inline">\(\est{\Delta}\)</span> (they are tighter because
prompt-level noise cancels).</p>
</section>
<section id="uncertainty-that-stays-honest-oua" class="level2">
<h2>Uncertainty that stays honest (OUA)</h2>
<p>Treating <span class="math inline">\(f\)</span> as fixed understates
uncertainty when the oracle slice is small. <strong>(oracle-uncertainty
aware)</strong> inference refits <span class="math inline">\(f\)</span>
across label folds and adds the induced variance: <span
class="math display">\[\SE^2_{\text{total}} = \Var_{\text{main}} +
\Var_{\oua},\]</span> and forms 95% CIs using a
<strong>t</strong>-critical value with Satterthwaite df when
clusters/oracle folds are small; normal quantile otherwise (see §<a
href="#sec:uncertainty" data-reference-type="ref"
data-reference="sec:uncertainty">[sec:uncertainty]</a>). Always show the
share <span
class="math inline">\(\Var_{\oua}/\SE^2_{\text{total}}\)</span>.</p>
</section>
<section id="diagnostics-highest-leverage-quick-fixes" class="level2">
<h2>Diagnostics (highest leverage) &amp; quick fixes</h2>
<ol>
<li><p><strong>Score coverage</strong> (fraction of evaluated <span
class="math inline">\(S\)</span> inside the labeled range; boundary
slopes).</p>
<p><em>Fix:</em> add a small number of labels targeting uncovered <span
class="math inline">\(S\)</span> bins; consider narrowing the prompt set
to the intended deployment slice.</p></li>
<li><p><strong>Calibration reliability</strong> (OOF curve + regional
error).</p>
<p><em>Fix:</em> enable the two-stage fallback; add a coarse index
(prompt family / length); collect a handful of labels in problematic
regions.</p></li>
<li><p><strong>Judge stability / drift</strong> (rank stability on a
small anchor set; unchanged judge config).</p>
<p><em>Fix:</em> freeze judge version during the eval window; if drift
appears, refresh the oracle slice and re-fit .</p></li>
<li><p><strong>share</strong> (how much labels drive the CI).</p>
<p><em>Fix:</em> if large, add labels (especially in sparse <span
class="math inline">\(S\)</span> regions); otherwise more prompts help
most.</p></li>
</ol>
</section>
<section id="reporting-template-dm" class="level2">
<h2>Reporting template (DM)</h2>
<p>For each policy (and for key pairwise contrasts), report:</p>
<ul>
<li><p>Calibrated mean <span
class="math inline">\(\est{V}_{\dm}(\pi)\)</span> with 95% CI (paired
for <span class="math inline">\(\est{\Delta}\)</span>).</p></li>
<li><p>share; note if paired design was used.</p></li>
<li><p>reliability snippet (with regional error) and the score-coverage
badge.</p></li>
<li><p>Any drift checks performed on the judge.</p></li>
</ul>
</section>
<section id="sample-size-label-planner-rules-of-thumb" class="level2">
<h2>Sample size &amp; label planner (rules of thumb)</h2>
<p>Let <span class="math inline">\(\hat{\sigma}_R^2\)</span> be the
prompt-level variance of <span class="math inline">\(R\)</span> for a
policy on the evaluation set. Then a rough CI half-width for one policy
is <span class="math display">\[\text{HalfWidth} \approx 1.96
\sqrt{\frac{\hat{\sigma}_R^2}{m} + \Var_{\oua}}.\]</span></p>
<p>More prompts shrink the first term; more labels shrink the second ().
For pairwise contrasts, replace <span
class="math inline">\(\hat{\sigma}_R^2\)</span> with the variance of
<span class="math inline">\(R_\pi - R_{\pi&#39;}\)</span> (often smaller
due to pairing).</p>
</section>
<section id="common-pitfalls-and-how-to-avoid-them" class="level2">
<h2>Common pitfalls (and how to avoid them)</h2>
<ul>
<li><p><strong>Different prompts across policies.</strong> Always
evaluate on the same <span class="math inline">\(\mathcal{X}\)</span>;
match seeds if stochastic.</p></li>
<li><p><strong>Thin coverage at the edges of <span
class="math inline">\(S\)</span>.</strong> Target labels to those bins
or focus the prompt set; report the coverage badge.</p></li>
<li><p><strong>Calibration curve looks good overall but bad in one
region.</strong> Use two-stage or add a coarse index and a few
labels.</p></li>
<li><p><strong>Large share.</strong> Label more—prioritize regions where
<span class="math inline">\(S\)</span> is sparse or where policy
comparisons matter.</p></li>
</ul>
</section>
<section id="minimal-recipe-pseudocode" class="level2">
<h2>Minimal recipe (pseudocode)</h2>
<pre class="python" data-language="Python"
data-caption="DM Recipe"><code># Inputs: prompts X, candidate policies Pi, judge s(.), oracle slice {(S, Y)}
# Output: calibrated means, paired contrasts, and CIs with OUA

# 1) Fit AutoCal-R on oracle (cross-fitted); persist reliability + coverage edges
f = fit_autocal_r(oracle_S, oracle_Y, K=5)

# 2) For each policy pi:
for pi in candidates:
    for i in range(m):
        A_pi[i] = generate(pi, X[i])     # same prompts for all
        S_pi[i] = judge_score(X[i], A_pi[i])
        R_pi[i] = f(S_pi[i])

    V_hat[pi] = np.mean(R_pi)

# 3) For contrasts (pi, pi&#39;):
for (pi, pi_prime) in pairs:
    Delta_hat = np.mean(R_pi - R_pi_prime)    # paired difference
    SE_main = np.std(R_pi - R_pi_prime) / np.sqrt(m)

# 4) OUA: refit AutoCal-R across K folds of oracle; recompute step (2-3)
for k in range(K):
    f_k = fit_autocal_r(oracle_minus_fold_k)
    # ... recompute V_hat with f_k ...
SE_total_squared = SE_main**2 + Var_OUA

# 5) Report V_hat, Delta_hat, 95% CI using SE_total, OUA share,
#    reliability, coverage</code></pre>
</section>
<section id="scope-notes" class="level2">
<h2>Scope notes</h2>
<p>is on-policy for your chosen prompt distribution. If deployment
prompts differ materially, treat that as a distribution-shift question:
either adapt the prompt set to match deployment or build a small label
slice representative of the target domain and re-fit .</p>
</section>
</section>
</body>
</html>
