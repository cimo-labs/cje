<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Large language models (LLMs) are increasingly used as judges: a model
(or rubricized prompt) assigns a scalar score <span
class="math inline">\(S = s(X, A)\)</span> to an answer <span
class="math inline">\(A\)</span> given a prompt <span
class="math inline">\(X\)</span>. This setup is attractive: it is fast,
cheap, and often strongly correlated with human preferences. As a
result, many teams now compare policies by averaging raw judge scores
and declaring a winner.</p>
<section id="what-practitioners-really-want" class="level2">
<h2>What practitioners really want</h2>
<p>In day-to-day evaluation work there are two goals: (i) rank candidate
policies on a shared prompt set; and (ii) when stakes justify it,
estimate levels for a KPI that matters (e.g., pass rate), with a
confidence interval. Both questions are causal: <em>what would happen if
we shipped policy <span class="math inline">\(\pi\)</span>?</em>
Formally, the target is <span class="math display">\[V(\pi) =
\E[Y(\pi)],\]</span> where <span class="math inline">\(Y \in [0,
1]\)</span> is the outcome of interest under the policy we would
deploy.</p>
</section>
<section id="why-the-heuristic-fails" class="level2">
<h2>Why the heuristic fails</h2>
<p>Naively averaging judge scores is a heuristic that breaks in
predictable ways:</p>
<ul>
<li><p><strong>Wrong scale.</strong> Judge scores are not on your KPI
scale; deltas can be misleading.</p></li>
<li><p><strong>Hidden drift and slice bias.</strong> The meaning of a
score can change across time, prompt families, or length; averages hide
this.</p></li>
<li><p><strong>No uncertainty.</strong> Without CIs it is hard to tell
real uplifts from noise, especially at small label budgets.</p></li>
<li><p><strong>Off-policy illusion.</strong> Reusing one judged log to
assess many candidates without correction answers the wrong
counterfactual (it reflects the logger, not the candidate).</p></li>
</ul>
</section>
<section id="how-cje-works" class="level2">
<h2>How CJE works</h2>
<p><strong>Causal Judge Evaluation (CJE)</strong> turns judge scores
into reliable estimates with confidence intervals. The core
workflow:</p>
<ol>
<li><p><strong>Calibrate:</strong> Learn a mapping from judge scores to
oracle labels on a small random sample (AutoCal-R).</p></li>
<li><p><strong>Evaluate:</strong> For on-policy comparison
(<strong>Direct Mode</strong>), generate responses from each policy and
average calibrated scores. For counterfactual inference
(<strong>Off-Policy</strong>), reuse logged data with importance
weighting.</p></li>
<li><p><strong>Report:</strong> Confidence intervals that account for
all uncertainty (OUA).</p></li>
</ol>
<p>Most users only need <strong>Direct Mode</strong>: generate outputs,
calibrate judge scores, report CIs. For off-policy evaluation, CJE
stabilizes importance weights (SIMCal) and optionally adds a critic (DR)
for robustness. See §3 for details.</p>
</section>
<section id="quick-start-direct-mode-for-model-comparison"
class="level2">
<h2>Quick Start: Direct Mode for Model Comparison</h2>
<p><strong>Just want to compare models on an eval set?</strong> This is
the fastest path to reliable estimates with confidence intervals.</p>
<section id="what-you-need" class="level4">
<h4>What you need:</h4>
<ul>
<li><p>500–2000 prompts representative of deployment</p></li>
<li><p>Responses from each policy you want to compare</p></li>
<li><p>Judge scores for all responses (any scale works)</p></li>
<li><p>50–150 oracle labels (ground truth on a random sample)</p></li>
</ul>
</section>
<section id="steps" class="level4">
<h4>Steps:</h4>
<ol>
<li><p>Generate outputs from each policy on the same prompts (paired
sampling reduces variance).</p></li>
<li><p>Score all outputs with your judge (fixed version, frozen
config).</p></li>
<li><p>Label a random sample of 50–150 examples with ground truth <span
class="math inline">\(Y\)</span>.</p></li>
<li><p>Run <strong>Direct Mode</strong> (§2)—CJE learns the judge<span
class="math inline">\(\to\)</span>oracle calibration <span
class="math inline">\(f(S)\)</span> automatically via
AutoCal-R.</p></li>
<li><p>Check three core diagnostics: coverage (§4.1), calibration
(§4.2), judge stability (§4.3).</p></li>
<li><p>Report estimates with 95% CIs (includes OUA to account for
calibrator uncertainty).</p></li>
</ol>
</section>
<section id="what-to-read" class="level4">
<h4>What to read:</h4>
<ul>
<li><p><strong>Essential:</strong> §2 (Direct Mode), §4.1–4.4 (core
diagnostics), §6.1 (DM workflow).</p></li>
<li><p><strong>Skip for now:</strong> §3 (off-policy methods), §4.5–4.7
(weight diagnostics).</p></li>
</ul>
<p>You can return to off-policy methods (§3) later if you want
counterfactual inference (“What if we deployed policy X?”) without
regenerating outputs.</p>
</section>
</section>
<section id="notation-at-a-glance" class="level2">
<h2>Notation at a glance</h2>
<div id="tab:notation">
<table>
<caption>Key notation and terminology</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Symbol/Term</strong></th>
<th style="text-align: left;"><strong>Meaning</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="2" style="text-align: left;"><em>Core symbols (all
modes):</em></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span
class="math inline">\(X\)</span></td>
<td style="text-align: left;">Prompt (input)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span
class="math inline">\(A\)</span></td>
<td style="text-align: left;">Response (action/output)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span
class="math inline">\(S\)</span></td>
<td style="text-align: left;">Judge score <span class="math inline">\(S
= s(X, A) \in \mathbb{R}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span
class="math inline">\(Y\)</span></td>
<td style="text-align: left;">Oracle label (ground truth outcome) <span
class="math inline">\(\in [0, 1]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span
class="math inline">\(R\)</span></td>
<td style="text-align: left;">Calibrated reward <span
class="math inline">\(R = f(S) \in [0, 1]\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>AutoCal-R</strong></td>
<td style="text-align: left;"><strong>AutoCal-R</strong>: Automatic
Calibration for Rewards</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Isotonic map <span
class="math inline">\(f: S \to [0,1]\)</span> learned from oracle
slice</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>OUA</strong></td>
<td style="text-align: left;"><strong>OUA</strong>: Oracle-Uncertainty
Aware inference</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Accounts for calibrator noise in
confidence intervals</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>DM</strong></td>
<td style="text-align: left;">Direct Modeling (on-policy, fresh draws
only)</td>
</tr>
<tr class="even">
<td colspan="2" style="text-align: left;"><em>Off-policy symbols (§3
only):</em></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span
class="math inline">\(\pi_0\)</span></td>
<td style="text-align: left;">Logging policy (baseline, deployed)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span
class="math inline">\(\pi&#39;\)</span></td>
<td style="text-align: left;">Candidate policy (target for
evaluation)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span
class="math inline">\(W_i\)</span></td>
<td style="text-align: left;">Raw importance weight <span
class="math inline">\(W_i = \pi&#39;(A_i \mid X_i) / \pi_0(A_i \mid
X_i)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span
class="math inline">\(\tilde{w}_i\)</span></td>
<td style="text-align: left;">Stabilized, mean-one weight after
SIMCal</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span
class="math inline">\(w^{\text{sn}}_i\)</span></td>
<td style="text-align: left;">Self-normalized (Hájek) weight <span
class="math inline">\(w^{\text{sn}}_i = W_i / \bar{W}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>SIMCal</strong></td>
<td style="text-align: left;"><strong>SIMCal</strong>: Score-Indexed
Monotone Calibration</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">Stabilizes weights via monotone projection
+ variance cap</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>IPS</strong></td>
<td style="text-align: left;">Importance Sampling (off-policy, logged
data)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>DR</strong></td>
<td style="text-align: left;">Doubly Robust (off-policy + critic/outcome
model)</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="when-you-need-more-off-policy-evaluation" class="level2">
<h2>When you need more: Off-policy evaluation</h2>
<p>Direct Mode requires generating fresh outputs for each policy. If you
want to answer counterfactual questions without regenerating—“What if we
deployed policy X instead of our current baseline?”—you need off-policy
methods:</p>
<ul>
<li><p><strong>Calibrated IPS</strong> reuses a single logged dataset to
evaluate multiple candidate policies by reweighting with importance
ratios. Requires per-sequence log probabilities.</p></li>
<li><p><strong>Calibrated DR</strong> combines importance weighting with
an outcome model for robustness and tighter confidence intervals.
Requires both logged data and a small set of fresh draws.</p></li>
</ul>
<p>Off-policy methods enable fast iteration (no regeneration), but
introduce complexity (importance weights, overlap diagnostics). See §3
for full details, assumptions, and diagnostics. Most users should start
with Direct Mode and adopt off-policy methods only when needed.</p>
</section>
<section id="core-diagnostics" class="level2">
<h2>Core diagnostics</h2>
<p>All CJE modes share three essential diagnostics that catch the most
common failure modes:</p>
<ol>
<li><p><strong>Score coverage:</strong> Does the oracle slice span the
evaluation <span class="math inline">\(S\)</span>-range? (fix: add
labels in uncovered bins)</p></li>
<li><p><strong>Calibration reliability:</strong> Is <span
class="math inline">\(f(S)\)</span> accurate across the <span
class="math inline">\(S\)</span> spectrum? (fix: two-stage AutoCal-R or
add labels in problematic regions)</p></li>
<li><p><strong>Judge stability:</strong> Is the judge’s scoring
consistent over time? (fix: freeze judge version, refresh oracle if
drift detected)</p></li>
</ol>
<p>These three checks apply whether you’re using Direct Mode or
off-policy methods. For off-policy evaluation (§3), three additional
diagnostics are required:</p>
<ol>
<li><p><strong>Effective sample size (ESS):</strong> Do we have enough
effective samples after reweighting? (fix: cohort restriction or switch
to DR)</p></li>
<li><p><strong>Tail heaviness:</strong> Are importance weights
catastrophically heavy-tailed? (fix: SIMCal tuning or overlap-aware
cohorting)</p></li>
<li><p><strong>DR orthogonality:</strong> Is the outcome model
orthogonal to the weights? (fix: improve critic or revisit weight
calibration)</p></li>
</ol>
<p>See §4 for detailed thresholds, interpretation, and fixes for each
diagnostic.</p>
</section>
<section id="what-cje-provides" class="level2">
<h2>What CJE provides</h2>
<p>CJE gives practitioners a complete workflow for reliable LLM
evaluation:</p>
<ul>
<li><p><strong>Three analysis modes:</strong> Direct Mode for on-policy
comparison (simplest), Calibrated IPS for off-policy reuse, Calibrated
DR for maximum accuracy when you have both logged data and fresh
draws.</p></li>
<li><p><strong>Automatic calibration:</strong> AutoCal-R maps judge
scores to oracle labels on a small random sample, putting estimates on
the right scale.</p></li>
<li><p><strong>Honest confidence intervals:</strong> OUA accounts for
calibrator uncertainty, not just sampling noise.</p></li>
<li><p><strong>Six diagnostics:</strong> Three core checks (coverage,
calibration, judge stability) plus three off-policy checks (ESS, tail
heaviness, orthogonality). Each includes thresholds, interpretation, and
concrete fixes.</p></li>
<li><p><strong>Off-policy robustness:</strong> SIMCal stabilizes
importance weights via monotone projection, and DR adds an outcome model
for double robustness.</p></li>
</ul>
</section>
<section id="scope-and-positioning" class="level2">
<h2>Scope and positioning</h2>
<p>This playbook focuses on practical implementation: assumptions in
plain English, minimal recipes, diagnostics with actionable fixes, and
reporting templates. Advanced material (theoretical guarantees,
projection properties, semiparametric efficiency) lives in the
appendix.</p>
<p>CJE builds on the treatment effect estimation literature,
particularly work on efficient estimation with surrogates and limited
outcome data <span class="citation" data-cites="kallus2024role"></span>.
Like that work, we avoid restrictive surrogacy conditions and instead
study efficiency gains when surrogates supplement—rather than
replace—primary outcomes under standard missing-at-random
assumptions.</p>
</section>
<section id="readers-guide" class="level2">
<h2>Reader’s guide</h2>
<section id="for-quick-model-comparison" class="level4">
<h4>For quick model comparison:</h4>
<p>Start with the Quick Start (§1.5), then read §2 (Direct Mode),
§4.1–4.4 (core diagnostics), and §6.1 (DM workflow). Skip §3 and
§4.5–4.7 unless you later need counterfactual inference.</p>
</section>
<section id="for-comprehensive-coverage" class="level4">
<h4>For comprehensive coverage:</h4>
<p>We begin with DM—the default when you can generate on shared
prompts—then show how to reuse judged logs with Calibrated IPS and
Calibrated DR, followed by diagnostics, assumptions for causal
interpretation, a practical playbook, compact case studies, and
implementation notes.</p>
</section>
</section>
</section>
