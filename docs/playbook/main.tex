\documentclass[11pt]{article}

% Load CJE style
\usepackage{style/cje}

% Version command (auto-generated by Makefile)
\input{VERSION}

% Header/Footer setup
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textit{CJE Practitioners' Playbook}}
\fancyhead[R]{\textit{v\cjeversion}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% Title and author
\title{%
  Causal Judge Evaluation (CJE): \\
  A Practitioners' Playbook for Reliable LLM-as-Judge Evaluation \\
  \large Version \cjeversion
}
\author{Eddie Landesberg \\ \texttt{eddie@cimolabs.com}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
LLM-as-judge has become the fastest way to compare model policies, yet most deployments still treat raw judge scores as heuristics: averages are taken at face value, failure modes are opaque, and confidence intervals are absent. This paper presents \textbf{Causal Judge Evaluation (CJE)}, a practitioner's playbook that turns judge scores into causally interpretable estimates and reliable rankings with honest uncertainty.

CJE centers evaluation on a single question---\emph{what would the KPI be if we deployed policy $\pi$?}---and provides three complementary analysis modes: \textbf{Direct Modeling (DM)} for on-policy evaluation on a shared prompt set; \textbf{Calibrated IPS} for reusing a single judged log across many candidates; and \textbf{Calibrated DR} that combines a critic with stabilized weights for robustness and tighter intervals.

Two light-weight components make the system practical. \textbf{\autocal} learns a mean-preserving, largely monotone mapping from judge score to outcome on a small labeled ``oracle'' slice, so estimates are on the right scale. \textbf{\oua} (oracle-uncertainty aware) inference then propagates calibration noise into confidence intervals. For off-policy reuse, \textbf{\simcal} stabilizes importance weights with a mean-one, score-indexed projection that raises effective sample size and tames tails without changing the target.

To keep operators in control, CJE surfaces a short list of high-leverage diagnostics---score coverage, calibration reliability, ESS and tail heaviness for IPS/DR, and a DR orthogonality check---each paired with concrete fixes (e.g., targeted labeling, cohort restriction, stronger critics). The paper spells out the assumptions required for DM, IPS, and DR to be read causally, offers copy-and-run recipes, and ships minimal artifacts for audit and reproducibility.
\end{abstract}

\tableofcontents
\newpage

% Include sections
\input{sections/01_introduction}
\input{sections/02_dm}
\input{sections/03_offpolicy}
\input{sections/04_diagnostics}
\input{sections/05_assumptions}
\input{sections/06_playbook}
\input{sections/07_cases}
\input{sections/08_implementation}
\input{sections/09_limitations}
\input{sections/10_conclusion}

% Bibliography
\bibliographystyle{plain}
\bibliography{references}

% Footer note on last page
\vfill
\begin{center}
\small
\textit{This document was auto-generated from the CJE repository at tag v\cjeversion} \\
\url{https://github.com/cimo-labs/cje/tree/v\cjeversion/docs/playbook}
\end{center}

\end{document}
