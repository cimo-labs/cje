{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CJE Quick Start: Direct Mode\n",
    "\n",
    "**Simple on-policy evaluation with judge-based scoring**\n",
    "\n",
    "This notebook shows you:\n",
    "1. What the data looks like\n",
    "2. How to run Direct Mode (simplest!)\n",
    "3. How to compare policies with confidence intervals\n",
    "\n",
    "**What is Direct Mode?**\n",
    "\n",
    "Direct Mode answers: *\"Which policy performs best on this evaluation set?\"*\n",
    "\n",
    "- No logprobs needed\n",
    "- No importance weighting\n",
    "- Just: Generate responses → Judge them → Calibrate → Average\n",
    "\n",
    "Perfect for:\n",
    "- Quick policy comparisons\n",
    "- Benchmarking on specific prompt sets\n",
    "- When you don't need counterfactual estimates\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_direct_mode_intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install CJE\n",
    "\n",
    "**Note for Colab users:** Colab comes with numpy 2.3+ which breaks scipy. We install compatible numpy first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab fix: Install compatible numpy first\n",
    "!pip install -q 'numpy>=2.0,<2.1' --force-reinstall\n",
    "\n",
    "# Install CJE from PyPI\n",
    "!pip install -q cje-eval\n",
    "\n",
    "# Verify installation\n",
    "import cje\n",
    "import numpy as np\n",
    "print(f\"✓ CJE version {cje.__version__} installed\")\n",
    "print(f\"✓ NumPy version {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Download Sample Data\n\n**About the Arena Dataset**\n\nWe use a sample from the [LMSYS Chatbot Arena](https://huggingface.co/datasets/agie-ai/lmsys-chatbot_arena_conversations) dataset - real user conversations with human preference judgments. This dataset was used to validate CJE in our ablation studies.\n\n**The Three Policies:**\n\n1. **`clone`** (Baseline)\n   - The original Arena responses\n   - Our logging/baseline policy\n   - Mean reward: ~0.76\n\n2. **`parallel_universe_prompt`** (Experimental)\n   - Modified system prompt: \"You are a helpful assistant from a parallel universe\"\n   - Tests whether a quirky prompt helps or hurts\n   - Mean reward: ~0.76 (similar to baseline)\n\n3. **`unhelpful`** (Adversarial Control)\n   - System prompt: \"You are a very unhelpful assistant\"\n   - Intentionally bad responses for testing\n   - Mean reward: ~0.14 (much worse, as expected)\n\n**Why include an adversarial policy?**\n- Validates that CJE correctly identifies bad policies\n- Tests robustness when target policy differs significantly from baseline\n- Demonstrates ESS diagnostics (unhelpful has poor overlap → low ESS)\n\n**Data generation:**\n- We took Arena prompts and generated fresh responses from each policy\n- Each response was scored by GPT-4 as judge (0-1 score)\n- A subset has oracle labels (human preferences) for calibration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data directory\n",
    "DATA_DIR = Path(\"arena_sample\")\n",
    "FRESH_DRAWS_DIR = DATA_DIR / \"fresh_draws\"\n",
    "FRESH_DRAWS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Base URL for sample data\n",
    "BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
    "\n",
    "# Download fresh draws for each policy\n",
    "policies = {\n",
    "    \"clone\": \"clone_responses.jsonl\",\n",
    "    \"parallel_universe_prompt\": \"parallel_universe_prompt_responses.jsonl\",\n",
    "    \"unhelpful\": \"unhelpful_responses.jsonl\"\n",
    "}\n",
    "\n",
    "for policy, filename in policies.items():\n",
    "    url = f\"{BASE_URL}/fresh_draws/{filename}\"\n",
    "    path = FRESH_DRAWS_DIR / filename\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, path)\n",
    "    print(f\"✓ Downloaded {filename}\")\n",
    "\n",
    "print(f\"\\n✓ All data downloaded to: {FRESH_DRAWS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understand the Data Structure\n",
    "\n",
    "Let's look at what Direct Mode data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load one policy's responses to inspect\n",
    "with open(FRESH_DRAWS_DIR / \"clone_responses.jsonl\") as f:\n",
    "    samples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "print(f\"\\nExample sample (first one):\")\n",
    "print(json.dumps(samples[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in each sample?\n",
    "\n",
    "**Required fields:**\n",
    "- `prompt_id`: Unique identifier for the prompt\n",
    "- `prompt`: The input text\n",
    "- `response`: The model's generated response\n",
    "- `judge_score`: LLM judge's evaluation score (0-1)\n",
    "\n",
    "**Optional but valuable:**\n",
    "- `oracle_label`: Ground truth label (0-1) for calibration\n",
    "  - Having 5-10% of samples with oracle labels is usually enough!\n",
    "  - CJE uses these to calibrate judge scores (AutoCal-R)\n",
    "\n",
    "**Not needed for Direct Mode:**\n",
    "- `base_policy_logprob`: Only needed for IPS/DR modes\n",
    "- `target_policy_logprobs`: Only needed for IPS/DR modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check oracle coverage\n",
    "n_with_oracle = sum(1 for s in samples if s.get('oracle_label') is not None)\n",
    "coverage = n_with_oracle / len(samples) if samples else 0\n",
    "\n",
    "print(f\"Oracle label coverage: {n_with_oracle}/{len(samples)} ({coverage:.1%})\")\n",
    "print(f\"\\nThis is {'plenty' if coverage >= 0.05 else 'a bit low'} for calibration!\")\n",
    "print(f\"Tip: Even 5-10% oracle coverage often gives good calibration.\")\n",
    "\n",
    "# Show judge score distribution\n",
    "judge_scores = [s['judge_score'] for s in samples if 'judge_score' in s]\n",
    "print(f\"\\nJudge scores (clone policy):\")\n",
    "print(f\"  Mean: {np.mean(judge_scores):.3f}\")\n",
    "print(f\"  Std:  {np.std(judge_scores):.3f}\")\n",
    "print(f\"  Range: [{min(judge_scores):.3f}, {max(judge_scores):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Direct Mode\n",
    "\n",
    "Now let's run CJE in Direct Mode to compare our three policies.\n",
    "\n",
    "**What CJE does:**\n",
    "1. Loads fresh draws for all policies\n",
    "2. Calibrates judge scores using oracle labels (AutoCal-R)\n",
    "3. Computes mean calibrated reward per policy\n",
    "4. Reports confidence intervals accounting for all uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje import analyze_dataset\n",
    "\n",
    "# Run Direct Mode - just point to fresh draws directory!\n",
    "results = analyze_dataset(\n",
    "    fresh_draws_dir=str(FRESH_DRAWS_DIR),\n",
    "    estimator=\"auto\",  # Auto-detects Direct mode\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Direct Mode Results\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Output\n",
    "\n",
    "Let's break down what we see above:\n",
    "\n",
    "**Calibration Info:**\n",
    "- CJE found samples with oracle labels\n",
    "- It fit an isotonic regression: judge_score → oracle_label\n",
    "- RMSE tells you how well judges match ground truth\n",
    "- Mode (monotone/linear/none) is auto-selected based on data\n",
    "\n",
    "**Why calibration matters:**\n",
    "- Raw judge scores may be biased (too high or too low)\n",
    "- Calibration maps judges to oracle scale\n",
    "- This makes estimates comparable to production metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View and Compare Results\n",
    "\n",
    "Now let's look at the policy estimates and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "policies = results.metadata['target_policies']\n",
    "estimates = results.estimates\n",
    "std_errors = results.standard_errors\n",
    "\n",
    "# Display results table\n",
    "print(f\"{'Policy':<35} {'Estimate':<12} {'Std Error':<12} {'95% CI':<20}\")\n",
    "print(\"-\" * 79)\n",
    "for i, policy in enumerate(policies):\n",
    "    est = estimates[i]\n",
    "    se = std_errors[i]\n",
    "    ci_low = est - 1.96 * se\n",
    "    ci_high = est + 1.96 * se\n",
    "    print(f\"{policy:<35} {est:>6.3f}       {se:>6.3f}       [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "\n",
    "print(\"\\n💡 Interpretation:\")\n",
    "print(\"   • Estimate = calibrated mean reward on eval set\")\n",
    "print(\"   • Std Error accounts for sampling + calibration uncertainty\")\n",
    "print(\"   • 95% CI = we're 95% confident true value is in this range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best policy\n",
    "best_idx = results.best_policy()\n",
    "best_policy = policies[best_idx]\n",
    "best_est = estimates[best_idx]\n",
    "best_se = std_errors[best_idx]\n",
    "\n",
    "print(f\"🏆 Best policy: {best_policy}\")\n",
    "print(f\"   Estimate: {best_est:.3f} ± {best_se:.3f}\")\n",
    "print(f\"   95% CI: [{best_est - 1.96*best_se:.3f}, {best_est + 1.96*best_se:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Against Baseline\n",
    "\n",
    "Let's compare each policy to the baseline using proper statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set baseline\n",
    "baseline_policy = 'clone'\n",
    "baseline_idx = policies.index(baseline_policy)\n",
    "\n",
    "print(f\"📊 Comparison to baseline ({baseline_policy}):\")\n",
    "print(f\"   Baseline: {estimates[baseline_idx]:.3f} ± {std_errors[baseline_idx]:.3f}\")\n",
    "print()\n",
    "print(f\"{'Policy':<35} {'Δ vs baseline':<15} {'p-value':<12} {'Significant?'}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for i, policy in enumerate(policies):\n",
    "    if i == baseline_idx:\n",
    "        print(f\"{policy:<35} {'(baseline)':<15} {'':<12} {''}\")\n",
    "        continue\n",
    "    \n",
    "    # Use built-in comparison (properly accounts for paired data)\n",
    "    comp = results.compare_policies(i, baseline_idx)\n",
    "    \n",
    "    sig_text = \"✓ Yes (p<0.05)\" if comp['significant'] else \"No\"\n",
    "    \n",
    "    print(f\"{policy:<35} {comp['difference']:+.4f}          \"\n",
    "          f\"{comp['p_value']:.4f}      {sig_text}\")\n",
    "\n",
    "print(\"\\n💡 Interpretation:\")\n",
    "print(\"   • Δ > 0: Policy beats baseline\")\n",
    "print(\"   • Δ < 0: Policy worse than baseline\")\n",
    "print(\"   • Significant (p<0.05): Difference is statistically reliable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Check Diagnostics\n",
    "\n",
    "Always check diagnostics to validate your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key diagnostics\n",
    "diag = results.diagnostics\n",
    "\n",
    "print(\"Diagnostics Summary\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {results.metadata.get('mode', 'N/A')}\")\n",
    "print(f\"Estimator: {results.metadata.get('estimator', 'N/A')}\")\n",
    "print(f\"\\nSample sizes:\")\n",
    "print(f\"  Total samples: {diag.n_samples_total}\")\n",
    "print(f\"  Valid samples: {diag.n_samples_valid}\")\n",
    "print(f\"  Samples per policy: {diag.n_samples_used}\")\n",
    "\n",
    "print(f\"\\nCalibration:\")\n",
    "if diag.calibration_rmse is not None:\n",
    "    print(f\"  RMSE: {diag.calibration_rmse:.4f}\")\n",
    "    print(f\"  R²: {diag.calibration_r2:.4f}\" if diag.calibration_r2 else \"\")\n",
    "    print(f\"  Oracle labels used: {diag.n_oracle_labels}\")\n",
    "    print(f\"  Coverage: {diag.calibration_coverage:.1%}\" if diag.calibration_coverage else \"\")\n",
    "else:\n",
    "    print(\"  No calibration performed (no oracle labels)\")\n",
    "\n",
    "print(\"\\n✓ Diagnostics look good!\" if diag.weight_status.name == 'GOOD' else \"\\n⚠ Check diagnostics carefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Visualize Results\n\nCJE provides built-in visualization to help communicate your findings.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Just evaluate the result to see a nice HTML table\nresults",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Jupyter Auto-Display\n\nIn Jupyter notebooks (including Colab), results automatically display as formatted HTML tables when you evaluate them:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Even simpler: use the convenience method\nfig = results.plot_estimates()\nplt.show()\n\nprint(\"\\n💡 The .plot_estimates() method automatically extracts data from the result object\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quick Plotting with Convenience Method\n\nYou can also use the convenience method on `EstimationResult` for quick plotting:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import visualization function from cje\nfrom cje import plot_policy_estimates\n\n# Extract estimates and standard errors as dictionaries\nestimates_dict = {policy: float(estimates[i]) for i, policy in enumerate(policies)}\nses_dict = {policy: float(std_errors[i]) for i, policy in enumerate(policies)}\n\n# Create forest plot with confidence intervals\nfig = plot_policy_estimates(\n    estimates=estimates_dict,\n    standard_errors=ses_dict,\n    figsize=(10, 4)\n)\n\n# Display\nimport matplotlib.pyplot as plt\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n✓ Forest plot shows point estimates with 95% confidence intervals\")\nprint(\"  Green dot = best policy\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Direct Mode Quick Reference\n\n### When to Use Direct Mode\n\n✅ **Use Direct Mode when:**\n- You want quick on-policy comparisons\n- You can generate fresh responses from each policy\n- You don't need counterfactual deployment estimates\n- You want the simplest CJE workflow\n\n❌ **Don't use Direct Mode when:**\n- You want to estimate counterfactual deployment value\n- You can't generate fresh responses (use IPS instead)\n- You need to reuse logged data (use IPS/DR instead)\n\n### Required Data\n\n```\nfresh_draws/\n  policy_a_responses.jsonl  # One file per policy\n  policy_b_responses.jsonl\n  policy_c_responses.jsonl\n```\n\nEach JSONL line:\n```json\n{\n  \"prompt_id\": \"prompt_001\",\n  \"prompt\": \"What is 2+2?\",\n  \"response\": \"2+2 equals 4.\",\n  \"judge_score\": 0.95,\n  \"oracle_label\": 1.0  // Optional but recommended (5-10% coverage is fine)\n}\n```\n\n### Code Template\n\n```python\nfrom cje import analyze_dataset\n\n# Run Direct Mode\nresults = analyze_dataset(\n    fresh_draws_dir=\"path/to/fresh_draws\",\n    estimator=\"auto\",  # Auto-detects Direct mode\n    verbose=True,\n)\n\n# View results\nfor i, policy in enumerate(results.metadata['target_policies']):\n    print(f\"{policy}: {results.estimates[i]:.3f} ± {results.standard_errors[i]:.3f}\")\n\n# Find best policy\nbest_idx = results.best_policy()\nprint(f\"Best: {results.metadata['target_policies'][best_idx]}\")\n\n# Compare to baseline\ncomparison = results.compare_policies(policy_idx, baseline_idx)\nprint(f\"Δ: {comparison['difference']:.3f}, p={comparison['p_value']:.3f}\")\n```\n\n### Key Diagnostics to Check\n\n1. **Oracle coverage**: 5-10% is usually enough for good calibration\n2. **Calibration RMSE**: Lower is better (judges match oracle well)\n3. **Sample sizes**: Make sure n ≥ 100 per policy for reliable CIs\n\n---\n\n## Next Steps: Off-Policy Evaluation\n\nReady for more advanced counterfactual estimation?\n\n**➡️ Continue to the [OPE Methods Notebook](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_ope_methods.ipynb)**\n\nLearn about:\n- **IPS Mode**: Reuse logged data for counterfactual estimates\n- **DR Mode**: Combine logged + fresh data for best accuracy\n- **ESS Diagnostics**: When to trust IPS vs switch to DR\n- **Orthogonality**: Validating doubly robust estimates\n\n---\n\n## Resources\n\n- **GitHub**: [github.com/cimo-labs/cje](https://github.com/cimo-labs/cje)\n- **Documentation**: See README and examples/\n- **Paper**: Coming soon with full technical details\n- **Questions?**: Open an issue on GitHub\n\nHappy evaluating! 🎉"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}