{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CJE Advanced: Off-Policy Evaluation\n",
        "\n",
        "**IPS and DR modes for counterfactual policy evaluation**\n",
        "\n",
        "This notebook covers advanced off-policy evaluation methods:\n",
        "\n",
        "1. **IPS Mode**: Counterfactual estimates from logged data (importance sampling)\n",
        "2. **DR Mode**: Doubly robust estimation (most accurate)\n",
        "3. **Diagnostics**: ESS, overlap, weight analysis\n",
        "4. **Comparison**: When to use IPS vs DR vs Direct\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**New to CJE?** Start with [`cje_tutorial.ipynb`](cje_tutorial.ipynb) for Direct mode basics.\n",
        "\n",
        "## Off-Policy Evaluation\n",
        "\n",
        "**Direct mode** answers: \"Which policy is best on this eval set?\"\n",
        "\n",
        "**IPS and DR** answer the counterfactual: \"What would our KPI be if we deployed policy œÄ' instead of œÄ‚ÇÄ?\"\n",
        "\n",
        "**Key difference**: Off-policy methods estimate deployment value, not just eval set performance.\n",
        "\n",
        "**When you need off-policy evaluation**:\n",
        "- Reusing logged data from production/experiments\n",
        "- Estimating deployment performance without actual deployment\n",
        "- A/B test analysis with shared logging policy\n",
        "- Cost-effective evaluation (reuse existing logs)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_advanced.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Install CJE and download the Arena sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install CJE\n",
        "!pip install -q 'numpy>=2.0,<2.1' --force-reinstall\n",
        "!pip install --no-cache-dir --upgrade cje-eval\n",
        "\n",
        "import cje\n",
        "print(f\"‚úì CJE version {cje.__version__} installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Arena sample data\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"arena_sample\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "(DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
        "\n",
        "# Download logged data (for IPS/DR modes)\n",
        "print(\"Downloading logged_data.jsonl...\")\n",
        "urllib.request.urlretrieve(\n",
        "    f\"{BASE_URL}/logged_data.jsonl\",\n",
        "    DATA_DIR / \"logged_data.jsonl\"\n",
        ")\n",
        "print(\"‚úì Downloaded logged_data.jsonl\")\n",
        "\n",
        "# Download fresh draws (for DR mode)\n",
        "fresh_draw_files = {\n",
        "    \"base\": \"base_responses.jsonl\",\n",
        "    \"clone\": \"clone_responses.jsonl\",\n",
        "    \"parallel_universe_prompt\": \"parallel_universe_prompt_responses.jsonl\",\n",
        "    \"unhelpful\": \"unhelpful_responses.jsonl\"\n",
        "}\n",
        "\n",
        "for policy, filename in fresh_draw_files.items():\n",
        "    print(f\"Downloading fresh_draws/{filename}...\")\n",
        "    urllib.request.urlretrieve(\n",
        "        f\"{BASE_URL}/fresh_draws/{filename}\",\n",
        "        DATA_DIR / \"fresh_draws\" / filename\n",
        "    )\n",
        "\n",
        "print(\"\\n‚úì All data downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Inspect Logged Data\n",
        "\n",
        "For off-policy evaluation, we need logged data with:\n",
        "- Responses from a **base/logging policy** œÄ‚ÇÄ\n",
        "- **Logprobs** from both base and target policies\n",
        "- **Judge scores** for all responses\n",
        "- **Oracle labels** (subset) for calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load logged data\n",
        "with open(DATA_DIR / \"logged_data.jsonl\") as f:\n",
        "    logged_samples = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"Logged data: {len(logged_samples)} samples\")\n",
        "print(f\"\\nExample sample:\")\n",
        "sample = logged_samples[0]\n",
        "print(f\"  Prompt: {sample['prompt'][:80]}...\")\n",
        "print(f\"  Response: {sample['response'][:100]}...\")\n",
        "print(f\"  Judge score: {sample['judge_score']}\")\n",
        "print(f\"  Oracle label: {sample.get('oracle_label', 'N/A')}\")\n",
        "print(f\"  Base logprob: {sample['base_policy_logprob']}\")\n",
        "print(f\"  Target policies: {list(sample['target_policy_logprobs'].keys())}\")\n",
        "\n",
        "# Check coverage\n",
        "n_with_oracle = sum(1 for s in logged_samples if s.get('oracle_label') is not None)\n",
        "print(f\"\\nOracle coverage: {n_with_oracle}/{len(logged_samples)} ({n_with_oracle/len(logged_samples):.1%})\")\n",
        "print(f\"\\nüí° We'll use these {n_with_oracle} oracle labels for AutoCal-R\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: IPS Mode - Importance Sampling\n",
        "\n",
        "**Inverse Propensity Score (IPS)** reweights logged data to estimate target policy performance.\n",
        "\n",
        "### How IPS Works\n",
        "\n",
        "1. **Compute importance weights**: `W = œÄ'(a|x) / œÄ‚ÇÄ(a|x)` using logprobs\n",
        "2. **Calibrate rewards**: Judge scores ‚Üí oracle scale (AutoCal-R)\n",
        "3. **Stabilize weights**: Monotone projection (SIMCal-W)\n",
        "4. **Estimate**: `VÃÇ(œÄ') = (1/n) Œ£ W_i ¬∑ R_i`\n",
        "\n",
        "### When to Use IPS\n",
        "\n",
        "**Good for**:\n",
        "- Logged data from production/experiments\n",
        "- Target policies similar to base policy (good overlap)\n",
        "- Cost-effective evaluation (no fresh generations needed)\n",
        "\n",
        "**Watch out for**:\n",
        "- Poor overlap (ESS < 10%): Switch to DR or regenerate\n",
        "- Target policies very different from base: High variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cje import analyze_dataset\n",
        "\n",
        "# IPS mode: Logged data only\n",
        "results_ips = analyze_dataset(\n",
        "    logged_data_path=str(DATA_DIR / \"logged_data.jsonl\"),\n",
        "    estimator=\"auto\",  # Auto-selects calibrated-ips\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"IPS Results\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Mode: {results_ips.metadata['mode']}\")\n",
        "print(f\"Estimator: {results_ips.metadata['estimator']}\")\n",
        "print(f\"Calibration: {results_ips.metadata.get('calibration', 'none')}\")\n",
        "print()\n",
        "\n",
        "# Show estimates\n",
        "policies_ips = results_ips.metadata['target_policies']\n",
        "print(f\"{'Policy':<30} {'Estimate':<12} {'Std Error':<12} {'95% CI':<20}\")\n",
        "print(\"-\" * 74)\n",
        "for i, policy in enumerate(policies_ips):\n",
        "    est = results_ips.estimates[i]\n",
        "    se = results_ips.standard_errors[i]\n",
        "    ci_low = est - 1.96 * se\n",
        "    ci_high = est + 1.96 * se\n",
        "    print(f\"{policy:<30} {est:>6.3f}       {se:>6.3f}       [{ci_low:.3f}, {ci_high:.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check IPS Diagnostics\n",
        "\n",
        "**ESS (Effective Sample Size)** is critical for IPS reliability:\n",
        "\n",
        "- **ESS ‚â• 50%**: Excellent overlap ‚úì\n",
        "- **ESS ‚àà [10%, 50%)**: Moderate overlap (consider DR)\n",
        "- **ESS < 10%**: Poor overlap (use DR or regenerate)\n",
        "\n",
        "ESS measures how many \"effective\" samples contribute to the estimate after reweighting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check ESS diagnostics\n",
        "print(\"IPS Diagnostics\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for policy in policies_ips:\n",
        "    ess = results_ips.diagnostics.ess_per_policy.get(policy, 0.0)\n",
        "    \n",
        "    # Traffic light assessment\n",
        "    if ess >= 0.5:\n",
        "        status = \"‚úì EXCELLENT\"\n",
        "    elif ess >= 0.1:\n",
        "        status = \"‚ö† MODERATE (consider DR)\"\n",
        "    else:\n",
        "        status = \"‚úó POOR (use DR or regenerate)\"\n",
        "    \n",
        "    print(f\"\\n{policy}:\")\n",
        "    print(f\"  ESS: {ess:.1%} {status}\")\n",
        "    \n",
        "    # Show weight statistics\n",
        "    max_weight = results_ips.diagnostics.max_weight_per_policy.get(policy)\n",
        "    if max_weight:\n",
        "        print(f\"  Max weight: {max_weight:.2f}\")\n",
        "\n",
        "print(\"\\nüí° Low ESS means estimates dominated by a few samples with high weights.\")\n",
        "print(\"   DR mode can dramatically improve reliability in this case.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: DR Mode - Doubly Robust\n",
        "\n",
        "**Doubly Robust (DR)** combines IPS with outcome modeling for improved accuracy.\n",
        "\n",
        "### How DR Works\n",
        "\n",
        "1. **Everything from IPS**: Importance weights + calibrated rewards\n",
        "2. **Outcome model**: Train ƒù(S) to predict rewards on fresh draws\n",
        "3. **Combine**: `VÃÇ_DR(œÄ') = (1/n) Œ£ [W_i ¬∑ (R_i - ƒù(S_i)) + ƒù(S_i)]`\n",
        "\n",
        "### Double Robustness Property\n",
        "\n",
        "DR is **consistent** if *either*:\n",
        "- Importance weights are correct (overlap holds), OR\n",
        "- Outcome model is correct\n",
        "\n",
        "You don't need both! This makes DR more robust than IPS alone.\n",
        "\n",
        "### When to Use DR\n",
        "\n",
        "**Perfect for**:\n",
        "- Low ESS in IPS mode (< 50%)\n",
        "- Can generate fresh responses\n",
        "- Want maximum accuracy\n",
        "- Production deployment decisions\n",
        "\n",
        "**Benefits**:\n",
        "- Lower variance than IPS (tighter CIs)\n",
        "- More robust to model misspecification\n",
        "- Best statistical properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DR mode: Logged data + fresh draws\n",
        "results_dr = analyze_dataset(\n",
        "    logged_data_path=str(DATA_DIR / \"logged_data.jsonl\"),\n",
        "    fresh_draws_dir=str(DATA_DIR / \"fresh_draws\"),\n",
        "    estimator=\"auto\",  # Auto-selects stacked-dr\n",
        "    estimator_config={\"parallel\": False},  # Disable parallel for Colab\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DR Results\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Mode: {results_dr.metadata['mode']}\")\n",
        "print(f\"Estimator: {results_dr.metadata['estimator']}\")\n",
        "print()\n",
        "\n",
        "# Show estimates\n",
        "policies_dr = results_dr.metadata['target_policies']\n",
        "print(f\"{'Policy':<30} {'Estimate':<12} {'Std Error':<12} {'95% CI':<20}\")\n",
        "print(\"-\" * 74)\n",
        "for i, policy in enumerate(policies_dr):\n",
        "    est = results_dr.estimates[i]\n",
        "    se = results_dr.standard_errors[i]\n",
        "    ci_low = est - 1.96 * se\n",
        "    ci_high = est + 1.96 * se\n",
        "    print(f\"{policy:<30} {est:>6.3f}       {se:>6.3f}       [{ci_low:.3f}, {ci_high:.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Compare IPS vs DR\n",
        "\n",
        "Let's see how much DR improves over IPS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare standard errors (smaller is better)\n",
        "print(\"Standard Error Comparison: IPS vs DR\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Policy':<30} {'IPS SE':<12} {'DR SE':<12} {'Improvement':<15}\")\n",
        "print(\"-\" * 69)\n",
        "\n",
        "for i, policy in enumerate(policies_dr):\n",
        "    # Find policy in IPS results\n",
        "    if policy in policies_ips:\n",
        "        ips_idx = policies_ips.index(policy)\n",
        "        ips_se = results_ips.standard_errors[ips_idx]\n",
        "        dr_se = results_dr.standard_errors[i]\n",
        "        \n",
        "        if dr_se < ips_se:\n",
        "            improvement = f\"‚Üì {(1 - dr_se/ips_se)*100:.0f}% SE\"\n",
        "        else:\n",
        "            improvement = \"(similar)\"\n",
        "        \n",
        "        print(f\"{policy:<30} {ips_se:>6.3f}       {dr_se:>6.3f}       {improvement:<15}\")\n",
        "\n",
        "print(\"\\nüí° DR typically reduces standard errors by 20-60% compared to IPS.\")\n",
        "print(\"   Larger improvements when ESS is low or outcome model fits well.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Mode Comparison Summary\n",
        "\n",
        "Let's compare all three modes side-by-side.\n",
        "\n",
        "**Note**: For Direct mode, we'll run it quickly here (see `cje_tutorial.ipynb` for details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Direct mode for comparison\n",
        "results_direct = analyze_dataset(\n",
        "    fresh_draws_dir=str(DATA_DIR / \"fresh_draws\"),\n",
        "    estimator=\"auto\",\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "policies_direct = results_direct.metadata['target_policies']\n",
        "\n",
        "# Compare all three modes\n",
        "print(\"Mode Comparison: Direct vs IPS vs DR\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Policy':<30} {'Direct':<10} {'IPS':<10} {'DR':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for policy in policies_dr:\n",
        "    # Get estimates from each mode\n",
        "    direct_est = \"N/A\"\n",
        "    if policy in policies_direct:\n",
        "        direct_idx = policies_direct.index(policy)\n",
        "        direct_est = f\"{results_direct.estimates[direct_idx]:.3f}\"\n",
        "    \n",
        "    ips_est = \"N/A\"\n",
        "    if policy in policies_ips:\n",
        "        ips_idx = policies_ips.index(policy)\n",
        "        ips_est = f\"{results_ips.estimates[ips_idx]:.3f}\"\n",
        "    \n",
        "    dr_idx = policies_dr.index(policy)\n",
        "    dr_est = f\"{results_dr.estimates[dr_idx]:.3f}\"\n",
        "    \n",
        "    print(f\"{policy:<30} {direct_est:>7}    {ips_est:>7}    {dr_est:>7}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Interpretation:\")\n",
        "print(\"=\"*70)\n",
        "print(\"- Direct mode: Performance on *this* eval set\")\n",
        "print(\"- IPS/DR modes: Estimated *deployment* performance\")\n",
        "print(\"- Differences are expected! They answer different questions.\")\n",
        "print(\"\\nüí° Use Direct for eval set rankings, IPS/DR for deployment decisions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Visualization\n",
        "\n",
        "Visualize DR results with confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Forest plot\n",
        "fig = results_dr.plot_estimates()\n",
        "plt.title(\"DR Mode: Policy Estimates with 95% CIs\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Choosing the Right Mode\n",
        "\n",
        "| Mode | Data Required | Estimand | Best For |\n",
        "|------|--------------|----------|----------|\n",
        "| **Direct** | Fresh draws | \"Best on eval set\" | Quick comparisons, leaderboards |\n",
        "| **IPS** | Logged data + logprobs | \"Deployment value\" | Reusing logs, good overlap (ESS > 50%) |\n",
        "| **DR** | Logged + fresh draws | \"Deployment value\" | Low ESS, max accuracy, production decisions |\n",
        "\n",
        "### Decision Tree\n",
        "\n",
        "1. **Do you need deployment estimates?**\n",
        "   - No ‚Üí Use **Direct mode**\n",
        "   - Yes ‚Üí Continue\n",
        "\n",
        "2. **Can you generate fresh responses?**\n",
        "   - Yes ‚Üí Use **DR mode** (most accurate)\n",
        "   - No ‚Üí Use **IPS mode** (check ESS!)\n",
        "\n",
        "3. **If using IPS, check ESS:**\n",
        "   - ESS ‚â• 50%: Great, IPS is reliable\n",
        "   - ESS < 50%: Consider generating fresh draws for DR\n",
        "   - ESS < 10%: Definitely use DR or regenerate\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "‚úì **IPS**: Reweights logged data using importance weights (requires logprobs)\n",
        "\n",
        "‚úì **DR**: Adds outcome modeling to IPS (needs fresh draws, most robust)\n",
        "\n",
        "‚úì **ESS**: Critical diagnostic for IPS reliability (aim for ‚â• 50%)\n",
        "\n",
        "‚úì **Double robustness**: DR works if *either* weights or outcome model is right\n",
        "\n",
        "‚úì **Variance reduction**: DR typically reduces SE by 20-60% vs IPS\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "**Documentation**:\n",
        "- [CJE README](https://github.com/cimo-labs/cje)\n",
        "- [Blog Post: Arena Experiment](https://www.cimolabs.com/blog/arena-experiment)\n",
        "- [API Reference](https://github.com/cimo-labs/cje/tree/main/cje/interface)\n",
        "\n",
        "**Want more?**\n",
        "- Explore calibration plots: `plot_calibration_comparison()`\n",
        "- Weight diagnostics: `plot_weight_dashboard_summary()`\n",
        "- DR diagnostics: `plot_dr_dashboard()`\n",
        "\n",
        "Questions? Open an issue on [GitHub](https://github.com/cimo-labs/cje/issues)!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
