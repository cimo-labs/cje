{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CJE: Off-Policy Evaluation Methods\n\n**IPS and DR modes for counterfactual policy evaluation**\n\nThis notebook covers:\n1. **IPS Mode**: Importance sampling with logged data\n2. **DR Mode**: Doubly robust estimation (IPS + outcome models)\n3. **Diagnostics**: ESS, overlap, orthogonality\n4. **When to use each method**\n\n**Prerequisites**: Complete `cje_direct_mode_intro.ipynb` first!\n\n## What is Off-Policy Evaluation?\n\n**Direct Mode** answers: \"Which policy is best *on this eval set*?\"\n\n**OPE** answers: \"What would our *production metrics* be if we deployed policy π'?\"\n\nKey difference:\n- Direct Mode: On-policy comparison (simple average)\n- OPE: Counterfactual inference (reweight logged data)\n\n**Use OPE when:**\n- You have logged data from production/baseline\n- You want to estimate deployment value before rolling out\n- Your eval set may not match production distribution"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install and Download Data\n\n**About the Arena Dataset**\n\nWe use a sample from the [LMSYS Chatbot Arena](https://huggingface.co/datasets/agie-ai/lmsys-chatbot_arena_conversations) dataset - real user conversations with human preference judgments. This dataset was used to validate CJE in our ablation studies.\n\n**The Three Policies:**\n\n1. **`clone`** (Baseline/Logging Policy)\n   - The original Arena responses\n   - This is π₀ - the policy that generated our logged data\n   - Mean reward: ~0.76\n\n2. **`parallel_universe_prompt`** (Target Policy)\n   - Modified system prompt: \"You are a helpful assistant from a parallel universe\"\n   - Tests whether a quirky prompt helps or hurts\n   - Mean reward: ~0.76 (similar to baseline)\n   - Good ESS (~40-50%) - reasonable overlap with baseline\n\n3. **`unhelpful`** (Adversarial Target)\n   - System prompt: \"You are a very unhelpful assistant\"\n   - Intentionally bad responses for testing\n   - Mean reward: ~0.14 (much worse, as expected)\n   - Poor ESS (~5-10%) - demonstrates low overlap challenges\n\n**Why this is a good OPE test case:**\n- Real production-like data (Arena conversations)\n- Mix of good overlap (parallel_universe) and poor overlap (unhelpful)\n- Demonstrates when IPS works (parallel_universe) vs when DR is needed (unhelpful)\n- Has oracle labels (human preferences) for calibration\n\n**Data structure:**\n- **Logged data**: Responses from `clone` policy with logprobs for all three policies\n- **Fresh draws**: New responses generated from each target policy on the same prompts\n- **Oracle labels**: Human preference judgments (subset of samples)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab fix: Install compatible numpy first\n",
    "!pip install -q 'numpy>=2.0,<2.1' --force-reinstall\n",
    "\n",
    "# Install CJE\n",
    "!pip install -q cje-eval\n",
    "\n",
    "import cje\n",
    "import numpy as np\n",
    "print(f\"✓ CJE version {cje.__version__}\")\n",
    "print(f\"✓ NumPy version {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR = Path(\"arena_sample\")\n",
    "FRESH_DRAWS_DIR = DATA_DIR / \"fresh_draws\"\n",
    "FRESH_DRAWS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
    "\n",
    "# Download logged data (needed for OPE!)\n",
    "print(\"Downloading logged_data.jsonl...\")\n",
    "urllib.request.urlretrieve(\n",
    "    f\"{BASE_URL}/logged_data.jsonl\",\n",
    "    DATA_DIR / \"logged_data.jsonl\"\n",
    ")\n",
    "print(\"✓ Downloaded logged_data.jsonl\")\n",
    "\n",
    "# Download fresh draws (for DR mode)\n",
    "policies = {\n",
    "    \"clone\": \"clone_responses.jsonl\",\n",
    "    \"parallel_universe_prompt\": \"parallel_universe_prompt_responses.jsonl\",\n",
    "    \"unhelpful\": \"unhelpful_responses.jsonl\"\n",
    "}\n",
    "\n",
    "for policy, filename in policies.items():\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        f\"{BASE_URL}/fresh_draws/{filename}\",\n",
    "        FRESH_DRAWS_DIR / filename\n",
    "    )\n",
    "    print(f\"✓ Downloaded {filename}\")\n",
    "\n",
    "print(f\"\\n✓ All data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understand Logged Data Structure\n",
    "\n",
    "For OPE, we need logged data with **importance weights**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load logged data\n",
    "with open(DATA_DIR / \"logged_data.jsonl\") as f:\n",
    "    logged_samples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Logged samples: {len(logged_samples)}\")\n",
    "print(f\"\\nExample logged sample:\")\n",
    "print(json.dumps(logged_samples[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Fields for OPE\n",
    "\n",
    "**Required for IPS/DR:**\n",
    "- `base_policy_logprob`: Log P(response | prompt, π₀)\n",
    "  - This is the **logging policy** (what generated the data)\n",
    "- `target_policy_logprobs`: {policy_name: log P(response | prompt, π')}\n",
    "  - These are the **target policies** we want to evaluate\n",
    "\n",
    "**Why logprobs?**\n",
    "- Importance weight: W = P(a|x, π') / P(a|x, π₀) = exp(log π' - log π₀)\n",
    "- Reweights logged data to match target policy distribution\n",
    "\n",
    "**Also used:**\n",
    "- `judge_score`: For calibration (judge → oracle)\n",
    "- `oracle_label`: Ground truth for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logprob coverage\n",
    "n_with_logprobs = sum(\n",
    "    1 for s in logged_samples \n",
    "    if s.get('base_policy_logprob') is not None\n",
    "    and s.get('target_policy_logprobs')\n",
    ")\n",
    "\n",
    "coverage = n_with_logprobs / len(logged_samples)\n",
    "print(f\"Logprob coverage: {n_with_logprobs}/{len(logged_samples)} ({coverage:.1%})\")\n",
    "print(f\"Target policies: {list(logged_samples[0]['target_policy_logprobs'].keys())}\")\n",
    "\n",
    "# Check oracle coverage\n",
    "n_with_oracle = sum(1 for s in logged_samples if s.get('oracle_label') is not None)\n",
    "oracle_coverage = n_with_oracle / len(logged_samples)\n",
    "print(f\"\\nOracle coverage: {n_with_oracle}/{len(logged_samples)} ({oracle_coverage:.1%})\")\n",
    "print(f\"→ Used for calibrating judge scores to oracle scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: IPS Mode (Importance Sampling)\n",
    "\n",
    "**What IPS does:**\n",
    "1. Compute importance weights: W_i = π'(a_i|x_i) / π₀(a_i|x_i)\n",
    "2. Calibrate judge scores: S_i → R_i (AutoCal-R)\n",
    "3. Stabilize weights: W_i → W_i^{cal} (SIMCal)\n",
    "4. Estimate: V̂(π') = Σ W_i^{cal} · R_i / Σ W_i^{cal}\n",
    "\n",
    "**Key assumption**: Overlap - logged policy must have positive probability where target policy does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje import analyze_dataset\n",
    "\n",
    "# IPS mode: logged data only\n",
    "results_ips = analyze_dataset(\n",
    "    logged_data_path=str(DATA_DIR / \"logged_data.jsonl\"),\n",
    "    estimator=\"auto\",  # Auto-detects IPS mode\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IPS Mode Results\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display IPS estimates\n",
    "policies = results_ips.metadata['target_policies']\n",
    "estimates = results_ips.estimates\n",
    "std_errors = results_ips.standard_errors\n",
    "\n",
    "print(f\"{'Policy':<35} {'Estimate':<12} {'Std Error':<12} {'95% CI':<20}\")\n",
    "print(\"-\" * 79)\n",
    "for i, policy in enumerate(policies):\n",
    "    est = estimates[i]\n",
    "    se = std_errors[i]\n",
    "    ci_low = est - 1.96 * se\n",
    "    ci_high = est + 1.96 * se\n",
    "    print(f\"{policy:<35} {est:>6.3f}       {se:>6.3f}       [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check IPS Diagnostics: ESS\n",
    "\n",
    "**ESS (Effective Sample Size)** is the most important IPS diagnostic.\n",
    "\n",
    "ESS = (Σ W)² / Σ W²\n",
    "\n",
    "**Interpretation:**\n",
    "- ESS = 100%: Perfect overlap (all samples equally useful)\n",
    "- ESS = 50%: Good overlap\n",
    "- ESS = 10%: Minimum for reliable IPS\n",
    "- ESS < 10%: Poor overlap - switch to DR or regenerate\n",
    "\n",
    "**Why ESS matters:**\n",
    "- Low ESS → high variance → wide confidence intervals\n",
    "- A few large weights dominate the estimate\n",
    "- Results become unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IPS Diagnostics: Effective Sample Size\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for policy in policies:\n",
    "    ess = results_ips.diagnostics.ess_per_policy.get(policy, 0.0)\n",
    "    max_weight = results_ips.diagnostics.max_weight_per_policy.get(policy, 0.0)\n",
    "    \n",
    "    # Status assessment\n",
    "    if ess >= 0.5:\n",
    "        status = \"✓ EXCELLENT\"\n",
    "        advice = \"IPS is reliable\"\n",
    "    elif ess >= 0.1:\n",
    "        status = \"⚠ MODERATE\"\n",
    "        advice = \"Consider DR for better accuracy\"\n",
    "    else:\n",
    "        status = \"✗ POOR\"\n",
    "        advice = \"Use DR or regenerate data\"\n",
    "    \n",
    "    print(f\"\\n{policy}:\")\n",
    "    print(f\"  ESS: {ess:.1%} {status}\")\n",
    "    print(f\"  Max weight: {max_weight:.3f}\")\n",
    "    print(f\"  → {advice}\")\n",
    "\n",
    "print(\"\\n💡 ESS tells you how many 'effective' samples you have after reweighting.\")\n",
    "print(\"   ESS ≥ 50% is excellent, ESS ≥ 10% is acceptable, ESS < 10% is risky.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: DR Mode (Doubly Robust)\n",
    "\n",
    "**What DR adds to IPS:**\n",
    "- Trains outcome model: ĝ(S) predicts reward from judge score\n",
    "- Uses fresh draws to fit model\n",
    "- Combines IPS weights with outcome predictions\n",
    "\n",
    "**DR estimator:**\n",
    "V̂_DR(π') = (1/n) Σ [W_i · (R_i - ĝ(S_i)) + ĝ(S_i)]\n",
    "\n",
    "**Why DR is better:**\n",
    "1. **Double robustness**: Consistent if *either* weights or model is correct\n",
    "2. **Variance reduction**: Model predictions reduce noise\n",
    "3. **Better with poor overlap**: Works even when ESS is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR mode: logged data + fresh draws\n",
    "results_dr = analyze_dataset(\n",
    "    logged_data_path=str(DATA_DIR / \"logged_data.jsonl\"),\n",
    "    fresh_draws_dir=str(FRESH_DRAWS_DIR),\n",
    "    estimator=\"auto\",  # Auto-detects DR mode (uses stacked-dr)\n",
    "    estimator_config={\"parallel\": False},  # Disable parallel for Colab\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DR Mode Results\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare IPS vs DR\n",
    "\n",
    "Let's see how DR improves on IPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimate Comparison: IPS vs DR\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Policy':<35} {'IPS Est':<12} {'DR Est':<12} {'Difference'}\")\n",
    "print(\"-\" * 71)\n",
    "for i, policy in enumerate(policies):\n",
    "    ips_est = results_ips.estimates[i]\n",
    "    dr_est = results_dr.estimates[i]\n",
    "    diff = dr_est - ips_est\n",
    "    print(f\"{policy:<35} {ips_est:>6.3f}       {dr_est:>6.3f}       {diff:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Standard Error Comparison: IPS vs DR\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Policy':<35} {'IPS SE':<12} {'DR SE':<12} {'Improvement'}\")\n",
    "print(\"-\" * 72)\n",
    "for i, policy in enumerate(policies):\n",
    "    ips_se = results_ips.standard_errors[i]\n",
    "    dr_se = results_dr.standard_errors[i]\n",
    "    \n",
    "    if dr_se < ips_se:\n",
    "        improvement = f\"↓ {(1 - dr_se/ips_se)*100:.0f}%\"\n",
    "    else:\n",
    "        improvement = \"(similar)\"\n",
    "    \n",
    "    print(f\"{policy:<35} {ips_se:>6.4f}       {dr_se:>6.4f}       {improvement:<12}\")\n",
    "\n",
    "print(\"\\n💡 DR typically has lower standard errors → narrower confidence intervals\")\n",
    "print(\"   The outcome model reduces variance, making estimates more precise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check DR Diagnostics: Orthogonality\n",
    "\n",
    "**Orthogonality score**: E[W · (R - ĝ)]\n",
    "\n",
    "This tests if the DR correction term has mean zero.\n",
    "\n",
    "**What it means:**\n",
    "- CI contains 0: ✓ Good - weights and model are compatible\n",
    "- CI excludes 0: ⚠ Problem - weights or model may be misspecified\n",
    "\n",
    "**Why it matters:**\n",
    "- Orthogonality ensures √n-consistency\n",
    "- Non-orthogonal DR can be biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DR Orthogonality Diagnostics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'orthogonality_scores' in results_dr.metadata:\n",
    "    ortho_scores = results_dr.metadata['orthogonality_scores']\n",
    "    \n",
    "    for policy in policies:\n",
    "        if policy in ortho_scores:\n",
    "            ortho = ortho_scores[policy]\n",
    "            score = ortho.get('score', 0)\n",
    "            ci_low = ortho.get('ci_lower', 0)\n",
    "            ci_high = ortho.get('ci_upper', 0)\n",
    "            \n",
    "            # Check if CI contains zero\n",
    "            contains_zero = ci_low <= 0 <= ci_high\n",
    "            status = \"✓ PASS\" if contains_zero else \"⚠ CHECK\"\n",
    "            \n",
    "            print(f\"\\n{policy}:\")\n",
    "            print(f\"  Score: {score:.5f}\")\n",
    "            print(f\"  95% CI: [{ci_low:.5f}, {ci_high:.5f}]\")\n",
    "            print(f\"  Status: {status}\")\n",
    "            \n",
    "            if not contains_zero:\n",
    "                print(f\"  → CI excludes 0. Consider:\")\n",
    "                print(f\"    • Checking outcome model fit (R² below)\")\n",
    "                print(f\"    • Adding more fresh draws\")\n",
    "                print(f\"    • Reviewing weight calibration\")\n",
    "else:\n",
    "    print(\"\\nOrthogonality scores not available for this estimator.\")\n",
    "    print(\"(Stacked-DR may not report per-base-estimator orthogonality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome Model Quality\n",
    "\n",
    "Check how well the outcome model predicts rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outcome Model R² (out-of-fold)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diag = results_dr.diagnostics\n",
    "\n",
    "if hasattr(diag, 'outcome_r2_range') and diag.outcome_r2_range:\n",
    "    r2_min, r2_max = diag.outcome_r2_range\n",
    "    print(f\"\\nR² range across policies: [{r2_min:.3f}, {r2_max:.3f}]\")\n",
    "    \n",
    "    if r2_max >= 0.5:\n",
    "        print(\"✓ Good outcome model fit\")\n",
    "    elif r2_max >= 0.2:\n",
    "        print(\"⚠ Moderate fit - DR still helps but may not be optimal\")\n",
    "    else:\n",
    "        print(\"✗ Poor fit - outcome model not capturing reward structure well\")\n",
    "else:\n",
    "    print(\"\\nOutcome model R² not available in diagnostics.\")\n",
    "\n",
    "print(\"\\n💡 R² measures how much variance the outcome model explains.\")\n",
    "print(\"   Higher R² → better variance reduction → narrower confidence intervals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Method Comparison Summary\n",
    "\n",
    "Let's compare all three methods side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Direct mode results for comparison\n",
    "results_direct = analyze_dataset(\n",
    "    fresh_draws_dir=str(FRESH_DRAWS_DIR),\n",
    "    estimator=\"auto\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"Method Comparison: Direct vs IPS vs DR\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Policy':<30} {'Direct':<12} {'IPS':<12} {'DR':<12} {'Best Method'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i, policy in enumerate(policies):\n",
    "    direct_est = results_direct.estimates[i]\n",
    "    ips_est = results_ips.estimates[i]\n",
    "    dr_est = results_dr.estimates[i]\n",
    "    \n",
    "    # Find lowest SE (best precision)\n",
    "    direct_se = results_direct.standard_errors[i]\n",
    "    ips_se = results_ips.standard_errors[i]\n",
    "    dr_se = results_dr.standard_errors[i]\n",
    "    \n",
    "    best_method = [\"Direct\", \"IPS\", \"DR\"][np.argmin([direct_se, ips_se, dr_se])]\n",
    "    \n",
    "    print(f\"{policy:<30} {direct_est:>6.3f}       {ips_est:>6.3f}       \"\n",
    "          f\"{dr_est:>6.3f}       {best_method}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Standard Errors (Lower is Better)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Policy':<30} {'Direct SE':<12} {'IPS SE':<12} {'DR SE':<12}\")\n",
    "print(\"-\" * 66)\n",
    "\n",
    "for i, policy in enumerate(policies):\n",
    "    direct_se = results_direct.standard_errors[i]\n",
    "    ips_se = results_ips.standard_errors[i]\n",
    "    dr_se = results_dr.standard_errors[i]\n",
    "    \n",
    "    print(f\"{policy:<30} {direct_se:>6.4f}       {ips_se:>6.4f}       {dr_se:>6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Direct Mode:**\n",
    "- Estimates performance *on the eval set*\n",
    "- Not counterfactual (doesn't estimate production value)\n",
    "- Good for: Policy comparison on fixed prompt distribution\n",
    "\n",
    "**IPS Mode:**\n",
    "- Estimates counterfactual deployment value\n",
    "- Reweights logged data to match target policy\n",
    "- Good for: When ESS ≥ 10-50% and no fresh draws available\n",
    "\n",
    "**DR Mode:**\n",
    "- Also estimates counterfactual deployment value\n",
    "- Uses outcome model to reduce variance\n",
    "- Good for: When you have fresh draws (almost always best!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Choosing Your OPE Method\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "```\n",
    "Do you need counterfactual estimates?\n",
    "├─ No → Use Direct Mode\n",
    "│         (on-policy evaluation, simplest)\n",
    "│\n",
    "└─ Yes → Do you have logged data with logprobs?\n",
    "          ├─ No → Use Direct Mode\n",
    "          │        (can't do OPE without logprobs)\n",
    "          │\n",
    "          └─ Yes → Do you have fresh draws?\n",
    "                   ├─ No → Use IPS Mode\n",
    "                   │        (check ESS ≥ 10%!)\n",
    "                   │\n",
    "                   └─ Yes → Use DR Mode ✓\n",
    "                            (best accuracy + robustness)\n",
    "```\n",
    "\n",
    "### Data Requirements\n",
    "\n",
    "| Method | Logged Data | Logprobs | Fresh Draws | Oracle Labels |\n",
    "|--------|------------|----------|-------------|---------------|\n",
    "| **Direct** | ✗ | ✗ | ✓ | Optional (5-10%) |\n",
    "| **IPS** | ✓ | ✓ | ✗ | Recommended (5-10%) |\n",
    "| **DR** | ✓ | ✓ | ✓ | Recommended (5-10%) |\n",
    "\n",
    "### Key Diagnostics Checklist\n",
    "\n",
    "**IPS Mode:**\n",
    "- [ ] ESS ≥ 10% for all policies (prefer ≥ 50%)\n",
    "- [ ] Max weight not too large (< 100)\n",
    "- [ ] Calibration RMSE reasonable\n",
    "\n",
    "**DR Mode:**\n",
    "- [ ] Orthogonality CI contains 0\n",
    "- [ ] Outcome model R² ≥ 0.2 (prefer ≥ 0.5)\n",
    "- [ ] Standard errors lower than IPS\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Low ESS (< 10%):**\n",
    "- Problem: Poor overlap between logging and target policies\n",
    "- Solutions:\n",
    "  1. Switch to DR mode (most robust)\n",
    "  2. Regenerate data with better overlap\n",
    "  3. Use a closer baseline policy for logging\n",
    "\n",
    "**Non-orthogonal DR:**\n",
    "- Problem: Outcome model or weights misspecified\n",
    "- Solutions:\n",
    "  1. Add more fresh draws (improves outcome model)\n",
    "  2. Check calibration diagnostics\n",
    "  3. Try different DR variant (e.g., oc-dr-cpo)\n",
    "\n",
    "**High variance:**\n",
    "- Problem: Not enough data or poor overlap\n",
    "- Solutions:\n",
    "  1. Collect more samples\n",
    "  2. Use DR instead of IPS\n",
    "  3. Use weight calibration (SIMCal, enabled by default)\n",
    "\n",
    "### Pro Tips\n",
    "\n",
    "1. **Always use `estimator=\"auto\"`** - CJE picks the best method\n",
    "2. **Check diagnostics first** - Don't trust estimates with bad diagnostics\n",
    "3. **DR > IPS when possible** - More robust and accurate\n",
    "4. **5-10% oracle coverage is enough** - Don't need labels for all samples\n",
    "5. **Report confidence intervals** - CJE's CIs account for all uncertainty\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Documentation**: [GitHub README](https://github.com/cimo-labs/cje)\n",
    "- **API Reference**: See docstrings in `analyze_dataset()`\n",
    "- **Paper**: Coming soon with full technical details\n",
    "- **Questions?**: [Open an issue](https://github.com/cimo-labs/cje/issues)\n",
    "\n",
    "Happy evaluating! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}