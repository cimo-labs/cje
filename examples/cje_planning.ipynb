{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CJE Planning: Optimize Your Evaluation Budget\n\nBefore running a large evaluation, answer these questions:\n\n- **How many samples** do I need?\n- **How many oracle labels** are worth the cost?\n- **What effect size** can I reliably detect?\n\nCJE's planning tools help you find the optimal allocation between cheap surrogate scores and expensive oracle labels.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_planning.ipynb)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CJE\n",
    "!pip install -q --upgrade cje-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Getting Started: Create a Pilot Dataset\n\nTo plan your evaluation, you need a **pilot dataset** to estimate variance components.\n\n### Pilot Size Requirements\n\n| Pilot Size | Total Prompts | Oracle Labels | Oracle % |\n|------------|---------------|---------------|----------|\n| **Minimum** | 300 | 120 | 40% |\n| Recommended | 500 | 150-200 | 30-40% |\n\n**Key insight:** The pilot needs ~30-40% oracle coverage to reliably fit the variance model. With less, the measurements are too noisy.\n\n### How to Create Your Pilot\n\n1. **Sample 300-500 prompts** representative of your evaluation\n2. **Score all prompts** with your cheap judge (e.g., GPT-4o-mini)  \n3. **Randomly select 30-40%** and label them with your oracle\n\n**Important:** Oracle labels must be a *random* subset â€” don't cherry-pick.\n\n### Data Format\n\nSave as JSONL with one response per line:\n```json\n{\"prompt_id\": \"p1\", \"judge_score\": 0.82, \"oracle_label\": 0.75}\n{\"prompt_id\": \"p2\", \"judge_score\": 0.65, \"oracle_label\": null}\n{\"prompt_id\": \"p3\", \"judge_score\": 0.91, \"oracle_label\": 0.88}\n```\n\n### Can't Collect 300 Prompts?\n\nUse **rule-of-thumb allocations** instead of variance modeling:\n- **16Ã— cost ratio** (e.g., GPT-4o-mini â†’ GPT-4o): ~50% oracle\n- **100Ã— cost ratio** (e.g., cheap model â†’ human): ~20% oracle\n\nA small pilot (50-100 prompts) can validate judge-oracle correlation before scaling up."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load Your Pilot Data\n\n**Option A: Use your own pilot** (recommended)\n```python\nfrom cje.data.fresh_draws import load_fresh_draws_auto\npilot_data = load_fresh_draws_auto(\"path/to/your/pilot\", \"your_policy_name\")\n```\n\n**Option B: Try with Arena sample data** (for this tutorial)\n\nWe'll use real data from Chatbot Arena â€” 1000 samples with 480 oracle labels."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Arena sample data as reference\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"arena_sample\")\n",
    "if not (DATA_DIR / \"fresh_draws\" / \"base_responses.jsonl\").exists():\n",
    "    print(\"Downloading sample data...\")\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    (DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n",
    "    \n",
    "    BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
    "    for f in [\"base_responses.jsonl\", \"clone_responses.jsonl\", \n",
    "              \"parallel_universe_prompt_responses.jsonl\", \"unhelpful_responses.jsonl\"]:\n",
    "        urllib.request.urlretrieve(f\"{BASE_URL}/fresh_draws/{f}\", DATA_DIR / \"fresh_draws\" / f)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(f\"Data exists at {DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Fit Variance Model\n\nCJE measures how estimate variance changes with sample size. This takes ~30 seconds and only needs to be done once per evaluation setting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from cje import fit_variance_model\nfrom cje.data.fresh_draws import load_fresh_draws_auto\n\n# Load pilot data from the base policy (where calibration will be learned)\nbase_data = load_fresh_draws_auto(\"arena_sample/fresh_draws\", \"base\")\n\nprint(f\"Loaded base policy: {len(base_data.samples)} samples\")\nprint(f\"Oracle labels: {sum(1 for s in base_data.samples if s.oracle_label is not None)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fit the variance model (takes ~30 seconds)\nmodel = fit_variance_model(base_data, verbose=True)\n\n# RÂ² > 0.85 indicates the 1/n + 1/m model fits well\nif model.r_squared < 0.5:\n    print(\"\\nâš ï¸  Low RÂ² suggests the variance model may not fit your data well.\")\n    print(\"   Check that oracle labels are randomly sampled within your pilot.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Specify Your Costs\n",
    "\n",
    "**Critical:** Use actual dollar costs per API call. This ensures budget is in real dollars.\n",
    "\n",
    "| Surrogate (Judge) | Oracle | Cost Ratio |\n",
    "|-------------------|--------|------------|\n",
    "| GPT-4o-mini ($0.01) | GPT-4o ($0.16) | 16Ã— |\n",
    "| Claude Haiku ($0.008) | Claude Sonnet ($0.09) | 11Ã— |\n",
    "| Llama-70B ($0.002) | Human ($2.00) | 1000Ã— |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje import CostModel\n",
    "\n",
    "# Example: GPT-4o-mini as judge, GPT-4o as oracle\n",
    "cost_model = CostModel(\n",
    "    surrogate_cost=0.01,  # $/call for judge\n",
    "    oracle_cost=0.16      # $/call for oracle\n",
    ")\n",
    "\n",
    "print(f\"Surrogate cost: ${cost_model.surrogate_cost}/call\")\n",
    "print(f\"Oracle cost: ${cost_model.oracle_cost}/call\")\n",
    "print(f\"Cost ratio: {cost_model.oracle_cost/cost_model.surrogate_cost:.0f}Ã—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Budget-Constrained Planning\n",
    "\n",
    "**Question:** \"I have $X budget â€” what effect size can I reliably detect?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje import plan_evaluation\n",
    "\n",
    "# Plan with $5,000 budget\n",
    "plan = plan_evaluation(\n",
    "    budget=5000,\n",
    "    variance_model=model,\n",
    "    cost_model=cost_model\n",
    ")\n",
    "\n",
    "print(plan.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the results\n",
    "print(f\"ðŸ“Š What this means:\")\n",
    "print(f\"\")\n",
    "print(f\"   1. Collect {plan.n_samples:,} responses, each scored by your surrogate judge\")\n",
    "print(f\"   2. Randomly label {plan.m_oracle} of them with oracle ({plan.oracle_fraction:.1%} of samples)\")\n",
    "print(f\"   3. Run CJE to get calibrated estimates\")\n",
    "print(f\"\")\n",
    "print(f\"   With this setup, you can detect a {plan.mde:.1%} difference between policies\")\n",
    "print(f\"   with 80% statistical power (Î±=0.05).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MDE-Constrained Planning\n",
    "\n",
    "**Question:** \"I need to detect X% differences â€” what will it cost?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje import plan_for_mde\n",
    "\n",
    "# Plan to detect 2% differences\n",
    "plan_2pct = plan_for_mde(\n",
    "    target_mde=0.02,\n",
    "    variance_model=model,\n",
    "    cost_model=cost_model\n",
    ")\n",
    "\n",
    "print(f\"To detect 2% differences with 80% power:\")\n",
    "print(f\"  Budget needed: ${plan_2pct.total_cost:,.0f}\")\n",
    "print(f\"  Samples: {plan_2pct.n_samples:,} total, {plan_2pct.m_oracle} oracle labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different MDE targets\n",
    "print(f\"ðŸ“Š MDE vs Budget Tradeoff\")\n",
    "print(f\"=\"*40)\n",
    "print(f\"{'MDE':<10} {'Budget':<15} {'Samples':<10} {'Oracle'}\")\n",
    "print(f\"-\"*40)\n",
    "\n",
    "for target in [0.05, 0.03, 0.02, 0.015, 0.01]:\n",
    "    p = plan_for_mde(target_mde=target, variance_model=model, cost_model=cost_model)\n",
    "    print(f\"{target:.1%}       ${p.total_cost:>10,.0f}    {p.n_samples:>7,}   {p.m_oracle:>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Tradeoffs\n",
    "\n",
    "The planning dashboard shows three views:\n",
    "\n",
    "1. **MDE vs Budget** â€” How precision improves with spending\n",
    "2. **Power Curve** â€” Probability of detecting different effect sizes\n",
    "3. **Cost Sensitivity** â€” How oracle cost affects the tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje.visualization import plot_planning_dashboard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plot_planning_dashboard(model, cost_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Planning Loop\n",
    "\n",
    "Planning is iterative. Start with one constraint and check if the other is acceptable:\n",
    "\n",
    "```python\n",
    "# \"I have $10K but need 1.5% MDE\"\n",
    "plan = plan_evaluation(budget=10000, ...)  # â†’ MDE = 2.1% (too high!)\n",
    "plan = plan_for_mde(target_mde=0.015, ...) # â†’ $18K (too expensive!)\n",
    "plan = plan_for_mde(target_mde=0.018, ...) # â†’ $12K (compromise?)\n",
    "```\n",
    "\n",
    "**Rule of thumb:** Target MDE should be 2-3Ã— smaller than differences you actually care about. If you care about 5% differences, aim for 2% MDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive planning example\n",
    "print(\"Example planning session:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Start with budget constraint\n",
    "initial = plan_evaluation(budget=10000, variance_model=model, cost_model=cost_model)\n",
    "print(f\"\\n1. With $10K budget: MDE = {initial.mde:.1%}\")\n",
    "\n",
    "# Check what it costs to hit a tighter MDE\n",
    "target = plan_for_mde(target_mde=0.015, variance_model=model, cost_model=cost_model)\n",
    "print(f\"2. To hit 1.5% MDE: need ${target.total_cost:,.0f}\")\n",
    "\n",
    "# Find a compromise\n",
    "compromise = plan_for_mde(target_mde=0.02, variance_model=model, cost_model=cost_model)\n",
    "print(f\"3. Compromise at 2% MDE: ${compromise.total_cost:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### Quick Reference\n\n```python\nfrom cje import fit_variance_model, CostModel, plan_evaluation, plan_for_mde\nfrom cje.data.fresh_draws import load_fresh_draws_auto\n\n# 1. Fit variance model from pilot data\nbase_data = load_fresh_draws_auto(\"pilot_dir\", \"base\")\nmodel = fit_variance_model(base_data)\n\n# 2. Specify costs (use actual $/call)\ncost_model = CostModel(surrogate_cost=0.01, oracle_cost=0.16)\n\n# 3a. Budget â†’ MDE\nplan = plan_evaluation(budget=5000, variance_model=model, cost_model=cost_model)\n\n# 3b. MDE â†’ Budget  \nplan = plan_for_mde(target_mde=0.02, variance_model=model, cost_model=cost_model)\n```\n\n### Next Steps\n\nOnce you have your plan, run the evaluation:\n\n[![Core Tutorial](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_core_demo.ipynb)\n\n---\n\n## Appendix: How Planning Works (Optional)\n\n<details>\n<summary>Click to expand the math</summary>\n\nCJE's variance decomposes into two independent components:\n\n$$\\text{Var}(\\hat{\\theta}) = \\frac{\\sigma^2_{\\text{eval}}}{n} + \\frac{\\sigma^2_{\\text{cal}}}{m}$$\n\nWhere:\n- **n** = total samples (evaluated by cheap judge)\n- **m** = oracle-labeled samples (subset of n)\n- **ÏƒÂ²_eval** = evaluation variance (inherent noise in policy performance)\n- **ÏƒÂ²_cal** = calibration variance (uncertainty in learning judgeâ†’oracle mapping)\n\nGiven costs c_S (surrogate) and c_Y (oracle), the optimal allocation follows the **Square Root Law**:\n\n$$\\frac{m^*}{n^*} = \\sqrt{\\frac{c_S}{c_Y}} \\cdot \\sqrt{\\frac{\\sigma^2_{\\text{cal}}}{\\sigma^2_{\\text{eval}}}}$$\n\nThis balances the marginal variance reduction per dollar spent on each component.\n\n</details>\n\n**Documentation:**\n- [CJE GitHub](https://github.com/cimo-labs/cje)\n- [CJE Paper (arXiv)](https://arxiv.org/abs/2512.11150) â€” See Appendix F for full derivation"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}