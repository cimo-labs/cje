{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# CJE (Causal Judge Evaluation) Demo\n",
    "\n",
    "**Interactive demo using Arena 10K sample data**\n",
    "\n",
    "This notebook demonstrates how to use CJE to evaluate LLM policies using judge scores and oracle labels. We'll walk through:\n",
    "\n",
    "1. **Setup**: Install CJE and download sample data\n",
    "2. **Understanding Modes**: IPS, DR, and Direct evaluation\n",
    "3. **Policy Comparison**: Find the best policy with confidence intervals\n",
    "4. **Diagnostics**: Check reliability with ESS and other metrics\n",
    "\n",
    "---\n",
    "\n",
    "## What is CJE?\n",
    "\n",
    "CJE turns LLM-as-judge scores into causally interpretable estimates. Instead of naively averaging judge scores, CJE:\n",
    "- **Calibrates** judge scores to match oracle labels (AutoCal-R)\n",
    "- **Stabilizes** importance weights for off-policy evaluation (SIMCal)\n",
    "- **Reports** confidence intervals that account for all sources of uncertainty\n",
    "\n",
    "**Key insight**: Judge scores are correlational. CJE makes them causal.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_arena_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": "## Step 1: Setup\n\nInstall CJE and download the Arena sample data.\n\n**Note:** Colab comes with numpy 2.3+ pre-installed, which breaks scipy. We force-reinstall numpy 2.0.x first."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": "# Colab comes with numpy 2.3+ which breaks scipy\n# Force install compatible numpy first\n!pip install -q 'numpy>=2.0,<2.1' --force-reinstall\n\n# Install CJE from PyPI\n!pip install -q cje-eval\n\n# Verify installation\nimport cje\nprint(f\"✓ CJE version {cje.__version__} installed\")\n\n# Check numpy version\nimport numpy as np\nprint(f\"✓ NumPy version {np.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_data"
   },
   "outputs": [],
   "source": "# Download Arena sample data from GitHub\nimport os\nimport urllib.request\nfrom pathlib import Path\n\n# Create data directory\nDATA_DIR = Path(\"arena_sample\")\nDATA_DIR.mkdir(exist_ok=True)\n(DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n\n# Base URL for raw files on GitHub\nBASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n\n# Download logged data\nprint(\"Downloading logged_data.jsonl...\")\nurllib.request.urlretrieve(\n    f\"{BASE_URL}/logged_data.jsonl\",\n    DATA_DIR / \"logged_data.jsonl\"\n)\nprint(\"✓ Downloaded logged_data.jsonl\")\n\n# Download fresh draws (note: actual filenames have _responses.jsonl suffix)\nfresh_draw_files = {\n    \"clone\": \"clone_responses.jsonl\",\n    \"parallel_universe_prompt\": \"parallel_universe_prompt_responses.jsonl\",\n    \"unhelpful\": \"unhelpful_responses.jsonl\"\n}\n\nfor policy, filename in fresh_draw_files.items():\n    print(f\"Downloading fresh_draws/{filename}...\")\n    urllib.request.urlretrieve(\n        f\"{BASE_URL}/fresh_draws/{filename}\",\n        DATA_DIR / \"fresh_draws\" / filename\n    )\n    print(f\"✓ Downloaded {filename}\")\n\nprint(\"\\n✓ All data downloaded successfully!\")\nprint(f\"\\nData location: {DATA_DIR.absolute()}\")\nprint(f\"Policies available: {list(fresh_draw_files.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inspect_data"
   },
   "source": [
    "## Step 2: Inspect the Data\n",
    "\n",
    "Let's look at what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load and inspect logged data\n",
    "with open(DATA_DIR / \"logged_data.jsonl\") as f:\n",
    "    logged_samples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Logged data: {len(logged_samples)} samples\")\n",
    "print(f\"\\nTarget policies: {list(logged_samples[0]['target_policy_logprobs'].keys())}\")\n",
    "print(f\"\\nExample sample:\")\n",
    "sample = logged_samples[0]\n",
    "print(f\"  Prompt: {sample['prompt'][:100]}...\")\n",
    "print(f\"  Response: {sample['response'][:150]}...\")\n",
    "print(f\"  Judge score: {sample['judge_score']}\")\n",
    "print(f\"  Oracle label: {sample['oracle_label']}\")\n",
    "print(f\"  Has logprobs: ✓\")\n",
    "\n",
    "# Check oracle coverage\n",
    "n_with_oracle = sum(1 for s in logged_samples if s.get('oracle_label') is not None)\n",
    "coverage = n_with_oracle / len(logged_samples)\n",
    "print(f\"\\nOracle label coverage: {n_with_oracle}/{len(logged_samples)} ({coverage:.1%})\")\n",
    "print(f\"→ {coverage:.1%} coverage enables {'IPS/DR modes' if coverage >= 0.5 else 'calibration'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mode_1"
   },
   "source": [
    "## Step 3: Mode 1 - IPS (Importance Sampling)\n",
    "\n",
    "**Use case**: Reuse logged data to estimate counterfactual performance\n",
    "\n",
    "**Estimand**: \"What would the KPI be if we deployed policy π' instead of π₀?\"\n",
    "\n",
    "**How it works**:\n",
    "1. Calibrate judge scores → oracle-scale rewards (AutoCal-R)\n",
    "2. Compute importance weights: W = π'(a|x) / π₀(a|x)\n",
    "3. Stabilize weights with monotone projection (SIMCal)\n",
    "4. Estimate: V̂(π') = (1/n) Σ W_i · R_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ips"
   },
   "outputs": [],
   "source": [
    "from cje import analyze_dataset\n",
    "\n",
    "# IPS mode: Logged data only (auto-selects calibrated-ips estimator)\n",
    "results_ips = analyze_dataset(\n",
    "    logged_data_path=str(DATA_DIR / \"logged_data.jsonl\"),\n",
    "    estimator=\"auto\",  # Auto-detects IPS mode\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IPS Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {results_ips.metadata['mode']}\")\n",
    "print(f\"Estimator: {results_ips.metadata['estimator']}\")\n",
    "print(f\"Logprob coverage: {results_ips.metadata['mode_selection']['logprob_coverage']:.1%}\")\n",
    "print()\n",
    "\n",
    "# Show estimates with confidence intervals\n",
    "policies = results_ips.metadata['target_policies']\n",
    "print(f\"{'Policy':<30} {'Estimate':<12} {'Std Error':<12} {'95% CI':<20}\")\n",
    "print(\"-\" * 74)\n",
    "for i, policy in enumerate(policies):\n",
    "    est = results_ips.estimates[i]\n",
    "    se = results_ips.standard_errors[i]\n",
    "    ci_low = est - 1.96 * se\n",
    "    ci_high = est + 1.96 * se\n",
    "    print(f\"{policy:<30} {est:>6.3f}       {se:>6.3f}       [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diagnostics"
   },
   "source": [
    "### Check IPS Diagnostics\n",
    "\n",
    "**ESS (Effective Sample Size)** is the key diagnostic for IPS reliability:\n",
    "- ESS ≥ 50%: Excellent overlap\n",
    "- ESS ∈ [10%, 50%): Moderate (DR recommended)\n",
    "- ESS < 10%: Poor overlap (switch to DR or regenerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ips_diagnostics"
   },
   "outputs": [],
   "source": [
    "# Check diagnostics for each policy\n",
    "print(\"IPS Diagnostics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for policy in policies:\n",
    "    diag = results_ips.diagnostics.get(policy, {})\n",
    "    ess = diag.get('ess_fraction', 0.0)\n",
    "    \n",
    "    # Traffic light assessment\n",
    "    if ess >= 0.5:\n",
    "        status = \"✓ EXCELLENT\"\n",
    "    elif ess >= 0.1:\n",
    "        status = \"⚠ MODERATE (consider DR)\"\n",
    "    else:\n",
    "        status = \"✗ POOR (use DR or regenerate)\"\n",
    "    \n",
    "    print(f\"\\n{policy}:\")\n",
    "    print(f\"  ESS: {ess:.1%} {status}\")\n",
    "    \n",
    "    # Show weight statistics\n",
    "    if 'weights' in diag:\n",
    "        weights_info = diag['weights']\n",
    "        print(f\"  Weight stats (after SIMCal):\")\n",
    "        print(f\"    Min:    {weights_info.get('min', 0):.3f}\")\n",
    "        print(f\"    Median: {weights_info.get('median', 0):.3f}\")\n",
    "        print(f\"    Max:    {weights_info.get('max', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mode_2"
   },
   "source": [
    "## Step 4: Mode 2 - DR (Doubly Robust)\n",
    "\n",
    "**Use case**: Most accurate counterfactual estimates when you have fresh draws\n",
    "\n",
    "**Estimand**: Same as IPS, but with better accuracy\n",
    "\n",
    "**How it works**:\n",
    "1. Everything from IPS mode\n",
    "2. Train outcome model ĝ(S) on fresh draws\n",
    "3. Combine: V̂_DR(π') = (1/n) Σ [W_i · (R_i - ĝ(S_i)) + ĝ(S_i)]\n",
    "\n",
    "**Double robustness**: Consistent if *either* weights or outcome model is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr"
   },
   "outputs": [],
   "source": [
    "# DR mode: Logged data + fresh draws (auto-selects stacked-dr estimator)\n",
    "results_dr = analyze_dataset(\n",
    "    logged_data_path=str(DATA_DIR / \"logged_data.jsonl\"),\n",
    "    fresh_draws_dir=str(DATA_DIR / \"fresh_draws\"),\n",
    "    estimator=\"auto\",  # Auto-detects DR mode\n",
    "    estimator_config={\"parallel\": False},  # Disable parallel for Colab\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DR Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {results_dr.metadata['mode']}\")\n",
    "print(f\"Estimator: {results_dr.metadata['estimator']}\")\n",
    "print()\n",
    "\n",
    "# Compare IPS vs DR\n",
    "print(f\"{'Policy':<30} {'IPS Est':<12} {'DR Est':<12} {'Improvement':<15}\")\n",
    "print(\"-\" * 69)\n",
    "for i, policy in enumerate(policies):\n",
    "    ips_est = results_ips.estimates[i]\n",
    "    dr_est = results_dr.estimates[i]\n",
    "    \n",
    "    ips_se = results_ips.standard_errors[i]\n",
    "    dr_se = results_dr.standard_errors[i]\n",
    "    \n",
    "    # Check if DR improved standard error\n",
    "    if dr_se < ips_se:\n",
    "        improvement = f\"↓ {(1 - dr_se/ips_se)*100:.0f}% SE\"\n",
    "    else:\n",
    "        improvement = \"(similar)\"\n",
    "    \n",
    "    print(f\"{policy:<30} {ips_est:>6.3f}       {dr_est:>6.3f}       {improvement:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr_diagnostics"
   },
   "source": [
    "### Check DR Orthogonality\n",
    "\n",
    "**Orthogonality score**: E[W · (R - ĝ)] should be ≈ 0\n",
    "- If CI contains 0: ✓ Good\n",
    "- If CI excludes 0: ⚠ Weights or outcome model may be poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr_ortho"
   },
   "outputs": [],
   "source": [
    "print(\"DR Orthogonality Diagnostics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for policy in policies:\n",
    "    diag = results_dr.diagnostics.get(policy, {})\n",
    "    \n",
    "    # Check orthogonality\n",
    "    if 'orthogonality' in diag:\n",
    "        ortho = diag['orthogonality']\n",
    "        score = ortho.get('score', 0)\n",
    "        ci_low = ortho.get('ci_lower', 0)\n",
    "        ci_high = ortho.get('ci_upper', 0)\n",
    "        \n",
    "        # Check if CI contains zero\n",
    "        contains_zero = ci_low <= 0 <= ci_high\n",
    "        status = \"✓ PASS\" if contains_zero else \"⚠ CHECK\"\n",
    "        \n",
    "        print(f\"\\n{policy}:\")\n",
    "        print(f\"  Orthogonality: {score:.4f} [{ci_low:.4f}, {ci_high:.4f}] {status}\")\n",
    "        \n",
    "        if not contains_zero:\n",
    "            print(f\"  → CI does not contain 0. Consider:\")\n",
    "            print(f\"    • Improving outcome model\")\n",
    "            print(f\"    • Adding more fresh draws\")\n",
    "            print(f\"    • Revisiting SIMCal settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mode_3"
   },
   "source": [
    "## Step 5: Mode 3 - Direct (On-Policy Evaluation)\n",
    "\n",
    "**Use case**: Compare policies on a specific evaluation set (non-counterfactual)\n",
    "\n",
    "**Estimand**: \"Which policy performs best on *this* prompt set?\"\n",
    "\n",
    "**How it works**:\n",
    "1. Generate fresh responses from each policy on same prompts\n",
    "2. Calibrate judge scores using oracle labels in fresh draws\n",
    "3. Average: V̂(π) = (1/m) Σ R_πi\n",
    "\n",
    "**Key difference**: No importance weighting (on-policy), not counterfactual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "direct"
   },
   "outputs": [],
   "source": [
    "# Direct mode: Fresh draws only (learns calibration from oracle labels in fresh draws)\n",
    "results_direct = analyze_dataset(\n",
    "    fresh_draws_dir=str(DATA_DIR / \"fresh_draws\"),  # No logged data!\n",
    "    estimator=\"auto\",  # Auto-detects Direct mode\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Direct Mode Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {results_direct.metadata['mode']}\")\n",
    "print(f\"Estimator: {results_direct.metadata['estimator']}\")\n",
    "print(f\"Calibration: {results_direct.metadata.get('calibration', 'none')}\")\n",
    "print(f\"Oracle coverage: {results_direct.metadata.get('oracle_coverage', 0):.1%}\")\n",
    "print()\n",
    "\n",
    "# Show all three modes side-by-side\n",
    "print(f\"{'Policy':<30} {'IPS':<10} {'DR':<10} {'Direct':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, policy in enumerate(policies):\n",
    "    print(f\"{policy:<30} {results_ips.estimates[i]:>6.3f}     {results_dr.estimates[i]:>6.3f}     {results_direct.estimates[i]:>6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## Step 6: Policy Selection and Comparison\n",
    "\n",
    "Find the best policy and compare against a baseline using proper statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "select_policy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use DR results (most accurate)\n",
    "estimates = np.array(results_dr.estimates)\n",
    "std_errors = np.array(results_dr.standard_errors)\n",
    "\n",
    "# Find best policy\n",
    "best_idx = np.argmax(estimates)\n",
    "best_policy = policies[best_idx]\n",
    "best_est = estimates[best_idx]\n",
    "best_se = std_errors[best_idx]\n",
    "\n",
    "print(\"Policy Selection (DR Mode)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n🏆 Best policy: {best_policy}\")\n",
    "print(f\"   Estimate: {best_est:.3f} ± {best_se:.3f}\")\n",
    "print(f\"   95% CI: [{best_est - 1.96*best_se:.3f}, {best_est + 1.96*best_se:.3f}]\")\n",
    "\n",
    "# Compare against baseline (clone)\n",
    "baseline_idx = policies.index('clone')\n",
    "baseline_est = estimates[baseline_idx]\n",
    "baseline_se = std_errors[baseline_idx]\n",
    "\n",
    "print(f\"\\n📊 Comparison to baseline ({policies[baseline_idx]}):\")\n",
    "print(f\"   Baseline: {baseline_est:.3f} ± {baseline_se:.3f}\")\n",
    "print()\n",
    "print(f\"{'Policy':<30} {'Delta':<12} {'Std Error':<12} {'Significant?':<15}\")\n",
    "print(\"-\" * 69)\n",
    "\n",
    "for i, policy in enumerate(policies):\n",
    "    if i == baseline_idx:\n",
    "        print(f\"{policy:<30} {'(baseline)':<12} {'':<12} {'':<15}\")\n",
    "        continue\n",
    "    \n",
    "    delta = estimates[i] - baseline_est\n",
    "    # Standard error of difference (assuming independence)\n",
    "    delta_se = np.sqrt(std_errors[i]**2 + baseline_se**2)\n",
    "    \n",
    "    # Z-test\n",
    "    z_score = delta / delta_se\n",
    "    significant = abs(z_score) > 1.96\n",
    "    \n",
    "    sig_text = \"✓ Yes (p<0.05)\" if significant else \"No\"\n",
    "    delta_text = f\"{delta:+.3f}\"\n",
    "    \n",
    "    print(f\"{policy:<30} {delta_text:<12} {delta_se:>6.3f}       {sig_text:<15}\")\n",
    "\n",
    "print(\"\\nNote: Assuming independence between policies (conservative).\")\n",
    "print(\"For paired comparisons, use CJE's built-in contrast computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary: When to Use Each Mode\n",
    "\n",
    "| Mode | Data Required | Estimand | Use When |\n",
    "|------|--------------|----------|----------|\n",
    "| **IPS** | Logged data + logprobs | Counterfactual deployment value | Have logged data, want off-policy estimates |\n",
    "| **DR** | Logged data + fresh draws | Counterfactual (most accurate) | Have both logged and fresh data |\n",
    "| **Direct** | Fresh draws (+ optional calibration data) | Performance on eval set | Just want on-policy comparison |\n",
    "\n",
    "### Key Diagnostics to Check\n",
    "\n",
    "- **IPS mode**: ESS ≥ 10% (prefer ≥ 50%)\n",
    "- **DR mode**: Orthogonality CI contains 0\n",
    "- **All modes**: Oracle coverage ≥ 85% for reliable calibration\n",
    "\n",
    "### Pro Tips\n",
    "\n",
    "1. **Use `estimator=\"auto\"`**: CJE selects the best mode and estimator automatically\n",
    "2. **Check diagnostics first**: Don't trust estimates if ESS < 10% or orthogonality fails\n",
    "3. **Report confidence intervals**: CJE's CIs account for all uncertainty sources (oracle, calibration, sampling)\n",
    "4. **DR > IPS when possible**: Doubly robust is more accurate and robust\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Documentation**: [CJE README](https://github.com/cimo-labs/cje)\n",
    "- **Paper**: Coming soon with full technical details\n",
    "- **Install locally**: `pip install cje-eval`\n",
    "- **More examples**: See [examples/](https://github.com/cimo-labs/cje/tree/main/examples)\n",
    "\n",
    "Questions? Open an issue on [GitHub](https://github.com/cimo-labs/cje/issues)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}