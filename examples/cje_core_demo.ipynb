{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CJE: Calibrate Your LLM Judge\n",
    "\n",
    "Your LLM judge scores are lying. CJE calibrates them to what actually matters.\n",
    "\n",
    "**The problem**: Cheap judge scores (S) don't match expensive oracle outcomes (Y). CJE learns the S→Y mapping so you can trust your metrics.\n",
    "\n",
    "[Read the full explanation →](https://cimolabs.com/blog/metrics-lying)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_core_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CJE (force upgrade to get latest features)\n",
    "!pip install -q --upgrade cje-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data (1000 Chatbot Arena prompts, 4 policies)\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"arena_sample\")\n",
    "if not (DATA_DIR / \"fresh_draws\" / \"base_responses.jsonl\").exists():\n",
    "    print(\"Downloading sample data...\")\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    (DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n",
    "    (DATA_DIR / \"probe_slice\").mkdir(exist_ok=True)\n",
    "    \n",
    "    BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
    "    for f in [\"base_responses.jsonl\", \"clone_responses.jsonl\", \n",
    "              \"parallel_universe_prompt_responses.jsonl\", \"unhelpful_responses.jsonl\"]:\n",
    "        urllib.request.urlretrieve(f\"{BASE_URL}/fresh_draws/{f}\", DATA_DIR / \"fresh_draws\" / f)\n",
    "    for f in [\"clone_probe.jsonl\", \"parallel_universe_prompt_probe.jsonl\", \"unhelpful_probe.jsonl\"]:\n",
    "        urllib.request.urlretrieve(f\"{BASE_URL}/probe_slice/{f}\", DATA_DIR / \"probe_slice\" / f)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(f\"Data exists at {DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Plan Your Sample Sizes\n",
    "\n",
    "Need to optimize your evaluation budget before collecting data? See the dedicated planning notebook:\n",
    "\n",
    "[![Planning Notebook](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_planning.ipynb)\n",
    "\n",
    "**Already have data?** Continue below to run your evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare Policies\n",
    "\n",
    "One line to analyze all your policies with calibrated estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje import analyze_dataset\n",
    "\n",
    "results = analyze_dataset(fresh_draws_dir=\"arena_sample/fresh_draws/\", verbose=False)\n",
    "\n",
    "# Show summary\n",
    "policies = results.metadata['target_policies']\n",
    "print(f\"Analyzed {len(policies)} policies:\\n\")\n",
    "for i, p in enumerate(policies):\n",
    "    est = results.estimates[i]\n",
    "    se = results.standard_errors[i]\n",
    "    print(f\"  {p}: {est:.3f} ± {1.96*se:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with confidence intervals\n",
    "results.plot_estimates(\n",
    "    policy_labels={\n",
    "        \"base\": \"Standard prompt\",\n",
    "        \"clone\": \"Same prompt (different seed)\",\n",
    "        \"parallel_universe_prompt\": \"Modified system prompt\",\n",
    "        \"unhelpful\": \"Adversarial prompt\",\n",
    "    }\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "policies = results.metadata['target_policies']\n",
    "best = policies[results.best_policy()]\n",
    "print(f\"Best: {best}\\n\")\n",
    "\n",
    "# Compare all to base\n",
    "base_idx = policies.index('base')\n",
    "for i, p in enumerate(policies):\n",
    "    if i != base_idx:\n",
    "        comp = results.compare_policies(i, base_idx)\n",
    "        sig = \"*\" if comp['significant'] else \"\"\n",
    "        print(f\"{p}: {comp['difference']:+.3f} (p={comp['p_value']:.3f}) {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check If Calibration Transfers\n",
    "\n",
    "Calibration is learned on one distribution. Does it still work on new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from cje.diagnostics import audit_transportability, plot_transport_comparison\n",
    "\n",
    "# Test on probe slices (held-out data with oracle labels)\n",
    "probe_files = {\n",
    "    \"clone\": \"arena_sample/probe_slice/clone_probe.jsonl\",\n",
    "    \"modified_prompt\": \"arena_sample/probe_slice/parallel_universe_prompt_probe.jsonl\",\n",
    "    \"adversarial\": \"arena_sample/probe_slice/unhelpful_probe.jsonl\",\n",
    "}\n",
    "\n",
    "audits = {}\n",
    "for name, path in probe_files.items():\n",
    "    data = [json.loads(line) for line in open(path)]\n",
    "    audits[name] = audit_transportability(results.calibrator, data)\n",
    "    print(audits[name].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: which variants break calibration?\n",
    "plot_transport_comparison(audits, title=\"Does Calibration Transfer?\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed view of failing variant - residuals by score decile\n",
    "audits['adversarial'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect What's Fooling the Judge\n",
    "\n",
    "When calibration fails, look at the actual samples. What patterns fool the judge but not the oracle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje.diagnostics import compute_residuals\n",
    "\n",
    "# Compute residuals for each sample (sorted by worst overestimate first)\n",
    "adversarial_data = [json.loads(line) for line in open(\"arena_sample/probe_slice/unhelpful_probe.jsonl\")]\n",
    "samples = compute_residuals(results.calibrator, adversarial_data)\n",
    "\n",
    "print(f\"Samples: {len(samples)}\")\n",
    "print(f\"Mean residual: {sum(s['residual'] for s in samples) / len(samples):.3f}\")\n",
    "print(f\"\\nResidual = Oracle - Calibrated\")\n",
    "print(f\"  Negative = judge overestimated (fooled)\")\n",
    "print(f\"  Positive = judge underestimated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the worst overestimates - where the judge was most fooled\n",
    "print(\"WORST OVERESTIMATES: Judge gave high scores, oracle gave low scores\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, s in enumerate(samples[:3]):\n",
    "    print(f\"\\nSample {i+1} | Residual: {s['residual']:.2f}\")\n",
    "    print(f\"  Judge: {s['judge_score']:.2f} → Calibrated: {s['calibrated']:.2f} | Oracle: {s['oracle_label']:.2f}\")\n",
    "    print(f\"  Prompt: {s['prompt']}\")\n",
    "    print(f\"  Response: {s['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we see**: The adversarial prompt produces responses that *sound* helpful (confident, structured, detailed) but are actually wrong or misleading. The cheap judge is fooled by surface features; the oracle catches the substance.\n",
    "\n",
    "**The fix**: This is exactly why you need calibration. Raw judge scores would rank the adversarial prompt too high. CJE's transportability check flags this before you ship bad decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor Calibration Over Time\n",
    "\n",
    "Calibration drifts. Periodically check it with fresh oracle labels. Here we simulate gradual drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate weekly monitoring with gradual drift\n",
    "import random\n",
    "\n",
    "# Load base data with oracle labels for monitoring simulation\n",
    "base_data = [json.loads(l) for l in open(\"arena_sample/fresh_draws/base_responses.jsonl\")]\n",
    "base_oracle = [r for r in base_data if r.get(\"oracle_label\") is not None]\n",
    "unhelpful_data = [json.loads(l) for l in open(\"arena_sample/probe_slice/unhelpful_probe.jsonl\")]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(base_oracle)\n",
    "\n",
    "# Week 1: stable (pure base data)\n",
    "week1 = base_oracle[:48]\n",
    "\n",
    "# Week 2: starting to drift (65% base, 35% adversarial)\n",
    "week2_base = base_oracle[48:79]\n",
    "week2_adv = unhelpful_data[:17]\n",
    "week2 = week2_base + week2_adv\n",
    "random.shuffle(week2)\n",
    "\n",
    "# Week 3: drifted (40% base, 60% adversarial) \n",
    "week3_base = base_oracle[79:99]\n",
    "week3_adv = unhelpful_data[17:47]\n",
    "week3 = week3_base + week3_adv\n",
    "random.shuffle(week3)\n",
    "\n",
    "weekly_audits = {\n",
    "    \"Week 1\": audit_transportability(results.calibrator, week1),\n",
    "    \"Week 2\": audit_transportability(results.calibrator, week2),\n",
    "    \"Week 3\": audit_transportability(results.calibrator, week3),\n",
    "}\n",
    "\n",
    "for name, audit in weekly_audits.items():\n",
    "    print(audit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transport_comparison(weekly_audits, title=\"Weekly Calibration Check\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "```python\n",
    "# Compare policies\n",
    "results = analyze_dataset(fresh_draws_dir=\"data/responses/\")\n",
    "results.plot_estimates()\n",
    "\n",
    "# Check calibration transfers\n",
    "audit = audit_transportability(results.calibrator, new_data)\n",
    "print(audit.summary())  # PASS or FAIL\n",
    "\n",
    "# Monitor over time\n",
    "plot_transport_comparison({\"Week 1\": audit1, \"Week 2\": audit2, ...})\n",
    "```\n",
    "\n",
    "**PASS** = calibration valid, trust the estimates  \n",
    "**FAIL** = something changed, investigate or recalibrate\n",
    "\n",
    "### Learn More\n",
    "- [Planning notebook](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_planning.ipynb) — Optimize sample sizes and oracle fractions\n",
    "- [Why your metrics lie](https://cimolabs.com/blog/metrics-lying) — The full explanation\n",
    "- [Arena experiment](https://www.cimolabs.com/research/arena-experiment) — Benchmarks on 5,000 prompts\n",
    "- [GitHub](https://github.com/cimo-labs/cje) — Documentation and source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
