{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CJE: Calibrate Your LLM Judge\n",
    "\n",
    "Your LLM judge scores are lying. CJE calibrates them to what actually matters.\n",
    "\n",
    "**The problem**: Cheap judge scores (S) don't match expensive oracle outcomes (Y). CJE learns the Sâ†’Y mapping so you can trust your metrics.\n",
    "\n",
    "[Read the full explanation â†’](https://cimolabs.com/blog/metrics-lying)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_core_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install CJE (force upgrade to get latest features)\n!pip install -q --upgrade cje-eval"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data (1000 Chatbot Arena prompts, 4 prompt variants)\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"arena_sample\")\n",
    "if not (DATA_DIR / \"fresh_draws\" / \"base_responses.jsonl\").exists():\n",
    "    print(\"Downloading sample data...\")\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    (DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n",
    "    (DATA_DIR / \"probe_slice\").mkdir(exist_ok=True)\n",
    "    \n",
    "    BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
    "    for f in [\"base_responses.jsonl\", \"clone_responses.jsonl\", \n",
    "              \"parallel_universe_prompt_responses.jsonl\", \"unhelpful_responses.jsonl\"]:\n",
    "        urllib.request.urlretrieve(f\"{BASE_URL}/fresh_draws/{f}\", DATA_DIR / \"fresh_draws\" / f)\n",
    "    for f in [\"clone_probe.jsonl\", \"parallel_universe_prompt_probe.jsonl\", \"unhelpful_probe.jsonl\"]:\n",
    "        urllib.request.urlretrieve(f\"{BASE_URL}/probe_slice/{f}\", DATA_DIR / \"probe_slice\" / f)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(f\"Data exists at {DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 0: Plan Your Evaluation\n\nBefore collecting expensive data, figure out:\n- **How many samples** do I need?\n- **How many oracle labels** are worth the cost?\n- **What effect size** can I reliably detect?\n\nThis prevents the #1 mistake: collecting underpowered data that can't answer your question.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from cje import fit_variance_model, CostModel, plan_evaluation, plan_for_mde\nfrom cje.data.fresh_draws import load_fresh_draws_auto, discover_policies_from_fresh_draws\n\n# Use the arena sample as our \"pilot\" to learn variance structure\npolicies = discover_policies_from_fresh_draws(\"arena_sample/fresh_draws\")\nfresh_draws_dict = {p: load_fresh_draws_auto(\"arena_sample/fresh_draws\", p) for p in policies}\n\n# Fit the variance model (this takes ~30 seconds)\nprint(\"Fitting variance model from pilot data...\")\nmodel = fit_variance_model(fresh_draws_dict, verbose=False)\nprint(f\"\\nâœ“ Variance model fitted (RÂ² = {model.r_squared:.2f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Specify your cost model - THIS IS CRITICAL\n# Use actual costs per call so budget is in real dollars\n# Example: GPT-4o-mini surrogate ($0.01/call) vs GPT-4o oracle ($0.16/call)\n\ncost_model = CostModel(surrogate_cost=0.01, oracle_cost=0.16)\n\nprint(f\"Costs: surrogate=${cost_model.surrogate_cost}/call, oracle=${cost_model.oracle_cost}/call\")\nprint(f\"Budget will be in real dollars (e.g., $5,000 = $5,000)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# \"I have $5,000 - what effect size can I detect?\"\nplan = plan_evaluation(budget=5000, variance_model=model, cost_model=cost_model)\n\nprint(plan.summary())\nprint(f\"\\nðŸ“Š What this means:\")\nprint(f\"   Collect {plan.n_samples:,} responses scored by surrogate judge\")\nprint(f\"   Randomly label {plan.m_oracle} with oracle ({plan.m_oracle/plan.n_samples:.1%} of samples)\")\nprint(f\"   Can detect {plan.mde:.1%} difference between policies (80% power)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# \"I need to detect 2% differences - what's the cost?\"\nplan_2pct = plan_for_mde(target_mde=0.02, variance_model=model, cost_model=cost_model)\n\nprint(f\"To detect 2% difference with 80% power:\")\nprint(f\"  Budget needed: ${plan_2pct.total_cost:,.0f}\")\nprint(f\"  Samples: {plan_2pct.n_samples:,} prompts, {plan_2pct.m_oracle} oracle labels\")\n\n# Compare different MDE targets\nprint(f\"\\nðŸ“Š MDE vs Budget tradeoff:\")\nfor target in [0.05, 0.03, 0.02, 0.01]:\n    p = plan_for_mde(target_mde=target, variance_model=model, cost_model=cost_model)\n    print(f\"   {target:.0%} MDE â†’ ${p.total_cost:,.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize: MDE vs Budget tradeoff\nfrom cje.visualization import plot_planning_dashboard\nimport matplotlib.pyplot as plt\n\nfig = plot_planning_dashboard(model, cost_model)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**The planning loop:**\n\n1. Start with budget constraint OR target MDE\n2. Check if the other is acceptable\n3. Iterate until both work\n\n```python\n# \"I have $10K but need 1.5% MDE\"\nplan = plan_evaluation(budget=10000, ...)  # â†’ MDE = 2.1% (too high!)\nplan = plan_for_mde(target_mde=0.015, ...) # â†’ $18K (too expensive!)\nplan = plan_for_mde(target_mde=0.018, ...) # â†’ $12K (compromise)\n```\n\n**Rule of thumb:** Target MDE should be 2-3Ã— smaller than differences you care about.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare Prompt Variants\n",
    "\n",
    "One line to analyze all your prompt variants with calibrated estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from cje import analyze_dataset\n\nresults = analyze_dataset(fresh_draws_dir=\"arena_sample/fresh_draws/\", verbose=False)\n\n# Show summary\npolicies = results.metadata['target_policies']\nprint(f\"Analyzed {len(policies)} policies:\\n\")\nfor i, p in enumerate(policies):\n    est = results.estimates[i]\n    se = results.standard_errors[i]\n    print(f\"  {p}: {est:.3f} Â± {1.96*se:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize with confidence intervals\nresults.plot_estimates(\n    policy_labels={\n        \"base\": \"Standard prompt\",\n        \"clone\": \"Same prompt (different seed)\",\n        \"parallel_universe_prompt\": \"Modified system prompt\",\n        \"unhelpful\": \"Adversarial prompt\",\n    }\n);"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "policies = results.metadata['target_policies']\n",
    "best = policies[results.best_policy()]\n",
    "print(f\"Best: {best}\\n\")\n",
    "\n",
    "# Compare all to base\n",
    "base_idx = policies.index('base')\n",
    "for i, p in enumerate(policies):\n",
    "    if i != base_idx:\n",
    "        comp = results.compare_policies(i, base_idx)\n",
    "        sig = \"*\" if comp['significant'] else \"\"\n",
    "        print(f\"{p}: {comp['difference']:+.3f} (p={comp['p_value']:.3f}) {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check If Calibration Transfers\n",
    "\n",
    "Calibration is learned on one distribution. Does it still work on new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom cje.diagnostics import audit_transportability, plot_transport_comparison\n\n# Test on probe slices (held-out data with oracle labels)\nprobe_files = {\n    \"clone\": \"arena_sample/probe_slice/clone_probe.jsonl\",\n    \"modified_prompt\": \"arena_sample/probe_slice/parallel_universe_prompt_probe.jsonl\",\n    \"adversarial\": \"arena_sample/probe_slice/unhelpful_probe.jsonl\",\n}\n\naudits = {}\nfor name, path in probe_files.items():\n    data = [json.loads(line) for line in open(path)]\n    audits[name] = audit_transportability(results.calibrator, data)\n    print(audits[name].summary())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize: which variants break calibration?\nplot_transport_comparison(audits, title=\"Does Calibration Transfer?\");"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed view of failing variant - residuals by score decile\naudits['adversarial'].plot();"
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Inspect What's Fooling the Judge\n\nWhen calibration fails, look at the actual samples. What patterns fool the judge but not the oracle?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from cje.diagnostics import compute_residuals\n\n# Compute residuals for each sample (sorted by worst overestimate first)\nadversarial_data = [json.loads(line) for line in open(\"arena_sample/probe_slice/unhelpful_probe.jsonl\")]\nsamples = compute_residuals(results.calibrator, adversarial_data)\n\nprint(f\"Samples: {len(samples)}\")\nprint(f\"Mean residual: {sum(s['residual'] for s in samples) / len(samples):.3f}\")\nprint(f\"\\nResidual = Oracle - Calibrated\")\nprint(f\"  Negative = judge overestimated (fooled)\")\nprint(f\"  Positive = judge underestimated\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Look at the worst overestimates - where the judge was most fooled\nprint(\"WORST OVERESTIMATES: Judge gave high scores, oracle gave low scores\")\nprint(\"-\" * 70)\n\nfor i, s in enumerate(samples[:3]):\n    print(f\"\\nSample {i+1} | Residual: {s['residual']:.2f}\")\n    print(f\"  Judge: {s['judge_score']:.2f} â†’ Calibrated: {s['calibrated']:.2f} | Oracle: {s['oracle_label']:.2f}\")\n    print(f\"  Prompt: {s['prompt']}\")\n    print(f\"  Response: {s['response']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**What we see**: The adversarial prompt produces responses that *sound* helpful (confident, structured, detailed) but are actually wrong or misleading. The cheap judge is fooled by surface features; the oracle catches the substance.\n\n**The fix**: This is exactly why you need calibration. Raw judge scores would rank the adversarial prompt too high. CJE's transportability check flags this before you ship bad decisions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Monitor Calibration Over Time\n\nCalibration drifts. Periodically check it with fresh oracle labels. Here we simulate gradual drift:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate weekly monitoring with gradual drift\nimport random\n\n# Load base data with oracle labels for monitoring simulation\nbase_data = [json.loads(l) for l in open(\"arena_sample/fresh_draws/base_responses.jsonl\")]\nbase_oracle = [r for r in base_data if r.get(\"oracle_label\") is not None]\nunhelpful_data = [json.loads(l) for l in open(\"arena_sample/probe_slice/unhelpful_probe.jsonl\")]\n\nrandom.seed(42)\nrandom.shuffle(base_oracle)\n\n# Week 1: stable (pure base data)\nweek1 = base_oracle[:48]\n\n# Week 2: starting to drift (65% base, 35% adversarial)\nweek2_base = base_oracle[48:79]\nweek2_adv = unhelpful_data[:17]\nweek2 = week2_base + week2_adv\nrandom.shuffle(week2)\n\n# Week 3: drifted (40% base, 60% adversarial) \nweek3_base = base_oracle[79:99]\nweek3_adv = unhelpful_data[17:47]\nweek3 = week3_base + week3_adv\nrandom.shuffle(week3)\n\nweekly_audits = {\n    \"Week 1\": audit_transportability(results.calibrator, week1),\n    \"Week 2\": audit_transportability(results.calibrator, week2),\n    \"Week 3\": audit_transportability(results.calibrator, week3),\n}\n\nfor name, audit in weekly_audits.items():\n    print(audit.summary())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plot_transport_comparison(weekly_audits, title=\"Weekly Calibration Check\");"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "```python\n",
    "# Compare prompt variants\n",
    "results = analyze_dataset(fresh_draws_dir=\"data/responses/\")\n",
    "results.plot_estimates()\n",
    "\n",
    "# Check calibration transfers\n",
    "audit = audit_transportability(results.calibrator, new_data)\n",
    "print(audit.summary())  # PASS or FAIL\n",
    "\n",
    "# Monitor over time\n",
    "plot_transport_comparison({\"Week 1\": audit1, \"Week 2\": audit2, ...})\n",
    "```\n",
    "\n",
    "**PASS** = calibration valid, trust the estimates  \n",
    "**FAIL** = something changed, investigate or recalibrate\n",
    "\n",
    "### Learn More\n",
    "- [Why your metrics lie](https://cimolabs.com/blog/metrics-lying) â€” The full explanation\n",
    "- [Arena experiment](https://www.cimolabs.com/research/arena-experiment) â€” Benchmarks on 5,000 prompts\n",
    "- [GitHub](https://github.com/cimo-labs/cje) â€” Documentation and source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}