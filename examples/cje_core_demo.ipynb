{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CJE: Calibrate Your LLM Judge\n",
    "\n",
    "Your LLM judge scores are lying. CJE calibrates them to what actually matters.\n",
    "\n",
    "**The problem**: Cheap judge scores (S) don't match expensive oracle outcomes (Y). CJE learns the S→Y mapping so you can trust your metrics.\n",
    "\n",
    "[Read the full explanation →](https://cimolabs.com/blog/metrics-lying)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_core_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CJE\n",
    "!pip install -q cje-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data (1000 Chatbot Arena prompts, 4 prompt variants)\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"arena_sample\")\n",
    "if not (DATA_DIR / \"fresh_draws\" / \"base_responses.jsonl\").exists():\n",
    "    print(\"Downloading sample data...\")\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    (DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n",
    "    (DATA_DIR / \"probe_slice\").mkdir(exist_ok=True)\n",
    "    \n",
    "    BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
    "    for f in [\"base_responses.jsonl\", \"clone_responses.jsonl\", \n",
    "              \"parallel_universe_prompt_responses.jsonl\", \"unhelpful_responses.jsonl\"]:\n",
    "        urllib.request.urlretrieve(f\"{BASE_URL}/fresh_draws/{f}\", DATA_DIR / \"fresh_draws\" / f)\n",
    "    for f in [\"clone_probe.jsonl\", \"parallel_universe_prompt_probe.jsonl\", \"unhelpful_probe.jsonl\"]:\n",
    "        urllib.request.urlretrieve(f\"{BASE_URL}/probe_slice/{f}\", DATA_DIR / \"probe_slice\" / f)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(f\"Data exists at {DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare Prompt Variants\n",
    "\n",
    "One line to analyze all your prompt variants with calibrated estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje import analyze_dataset\n",
    "\n",
    "results = analyze_dataset(fresh_draws_dir=\"arena_sample/fresh_draws/\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with confidence intervals\n",
    "results.plot_estimates(\n",
    "    policy_labels={\n",
    "        \"base\": \"Standard prompt\",\n",
    "        \"clone\": \"Same prompt (different seed)\",\n",
    "        \"parallel_universe_prompt\": \"Modified system prompt\",\n",
    "        \"unhelpful\": \"Adversarial prompt\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "policies = results.metadata['target_policies']\n",
    "best = policies[results.best_policy()]\n",
    "print(f\"Best: {best}\\n\")\n",
    "\n",
    "# Compare all to base\n",
    "base_idx = policies.index('base')\n",
    "for i, p in enumerate(policies):\n",
    "    if i != base_idx:\n",
    "        comp = results.compare_policies(i, base_idx)\n",
    "        sig = \"*\" if comp['significant'] else \"\"\n",
    "        print(f\"{p}: {comp['difference']:+.3f} (p={comp['p_value']:.3f}) {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check If Calibration Transfers\n",
    "\n",
    "Calibration is learned on one distribution. Does it still work on new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from cje.diagnostics import audit_transportability, plot_transport_comparison\n",
    "\n",
    "# Test on probe slices (held-out data with oracle labels)\n",
    "probe_files = {\n",
    "    \"clone\": \"arena_sample/probe_slice/clone_probe.jsonl\",\n",
    "    \"modified_prompt\": \"arena_sample/probe_slice/parallel_universe_prompt_probe.jsonl\",\n",
    "    \"adversarial\": \"arena_sample/probe_slice/unhelpful_probe.jsonl\",\n",
    "}\n",
    "\n",
    "audits = {}\n",
    "for name, path in probe_files.items():\n",
    "    data = [json.loads(line) for line in open(path)]\n",
    "    audits[name] = audit_transportability(results.calibrator, data)\n",
    "    print(audits[name].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: which variants break calibration?\n",
    "plot_transport_comparison(audits, title=\"Does Calibration Transfer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed view of failing variant - residuals by score decile\naudits['adversarial'].plot()"
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Inspect What's Fooling the Judge\n\nWhen calibration fails, look at the actual samples. What patterns fool the judge but not the oracle?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from cje.diagnostics import compute_residuals\n\n# Load adversarial samples and compute residuals (sorted by worst overestimate first)\nadversarial_data = [json.loads(line) for line in open(\"arena_sample/probe_slice/unhelpful_probe.jsonl\")]\nsamples = compute_residuals(results.calibrator, adversarial_data)\n\nprint(f\"Samples: {len(samples)}\")\nprint(f\"Mean residual: {sum(s['residual'] for s in samples) / len(samples):.3f}\")\nprint(f\"\\nResidual = Oracle - Calibrated\")\nprint(f\"  Negative = judge overestimated (fooled)\")\nprint(f\"  Positive = judge underestimated\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Look at the worst overestimates - where the judge was most fooled\nprint(\"=\" * 80)\nprint(\"WORST OVERESTIMATES: Judge gave high scores, oracle gave low scores\")\nprint(\"=\" * 80)\n\nfor i, s in enumerate(samples[:3]):\n    print(f\"\\n--- Sample {i+1} | Residual: {s['residual']:.2f}\")\n    print(f\"    Judge: {s['judge_score']:.2f} → Calibrated: {s['calibrated']:.2f} | Oracle: {s['oracle_label']:.2f}\")\n    print(f\"\\n    Prompt: {s['prompt'][:150]}...\")\n    print(f\"\\n    Response: {s['response'][:300]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**What we see**: The adversarial prompt produces responses that *sound* helpful (confident, structured, detailed) but are actually wrong or misleading. The cheap judge is fooled by surface features; the oracle catches the substance.\n\n**The fix**: This is exactly why you need calibration. Raw judge scores would rank the adversarial prompt too high. CJE's transportability check flags this before you ship bad decisions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Monitor Calibration Over Time\n\nCalibration drifts. Periodically check it with fresh oracle labels. Here we simulate gradual drift:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate weekly monitoring with gradual drift\nimport random\n\nbase_data = [json.loads(l) for l in open(\"arena_sample/fresh_draws/base_responses.jsonl\")]\nbase_oracle = [r for r in base_data if r.get(\"oracle_label\") is not None]\nunhelpful_data = [json.loads(l) for l in open(\"arena_sample/probe_slice/unhelpful_probe.jsonl\")]\n\nrandom.seed(42)\nrandom.shuffle(base_oracle)\n\n# Week 1: stable (pure base data)\nweek1 = base_oracle[:48]\n\n# Week 2: starting to drift (65% base, 35% adversarial)\nweek2_base = base_oracle[48:79]\nweek2_adv = unhelpful_data[:17]\nweek2 = week2_base + week2_adv\nrandom.shuffle(week2)\n\n# Week 3: drifted (40% base, 60% adversarial) \nweek3_base = base_oracle[79:99]\nweek3_adv = unhelpful_data[17:47]\nweek3 = week3_base + week3_adv\nrandom.shuffle(week3)\n\nweekly_audits = {\n    \"Week 1\": audit_transportability(results.calibrator, week1),\n    \"Week 2\": audit_transportability(results.calibrator, week2),\n    \"Week 3\": audit_transportability(results.calibrator, week3),\n}\n\nfor name, audit in weekly_audits.items():\n    print(audit.summary())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transport_comparison(weekly_audits, title=\"Weekly Calibration Check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "```python\n",
    "# Compare prompt variants\n",
    "results = analyze_dataset(fresh_draws_dir=\"data/responses/\")\n",
    "results.plot_estimates()\n",
    "\n",
    "# Check calibration transfers\n",
    "audit = audit_transportability(results.calibrator, new_data)\n",
    "print(audit.summary())  # PASS or FAIL\n",
    "\n",
    "# Monitor over time\n",
    "plot_transport_comparison({\"Week 1\": audit1, \"Week 2\": audit2, ...})\n",
    "```\n",
    "\n",
    "**PASS** = calibration valid, trust the estimates  \n",
    "**FAIL** = something changed, investigate or recalibrate\n",
    "\n",
    "### Learn More\n",
    "- [Why your metrics lie](https://cimolabs.com/blog/metrics-lying) — The full explanation\n",
    "- [Arena experiment](https://www.cimolabs.com/research/arena-experiment) — Benchmarks on 5,000 prompts\n",
    "- [GitHub](https://github.com/cimo-labs/cje) — Documentation and source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}