{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your AI Metrics Are Lying to You: CJE Core Demo\n",
    "\n",
    "This notebook demonstrates the **Causal Judge Evaluation (CJE)** concepts from first principles.\n",
    "\n",
    "**The problem**: What scores high on quick metrics can predict low value on what actually matters. \"You're absolutely right!\" scored high on politeness but tanked developer productivity.\n",
    "\n",
    "**The fix**: Learn how your cheap metrics (S) predict real outcomes (Y) using a calibration slice.\n",
    "\n",
    "## The Deliberation Ladder\n",
    "\n",
    "```\n",
    "Y* │ Idealized Deliberation Oracle (unobservable)\n",
    "   │ What you'd decide with unlimited time & perfect information\n",
    "   │\n",
    "Y  │ Oracle / High-Rung Outcome  \n",
    "   │ Expensive but practical labels (expert audits, task success)\n",
    "   │\n",
    "S  │ Cheap Surrogate\n",
    "   │ Fast signals at scale (LLM-judge scores, clicks)\n",
    "```\n",
    "\n",
    "CJE calibrates S→Y so you can aim abundant S data at Y*.\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_core_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "# !pip install -q scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download Arena sample data from GitHub (for Colab)\nDATA_DIR = Path(\"arena_sample\")\n\n# Check if data already exists locally\nif (DATA_DIR / \"fresh_draws\" / \"base_responses.jsonl\").exists():\n    print(f\"Data already exists at {DATA_DIR.absolute()}\")\nelse:\n    print(\"Downloading data from GitHub...\")\n    DATA_DIR.mkdir(exist_ok=True)\n    (DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n    (DATA_DIR / \"probe_slice\").mkdir(exist_ok=True)\n    \n    BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n    \n    # Fresh draws\n    fresh_files = [\"base_responses.jsonl\", \"clone_responses.jsonl\", \n                   \"parallel_universe_prompt_responses.jsonl\", \"unhelpful_responses.jsonl\"]\n    for filename in fresh_files:\n        print(f\"  Downloading fresh_draws/{filename}...\")\n        urllib.request.urlretrieve(\n            f\"{BASE_URL}/fresh_draws/{filename}\",\n            DATA_DIR / \"fresh_draws\" / filename\n        )\n    \n    # Probe slice (for transportability testing)\n    probe_files = [\"clone_probe.jsonl\", \"parallel_universe_prompt_probe.jsonl\", \"unhelpful_probe.jsonl\"]\n    for filename in probe_files:\n        print(f\"  Downloading probe_slice/{filename}...\")\n        urllib.request.urlretrieve(\n            f\"{BASE_URL}/probe_slice/{filename}\",\n            DATA_DIR / \"probe_slice\" / filename\n        )\n    print(\"Done!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load the Arena Sample Data\n\nWe have 4 policies evaluated on 1000 Chatbot Arena prompts:\n- **base**: Llama 3.3 70B with standard prompt (logging policy)\n- **clone**: Same as base, different seed (sanity check)\n- **parallel_universe_prompt**: Modified system prompt\n- **unhelpful**: Deliberately confusing responses (adversarial)\n\nEach response has:\n- **S (judge_score)**: GPT-4.1-nano score (cheap, 16x cheaper)\n- **Y (oracle_label)**: GPT-5 score (expensive oracle)\n\nOracle coverage:\n- **base**: ~48% (calibration training set)\n- **other policies**: ~5% each (probe slice for transportability testing)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_policy_data(policy: str, data_dir: Path) -> pd.DataFrame:\n    \"\"\"Load response data for a policy.\"\"\"\n    path = data_dir / \"fresh_draws\" / f\"{policy}_responses.jsonl\"\n    records = []\n    with open(path) as f:\n        for line in f:\n            r = json.loads(line)\n            records.append({\n                'prompt_id': r['prompt_id'],\n                'prompt': r['prompt'],\n                'response': r['response'],\n                'policy': policy,\n                'judge_score': r['judge_score'],  # S\n                'oracle_label': r.get('oracle_label'),  # Y (may be None)\n                'response_length': len(r['response'])\n            })\n    return pd.DataFrame(records)\n\ndef load_probe_slice(policy: str, data_dir: Path) -> pd.DataFrame:\n    \"\"\"Load probe slice with oracle labels for transportability testing.\"\"\"\n    path = data_dir / \"probe_slice\" / f\"{policy}_probe.jsonl\"\n    if not path.exists():\n        return pd.DataFrame()\n    records = []\n    with open(path) as f:\n        for line in f:\n            r = json.loads(line)\n            records.append({\n                'prompt_id': r['prompt_id'],\n                'prompt': r['prompt'],\n                'response': r['response'],\n                'policy': policy,\n                'judge_score': r['judge_score'],\n                'oracle_label': r['oracle_label'],\n                'response_length': len(r['response'])\n            })\n    return pd.DataFrame(records)\n\n# Load data for all policies\nPOLICIES = ['base', 'clone', 'parallel_universe_prompt', 'unhelpful']\n\n# Fresh draws (for estimation)\nall_data = pd.concat([load_policy_data(p, DATA_DIR) for p in POLICIES], ignore_index=True)\n\n# Probe slice (for transportability testing - separate from calibration training)\nprobe_data = pd.concat([load_probe_slice(p, DATA_DIR) for p in POLICIES if p != 'base'], ignore_index=True)\n\nprint(f\"Loaded {len(all_data):,} fresh draw samples across {all_data['policy'].nunique()} policies\")\nprint(f\"Loaded {len(probe_data):,} probe slice samples for transportability testing\")\n\n# Show oracle coverage\noracle_coverage = all_data.groupby('policy')['oracle_label'].apply(lambda x: x.notna().mean())\nprint(f\"\\nOracle coverage (fresh_draws):\")\nprint(oracle_coverage.round(2))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "all_data.groupby('policy')[['judge_score', 'oracle_label']].agg(['mean', 'std', 'count']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem: Naive S ≠ Y\n",
    "\n",
    "If we just use judge scores (S) directly, we might get the wrong answer. Let's see how S and Y differ across policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare S vs Y by policy (only samples with oracle labels)\n",
    "has_oracle = all_data[all_data['oracle_label'].notna()].copy()\n",
    "\n",
    "policy_stats = has_oracle.groupby('policy').agg({\n",
    "    'judge_score': 'mean',\n",
    "    'oracle_label': 'mean'\n",
    "}).round(3)\n",
    "policy_stats['gap'] = (policy_stats['judge_score'] - policy_stats['oracle_label']).round(3)\n",
    "policy_stats = policy_stats.sort_values('oracle_label', ascending=False)\n",
    "\n",
    "print(\"Policy Rankings: Judge (S) vs Oracle (Y)\")\n",
    "print(\"=\"*50)\n",
    "print(policy_stats)\n",
    "print(\"\\nNote the gap between judge and oracle for each policy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize S vs Y correlation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Overall correlation (samples with oracle)\n",
    "ax = axes[0]\n",
    "sample = has_oracle.sample(min(2000, len(has_oracle)), random_state=42)\n",
    "ax.scatter(sample['judge_score'], sample['oracle_label'], alpha=0.3, s=10)\n",
    "ax.plot([0, 1], [0, 1], 'r--', label='Perfect calibration')\n",
    "ax.set_xlabel('Judge Score (S)', fontsize=12)\n",
    "ax.set_ylabel('Oracle Label (Y)', fontsize=12)\n",
    "ax.set_title('S vs Y: Overall Correlation', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "# Right: By policy\n",
    "ax = axes[1]\n",
    "colors = {'base': 'C0', 'clone': 'C1', 'parallel_universe_prompt': 'C3', 'unhelpful': 'red'}\n",
    "for policy in POLICIES:\n",
    "    data = has_oracle[has_oracle['policy'] == policy]\n",
    "    if len(data) > 0:\n",
    "        ax.scatter(data['judge_score'].mean(), data['oracle_label'].mean(), \n",
    "                   s=200, label=policy, c=colors[policy], edgecolors='black', linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('Mean Judge Score (S)', fontsize=12)\n",
    "ax.set_ylabel('Mean Oracle Label (Y)', fontsize=12)\n",
    "ax.set_title('Policy Means: S vs Y', fontsize=14)\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe 'unhelpful' policy (red) shows the biggest gap - the judge is fooled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Solution: Calibrate S→Y\n",
    "\n",
    "We learn a calibration function on the **base policy** (our oracle slice), then apply it to all policies.\n",
    "\n",
    "**AutoCal-R**: Isotonic regression ensures predictions are monotone in S and mean-preserving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train calibrator on base policy only (samples with oracle labels)\n",
    "base_with_oracle = all_data[(all_data['policy'] == 'base') & (all_data['oracle_label'].notna())].copy()\n",
    "S_train = base_with_oracle['judge_score'].values\n",
    "Y_train = base_with_oracle['oracle_label'].values\n",
    "\n",
    "# Fit isotonic regression (AutoCal-R)\n",
    "calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrator.fit(S_train, Y_train)\n",
    "\n",
    "print(f\"Calibrator trained on {len(S_train):,} base policy samples with oracle labels\")\n",
    "print(f\"Mean S: {S_train.mean():.3f}, Mean Y: {Y_train.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the calibration curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Scatter of training data\n",
    "ax.scatter(S_train, Y_train, alpha=0.2, s=10, label='Training data (base with oracle)')\n",
    "\n",
    "# Calibration curve\n",
    "S_grid = np.linspace(0, 1, 100)\n",
    "Y_pred = calibrator.predict(S_grid)\n",
    "ax.plot(S_grid, Y_pred, 'r-', linewidth=3, label='Isotonic calibration')\n",
    "\n",
    "# Perfect calibration line\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect calibration (S=Y)')\n",
    "\n",
    "ax.set_xlabel('Judge Score (S)', fontsize=12)\n",
    "ax.set_ylabel('Oracle Label (Y) / Calibrated Prediction', fontsize=12)\n",
    "ax.set_title('AutoCal-R: Learning S→Y Mapping', fontsize=14)\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe calibration curve shows how to map judge scores to oracle-scale predictions.\")\n",
    "print(\"Notice the curve is monotone (higher S -> higher predicted Y).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Calibration & Compute Residuals\n",
    "\n",
    "Now we apply the calibrator to ALL policies and compute residuals (for samples with oracle labels):\n",
    "\n",
    "**Residual = Y - Y_hat = Oracle - Calibrated_Prediction**\n",
    "\n",
    "- Residual < 0: Calibrator **over-predicted** (predicted quality is higher than actual)\n",
    "- Residual > 0: Calibrator **under-predicted** (predicted quality is lower than actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply calibration to all data\nall_data['calibrated_pred'] = calibrator.predict(all_data['judge_score'].values)\n\n# Load probe slice and run transportability audit using CJE canonical interface\nfrom cje.diagnostics import audit_transportability\n\nprint(\"Transportability Audit (CJE Canonical Interface)\")\nprint(\"=\"*70)\n\ntransport_results = {}\nfor policy in ['clone', 'parallel_universe_prompt', 'unhelpful']:\n    # Load probe directly as dicts - no boilerplate needed!\n    probe_path = DATA_DIR / \"probe_slice\" / f\"{policy}_probe.jsonl\"\n    probe = [json.loads(line) for line in open(probe_path)]\n    \n    # Run audit\n    diag = audit_transportability(calibrator, probe, group_label=f\"policy:{policy}\")\n    transport_results[policy] = diag\n    print(diag.summary())\n\n# Also compute base residuals for comparison\nbase_with_oracle['calibrated_pred'] = calibrator.predict(base_with_oracle['judge_score'].values)\nbase_with_oracle['residual'] = base_with_oracle['oracle_label'] - base_with_oracle['calibrated_pred']\nprint(f\"\\nBase policy: mean residual = {base_with_oracle['residual'].mean():.4f} (training set, ~0 by construction)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transportability visualization using canonical CJE interface\nfrom cje.diagnostics import plot_transport_comparison\n\n# Plot comparison of all policies\nfig = plot_transport_comparison(transport_results, title=\"Transportability Audit: Does Calibration Transfer?\")\nplt.show()\n\n# Also show detailed view for unhelpful (the failing policy)\nprint(\"\\nDetailed view: unhelpful policy (FAIL)\")\nfig = transport_results['unhelpful'].plot()\nplt.show()\n\nprint(\"\\nKey insight: The unhelpful policy shows systematic overestimation (negative residuals)\")\nprint(\"across ALL score deciles - the judge is fooled regardless of how confident it is.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Deep Dive: Why Does 'unhelpful' Fail?\n\nThe transportability test showed `unhelpful` has a large negative mean residual - the calibrator **overestimates** its quality.\n\nLet's look at **individual samples** with the biggest residuals to understand what's fooling the calibrator."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute residuals for probe data (for visualization)\nprobe_data['calibrated_pred'] = calibrator.predict(probe_data['judge_score'].values)\nprobe_data['residual'] = probe_data['oracle_label'] - probe_data['calibrated_pred']\n\n# Get unhelpful policy data from probe slice\nunhelpful_data = probe_data[probe_data['policy'] == 'unhelpful'].copy()\nunhelpful_data['abs_residual'] = unhelpful_data['residual'].abs()\n\nif len(unhelpful_data) > 0:\n    # Summary stats\n    print(\"UNHELPFUL Policy Residual Analysis (from probe slice)\")\n    print(\"=\"*50)\n    print(f\"Samples with oracle labels: {len(unhelpful_data)}\")\n    print(f\"Mean residual: {unhelpful_data['residual'].mean():.4f}\")\n    print(f\"Std residual:  {unhelpful_data['residual'].std():.4f}\")\n    print(f\"Min residual:  {unhelpful_data['residual'].min():.4f} (biggest overestimate)\")\n    print(f\"Max residual:  {unhelpful_data['residual'].max():.4f} (biggest underestimate)\")\n    print(f\"\\nSamples with |residual| > 0.3: {(unhelpful_data['abs_residual'] > 0.3).sum():,}\")\n    print(f\"Samples with |residual| > 0.5: {(unhelpful_data['abs_residual'] > 0.5).sum():,}\")\nelse:\n    print(\"No probe slice data for unhelpful policy\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show top 5 biggest overestimates (where calibrator was most fooled)\nif len(unhelpful_data) > 0:\n    worst_overestimates = unhelpful_data.nsmallest(5, 'residual')\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"TOP 5 BIGGEST OVERESTIMATES (Calibrator was most fooled)\")\n    print(\"=\"*80)\n\n    for i, (_, row) in enumerate(worst_overestimates.iterrows(), 1):\n        print(f\"\\n--- #{i} | Residual: {row['residual']:.3f}\")\n        print(f\"    Judge: {row['judge_score']:.2f} -> Calibrated: {row['calibrated_pred']:.2f} | Oracle: {row['oracle_label']:.2f}\")\n        print(f\"    Prompt: {row['prompt'][:100]}...\")\n        print(f\"    Response: {row['response'][:200]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize residual distribution by policy\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Combine base (training) and probe (test) data for visualization\nwith_oracle = pd.concat([base_with_oracle, probe_data], ignore_index=True)\n\n# Left: Histogram of residuals\nax = axes[0]\nfor policy in ['base', 'clone', 'unhelpful']:\n    data = with_oracle[with_oracle['policy'] == policy]['residual']\n    if len(data) > 0:\n        ax.hist(data, bins=30, alpha=0.5, label=f\"{policy} (n={len(data)})\", density=True)\nax.axvline(x=0, color='black', linestyle='--', linewidth=2)\nax.set_xlabel('Residual (Y - Ŷ)', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('Residual Distribution by Policy', fontsize=14)\nax.legend()\n\n# Right: Residual vs Judge Score for unhelpful\nax = axes[1]\nif len(unhelpful_data) > 0:\n    scatter = ax.scatter(unhelpful_data['judge_score'], unhelpful_data['residual'], \n                         c=unhelpful_data['oracle_label'], cmap='RdYlGn', alpha=0.6, s=30)\n    ax.axhline(y=0, color='black', linestyle='--', linewidth=2)\n    ax.set_xlabel('Judge Score (S)', fontsize=12)\n    ax.set_ylabel('Residual (Y - Ŷ)', fontsize=12)\n    ax.set_title('Unhelpful Policy: Residual vs Judge Score', fontsize=14)\n    plt.colorbar(scatter, ax=ax, label='Oracle Label')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey insight: High judge scores (S > 0.6) often have negative residuals.\")\nprint(\"The judge gives high scores to confident nonsense; the oracle sees through it.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. What Patterns Fool the Judge?\n\nLet's analyze what characteristics are associated with large overestimates (negative residuals)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze response length vs residual\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Response length vs residual for unhelpful\nax = axes[0]\nif len(unhelpful_data) > 0:\n    ax.scatter(unhelpful_data['response_length'], unhelpful_data['residual'], alpha=0.3, s=20)\n    ax.axhline(y=0, color='red', linestyle='--', linewidth=2)\n\n    # Add trend line\n    z = np.polyfit(unhelpful_data['response_length'], unhelpful_data['residual'], 1)\n    p = np.poly1d(z)\n    x_line = np.linspace(unhelpful_data['response_length'].min(), unhelpful_data['response_length'].max(), 100)\n    ax.plot(x_line, p(x_line), 'orange', linewidth=2, label=f'Trend: slope={z[0]:.2e}')\n\n    ax.set_xlabel('Response Length (chars)', fontsize=12)\n    ax.set_ylabel('Residual (Y - Y_hat)', fontsize=12)\n    ax.set_title('Unhelpful: Does Length Predict Residual?', fontsize=14)\n    ax.legend()\n\n# Right: Compare response lengths by policy\nax = axes[1]\nall_data.boxplot(column='response_length', by='policy', ax=ax)\nax.set_xlabel('Policy', fontsize=12)\nax.set_ylabel('Response Length (chars)', fontsize=12)\nax.set_title('Response Length by Policy', fontsize=14)\nplt.suptitle('')  # Remove auto-title\n\nplt.tight_layout()\nplt.show()\n\n# Correlation analysis\nif len(unhelpful_data) > 0:\n    print(\"\\nCorrelation with Residual (unhelpful policy):\")\n    print(f\"  Response length: {unhelpful_data['response_length'].corr(unhelpful_data['residual']):.3f}\")\n    print(f\"  Judge score:     {unhelpful_data['judge_score'].corr(unhelpful_data['residual']):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. The Fix: Use Residuals to Improve Your Metrics\n\nThe CJE loop: **Calibrate -> Inspect Residuals -> Improve S -> Recalibrate**\n\nWhat we learned from the unhelpful policy:\n1. The judge is fooled by **confident-sounding nonsense**\n2. **High judge scores with low oracle** = reward hacking\n3. **Transportability fails** when response distribution shifts dramatically\n\n### Potential Fixes:\n- Add **response length** as a covariate (two-stage calibration)\n- Use a **domain-specific judge prompt** that catches deliberate misinformation\n- Add **factual verification** as an additional S signal\n- **Reject transportability** for adversarial policies and require policy-specific calibration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary: What to do with these insights\nprint(\"=\"*70)\nprint(\"SUMMARY: CJE Pipeline Insights\")\nprint(\"=\"*70)\n\nprint(\"\\nTransportability Results:\")\nprint(\"  base: PASS (training set, residual ~0 by construction)\")\nfor policy, diag in transport_results.items():\n    status_emoji = \"✓\" if diag.status == \"PASS\" else \"✗\"\n    print(f\"  {policy}: {diag.status} {status_emoji} (δ̂={diag.delta_hat:+.3f})\")\n\nprint(\"\\nRECOMMENDED ACTIONS:\")\nprint(\"  1. For PASS policies: Use calibrated estimates directly\")\nprint(\"  2. For FAIL policies: Report rankings only, not absolute values\")\nprint(\"  3. Investigate: What makes failing responses fool the judge?\")\nprint(\"  4. Iterate: Improve S or collect policy-specific oracle labels\")\n\n# Show recommended action for failing policies\nfor policy, diag in transport_results.items():\n    if diag.status == \"FAIL\" and diag.recommended_action:\n        print(f\"\\n  → {policy}: {diag.recommended_action}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Temporal Drift: When Calibration Goes Stale\n\nIn production, your calibration will **drift over time**:\n- User behavior changes\n- Model updates\n- World events shift topics\n- Judge/oracle relationship evolves\n\n**The solution**: Periodically collect fresh (S, Y) pairs and run residual checks.\n\n**Alert if**: Mean residual is significantly non-zero for 2+ consecutive checks.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simulate temporal drift\n# We'll pretend we're monitoring the base policy over 8 weeks\n# In weeks 1-4, calibration holds. In weeks 5-8, drift begins.\n\nnp.random.seed(42)\n\ndef simulate_drift_monitoring(base_data, calibrator, n_weeks=8, samples_per_week=50):\n    \"\"\"Simulate weekly monitoring with gradual drift starting at week 5.\"\"\"\n    \n    # Get samples with oracle labels\n    oracle_samples = base_data[base_data['oracle_label'].notna()].copy()\n    \n    weekly_results = []\n    \n    for week in range(1, n_weeks + 1):\n        # Sample a batch for this week\n        batch = oracle_samples.sample(n=min(samples_per_week, len(oracle_samples)), \n                                       replace=True, random_state=week)\n        \n        S = batch['judge_score'].values\n        Y = batch['oracle_label'].values\n        \n        # Simulate drift: starting week 5, oracle values degrade\n        # (model update made judge scores less predictive)\n        if week >= 5:\n            drift_amount = 0.03 * (week - 4)  # Growing drift\n            # Simulate: high S now corresponds to lower Y than expected\n            Y_drifted = Y - drift_amount * (S - 0.5)  # Larger effect for high S\n            Y_drifted = np.clip(Y_drifted, 0, 1)\n            Y = Y_drifted\n        \n        # Compute calibrated predictions and residuals\n        Y_hat = calibrator.predict(S)\n        residuals = Y - Y_hat\n        \n        weekly_results.append({\n            'week': week,\n            'mean_residual': np.mean(residuals),\n            'std_residual': np.std(residuals),\n            'n_samples': len(residuals),\n            'se': np.std(residuals) / np.sqrt(len(residuals))\n        })\n    \n    return pd.DataFrame(weekly_results)\n\n# Run simulation\nbase_only = all_data[all_data['policy'] == 'base']\ndrift_results = simulate_drift_monitoring(base_only, calibrator)\n\nprint(\"Weekly Residual Monitoring (Simulated)\")\nprint(\"=\"*60)\nprint(drift_results.to_string(index=False))\nprint(\"\\nNote: Drift injected starting week 5\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize drift detection\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Time series of mean residual with CI\nax = axes[0]\nweeks = drift_results['week']\nmeans = drift_results['mean_residual']\nses = drift_results['se']\n\nax.errorbar(weeks, means, yerr=1.96*ses, fmt='o-', capsize=5, \n            capthick=2, markersize=8, linewidth=2, color='C0')\n\n# Color points by alert status\nalert_threshold = 0.04  # Example threshold\nfor i, (w, m, se) in enumerate(zip(weeks, means, ses)):\n    ci_lower, ci_upper = m - 1.96*se, m + 1.96*se\n    if ci_upper < -alert_threshold or ci_lower > alert_threshold:\n        color = 'red'  # ALERT\n    elif ci_upper < 0 or ci_lower > 0:\n        color = 'orange'  # WARNING\n    else:\n        color = 'green'  # OK\n    ax.scatter(w, m, s=150, c=color, zorder=5, edgecolors='black', linewidth=2)\n\nax.axhline(y=0, color='black', linestyle='--', linewidth=2, label='Perfect calibration')\nax.axhspan(-alert_threshold, alert_threshold, alpha=0.2, color='green', label='Acceptable range')\nax.axvline(x=4.5, color='red', linestyle=':', linewidth=2, alpha=0.7, label='Drift starts')\n\nax.set_xlabel('Week', fontsize=12)\nax.set_ylabel('Mean Residual (Y - Y_hat)', fontsize=12)\nax.set_title('Drift Monitoring: Weekly Residual Checks', fontsize=14)\nax.legend(loc='lower left')\nax.set_xticks(range(1, 9))\n\n# Right: Decision logic\nax = axes[1]\nax.axis('off')\n\ndecision_text = \"\"\"\nDRIFT DETECTION LOGIC\n=====================\n\nFor each monitoring window:\n1. Collect ~50 samples with oracle labels\n2. Compute residuals: Y - calibrator(S)\n3. Test H0: mean(residual) = 0\n\nALERT LEVELS:\n  GREEN  = CI includes 0\n           Calibration still valid\n           \n  ORANGE = CI excludes 0 but small\n           Monitor closely\n           \n  RED    = CI excludes 0 significantly\n           Recalibrate immediately!\n\nRULE OF THUMB:\n  - 1 orange: Watch\n  - 2 consecutive orange: Investigate  \n  - 1 red: Recalibrate now\n  \nACTIONS:\n  1. Collect fresh oracle labels\n  2. Refit calibrator on recent data\n  3. Document what changed (model/users/world)\n\"\"\"\n\nax.text(0.1, 0.95, decision_text, transform=ax.transAxes, fontsize=11,\n        verticalalignment='top', fontfamily='monospace',\n        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nIn this simulation:\")\nprint(\"  Weeks 1-4: Calibration holds (green)\")\nprint(\"  Week 5-6:  Drift begins, warnings appear (orange)\")\nprint(\"  Week 7-8:  Significant drift, alerts triggered (red)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate recalibration after drift detection\n\ndef recalibrate_with_recent_data(old_calibrator, recent_S, recent_Y, blend_weight=0.5):\n    \"\"\"\n    Recalibrate using a blend of old predictions and new data.\n    \n    In practice, you might:\n    - Use only recent data (if drift is severe)\n    - Blend old and new (for gradual adaptation)\n    - Use exponential decay weighting by time\n    \"\"\"\n    # Option 1: Full recalibration on recent data only\n    new_calibrator = IsotonicRegression(out_of_bounds='clip')\n    new_calibrator.fit(recent_S, recent_Y)\n    \n    return new_calibrator\n\n# Simulate: we detected drift at week 6, collect 100 fresh labels\nnp.random.seed(123)\noracle_samples = base_only[base_only['oracle_label'].notna()]\nrecent_batch = oracle_samples.sample(n=100, replace=True)\n\n# Apply simulated drift to this batch (as if collected during drift period)\nS_recent = recent_batch['judge_score'].values\nY_recent = recent_batch['oracle_label'].values\ndrift_amount = 0.06  # Week 6 drift\nY_recent_drifted = Y_recent - drift_amount * (S_recent - 0.5)\nY_recent_drifted = np.clip(Y_recent_drifted, 0, 1)\n\n# Recalibrate\nnew_calibrator = recalibrate_with_recent_data(calibrator, S_recent, Y_recent_drifted)\n\n# Compare old vs new calibration curves\nfig, ax = plt.subplots(figsize=(10, 6))\n\nS_grid = np.linspace(0, 1, 100)\nY_old = calibrator.predict(S_grid)\nY_new = new_calibrator.predict(S_grid)\n\nax.plot(S_grid, Y_old, 'b-', linewidth=2, label='Old calibration (pre-drift)')\nax.plot(S_grid, Y_new, 'r-', linewidth=2, label='New calibration (post-drift)')\nax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='y=x')\n\nax.scatter(S_recent, Y_recent_drifted, alpha=0.3, s=20, c='red', label='Recent samples (drifted)')\n\nax.set_xlabel('Judge Score (S)', fontsize=12)\nax.set_ylabel('Calibrated Prediction', fontsize=12)\nax.set_title('Recalibration After Drift Detection', fontsize=14)\nax.legend()\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"After recalibration:\")\nprint(\"  - New calibrator learned the drifted S->Y relationship\")\nprint(\"  - High S values now map to lower Y (reflecting new reality)\")\nprint(\"  - Residual monitoring should return to green\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Using the CJE Library\n\nEverything above was from scratch. For production, use `cje-eval` which provides:\n\n- **AutoCal-R**: Isotonic/two-stage calibration with proper cross-fitting\n- **OUA inference**: Confidence intervals that account for calibration uncertainty\n- **Statistical tests**: Proper paired comparisons between policies\n- **Forest plots**: Publication-ready visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install CJE (uncomment for Colab)\n# !pip install cje-eval\n\nfrom cje import analyze_dataset\n\n# Run CJE analysis - one line does everything we did manually above\nresults = analyze_dataset(\n    fresh_draws_dir=str(DATA_DIR / \"fresh_draws\"),\n    estimator=\"auto\",\n    verbose=False,\n)\n\n# Compare our manual estimates vs CJE library\nprint(\"Comparison: Manual vs CJE Library\")\nprint(\"=\"*60)\nprint(f\"{'Policy':<30} {'Manual':<12} {'CJE Library':<12}\")\nprint(\"-\"*60)\n\ncje_estimates = dict(zip(results.metadata['target_policies'], results.estimates))\nfor policy in POLICIES:\n    # Get our manual calibrated mean\n    policy_data = all_data[all_data['policy'] == policy]\n    manual_est = policy_data['calibrated_pred'].mean()\n    cje_est = cje_estimates.get(policy, float('nan'))\n    print(f\"{policy:<30} {manual_est:.3f}       {cje_est:.3f}\")\n\nprint(\"\\nThe CJE library handles cross-fitting, uncertainty, and edge cases.\")"
  },
  {
   "cell_type": "code",
   "source": "# Forest plot with confidence intervals\nfig = results.plot_estimates(figsize=(10, 6))\nplt.title(\"Policy Comparison: CJE Calibrated Estimates with 95% CI\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nThe forest plot shows estimates with confidence intervals.\")\nprint(\"Overlapping CIs suggest no significant difference between policies.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Statistical comparisons between policies\ntarget_policies = results.metadata['target_policies']\nbest_idx = results.best_policy()\nbest_policy = target_policies[best_idx]\n\nprint(f\"Best policy: {best_policy}\")\nprint(\"=\"*65)\n\n# Compare all to base\nif 'base' in target_policies:\n    base_idx = target_policies.index('base')\n    print(f\"\\nComparisons vs base:\\n\")\n    print(f\"{'Policy':<30} {'Difference':<12} {'p-value':<10} {'Significant?'}\")\n    print(\"-\"*65)\n    \n    for i, policy in enumerate(target_policies):\n        if i == base_idx:\n            print(f\"{policy:<30} {'(baseline)':<12}\")\n            continue\n        comp = results.compare_policies(i, base_idx)\n        sig = \"Yes\" if comp['significant'] else \"No\"\n        print(f\"{policy:<30} {comp['difference']:+.3f}       {comp['p_value']:.3f}      {sig}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated the core CJE workflow:\n\n1. **Load data** with S (cheap) and Y (expensive) labels\n2. **Fit calibration** on an oracle slice (base policy)\n3. **Compute residuals** to check transportability\n4. **Identify failures** (e.g., policies where calibration doesn't transfer)\n5. **Analyze patterns** (what fools the judge?)\n6. **Detect drift** over time with residual monitoring\n7. **Recalibrate** when drift is detected\n\n### Key insight:\n**Ship value, not memes.** Calibrate your cheap metrics against real outcomes, catch inversions early, and stop shipping behavior that scores high on vibes but tanks on outcomes.\n\n### Learn more:\n- [CJE Advanced](cje_advanced.ipynb) - IPS and DR modes for off-policy evaluation\n- [GitHub](https://github.com/cimo-labs/cje) - Full documentation\n- [Blog: Your AI Metrics Are Lying to You](https://www.cimolabs.com/blog/your-ai-metrics-are-lying-to-you) - The full story"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}