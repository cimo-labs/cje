{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CJE (Causal Judge Evaluation) - Quick Start\n",
        "\n",
        "**Get started with CJE in 5 minutes using Arena 5K sample data (1000 prompts)**\n",
        "\n",
        "This notebook shows the simplest way to evaluate LLM policies:\n",
        "\n",
        "1. **Setup**: Install CJE and download sample data\n",
        "2. **Inspect Data**: Understand the dataset structure  \n",
        "3. **Direct Mode**: Compare policies on an evaluation set (no logprobs needed!)\n",
        "4. **Policy Selection**: Find the best policy with statistical tests\n",
        "5. **Visualization**: Plot results\n",
        "\n",
        "---\n",
        "\n",
        "## What is CJE?\n",
        "\n",
        "CJE turns LLM-as-judge scores into calibrated policy estimates:\n",
        "- **AutoCal-R**: Calibrates cheap judge scores to expensive oracle labels\n",
        "- **Valid inference**: Confidence intervals account for all uncertainty\n",
        "\n",
        "**Key insight**: Raw judge scores are correlational. CJE makes them causal.\n",
        "\n",
        "**This tutorial covers Direct Mode** - the simplest way to compare policies. For advanced off-policy evaluation (IPS and DR modes), see [`cje_advanced.ipynb`](cje_advanced.ipynb).\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "Install CJE and download the Arena sample data.\n",
        "\n",
        "**Note:** Colab comes with numpy 2.3+ pre-installed, which breaks scipy. We force-reinstall numpy 2.0.x first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab comes with numpy 2.3+ which breaks scipy\n",
        "# Force install compatible numpy first\n",
        "!pip install -q 'numpy>=2.0,<2.1' --force-reinstall\n",
        "\n",
        "# Install latest CJE from PyPI with cache busting\n",
        "!pip install --no-cache-dir --upgrade cje-eval\n",
        "\n",
        "# Verify installation\n",
        "import cje\n",
        "print(f\"âœ“ CJE version {cje.__version__} installed\")\n",
        "\n",
        "# Check numpy version\n",
        "import numpy as np\n",
        "print(f\"âœ“ NumPy version {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Arena sample data from GitHub\n",
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "# Create data directory\n",
        "DATA_DIR = Path(\"arena_sample\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "(DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n",
        "\n",
        "# Base URL for raw files on GitHub\n",
        "BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
        "\n",
        "# Download fresh draws for Direct mode\n",
        "fresh_draw_files = {\n",
        "    \"base\": \"base_responses.jsonl\",\n",
        "    \"clone\": \"clone_responses.jsonl\",\n",
        "    \"parallel_universe_prompt\": \"parallel_universe_prompt_responses.jsonl\",\n",
        "    \"unhelpful\": \"unhelpful_responses.jsonl\"\n",
        "}\n",
        "\n",
        "for policy, filename in fresh_draw_files.items():\n",
        "    print(f\"Downloading fresh_draws/{filename}...\")\n",
        "    urllib.request.urlretrieve(\n",
        "        f\"{BASE_URL}/fresh_draws/{filename}\",\n",
        "        DATA_DIR / \"fresh_draws\" / filename\n",
        "    )\n",
        "    print(f\"âœ“ Downloaded {filename}\")\n",
        "\n",
        "print(\"\\nâœ“ All data downloaded successfully!\")\n",
        "print(f\"\\nData location: {DATA_DIR.absolute()}\")\n",
        "print(f\"Policies: {list(fresh_draw_files.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Inspect the Data\n",
        "\n",
        "Let's look at what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load and inspect fresh draws (base policy)\n",
        "with open(DATA_DIR / \"fresh_draws\" / \"base_responses.jsonl\") as f:\n",
        "    base_fresh_draws = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"Base policy fresh draws: {len(base_fresh_draws)} samples\")\n",
        "print(f\"\\nExample sample:\")\n",
        "sample = base_fresh_draws[0]\n",
        "print(f\"  Prompt: {sample['prompt'][:100]}...\")\n",
        "print(f\"  Response: {sample['response'][:150]}...\")\n",
        "print(f\"  Judge score: {sample['judge_score']}\")\n",
        "oracle_label = sample.get('oracle_label', 'N/A')\n",
        "print(f\"  Oracle label: {oracle_label}\")\n",
        "\n",
        "# Check oracle coverage\n",
        "n_base_oracle = sum(1 for s in base_fresh_draws if s.get('oracle_label') is not None)\n",
        "base_coverage = n_base_oracle / len(base_fresh_draws)\n",
        "print(f\"\\nOracle label coverage: {n_base_oracle}/{len(base_fresh_draws)} ({base_coverage:.1%})\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ CJE uses these oracle labels for AutoCal-R calibration:\")\n",
        "print(f\"   1. Learn mapping: judge_score â†’ oracle_label (from base policy)\")\n",
        "print(f\"   2. Apply calibration to all policies for fair comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Direct Mode - Policy Comparison\n",
        "\n",
        "**Direct Mode** is the simplest way to compare policies:\n",
        "\n",
        "**Use case**: \"Which policy performs best on this evaluation set?\"\n",
        "\n",
        "**How it works**:\n",
        "1. Generate fresh responses from each policy on the same prompts\n",
        "2. Calibrate judge scores using oracle labels (AutoCal-R)\n",
        "3. Average calibrated scores: VÌ‚(Ï€) = (1/m) Î£ R_Ï€i\n",
        "\n",
        "**Key advantages**:\n",
        "- âœ“ No logprobs needed\n",
        "- âœ“ Simplest mode\n",
        "- âœ“ Works with any LLM (even closed-source APIs)\n",
        "- âœ“ Just 5-10% oracle labels often enough\n",
        "\n",
        "**When to use**: Quick policy comparisons, A/B testing, leaderboards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cje import analyze_dataset\n",
        "\n",
        "# Run Direct mode analysis\n",
        "results = analyze_dataset(\n",
        "    fresh_draws_dir=str(DATA_DIR / \"fresh_draws\"),\n",
        "    estimator=\"auto\",  # Auto-detects Direct mode\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Direct Mode Results\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Mode: {results.metadata['mode']}\")\n",
        "print(f\"Estimator: {results.metadata['estimator']}\")\n",
        "print(f\"Calibration: {results.metadata.get('calibration', 'none')}\")\n",
        "print(f\"Oracle coverage: {results.metadata.get('oracle_coverage', 0):.1%}\")\n",
        "print()\n",
        "\n",
        "# Show estimates with confidence intervals\n",
        "policies = results.metadata['target_policies']\n",
        "print(f\"{'Policy':<30} {'Estimate':<12} {'Std Error':<12} {'95% CI':<20}\")\n",
        "print(\"-\" * 74)\n",
        "for i, policy in enumerate(policies):\n",
        "    est = results.estimates[i]\n",
        "    se = results.standard_errors[i]\n",
        "    ci_low = est - 1.96 * se\n",
        "    ci_high = est + 1.96 * se\n",
        "    print(f\"{policy:<30} {est:>6.3f}       {se:>6.3f}       [{ci_low:.3f}, {ci_high:.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Policy Selection\n",
        "\n",
        "Now let's find the best policy and compare it to a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get results\n",
        "policies = results.metadata['target_policies']\n",
        "estimates = np.array(results.estimates)\n",
        "std_errors = np.array(results.standard_errors)\n",
        "\n",
        "# Find best policy\n",
        "best_idx = results.best_policy()\n",
        "best_policy = policies[best_idx]\n",
        "best_est = estimates[best_idx]\n",
        "best_se = std_errors[best_idx]\n",
        "\n",
        "print(\"Policy Selection\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nğŸ† Best policy: {best_policy}\")\n",
        "print(f\"   Estimate: {best_est:.3f} Â± {best_se:.3f}\")\n",
        "print(f\"   95% CI: [{best_est - 1.96*best_se:.3f}, {best_est + 1.96*best_se:.3f}]\")\n",
        "\n",
        "# Compare against baseline (clone)\n",
        "baseline_policy = 'clone'\n",
        "baseline_idx = policies.index(baseline_policy)\n",
        "\n",
        "print(f\"\\nğŸ“Š Comparison to baseline ({baseline_policy}):\")\n",
        "print(f\"   Baseline: {estimates[baseline_idx]:.3f} Â± {std_errors[baseline_idx]:.3f}\")\n",
        "print()\n",
        "print(f\"{'Policy':<30} {'Delta':<12} {'SE(Î”)':<12} {'p-value':<12} {'Significant?':<15}\")\n",
        "print(\"-\" * 81)\n",
        "\n",
        "for i, policy in enumerate(policies):\n",
        "    if i == baseline_idx:\n",
        "        print(f\"{policy:<30} {'(baseline)':<12} {'':<12} {'':<12} {'':<15}\")\n",
        "        continue\n",
        "    \n",
        "    # Use built-in comparison (uses influence functions for paired variance)\n",
        "    comp = results.compare_policies(i, baseline_idx)\n",
        "    \n",
        "    sig_text = \"âœ“ Yes (p<0.05)\" if comp['significant'] else \"No\"\n",
        "    \n",
        "    print(f\"{policy:<30} {comp['difference']:+.3f}       \"\n",
        "          f\"{comp['se_difference']:.3f}       \"\n",
        "          f\"{comp['p_value']:.3f}      {sig_text:<15}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ CJE uses influence functions for paired comparisons,\")\n",
        "print(\"   accounting for correlation between estimates on the same prompts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Visualization\n",
        "\n",
        "### HTML Display\n",
        "\n",
        "In Jupyter notebooks, results automatically display as formatted tables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Just evaluate the result object to see a nice HTML table\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Forest Plot\n",
        "\n",
        "Create a forest plot with confidence intervals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use the convenience method\n",
        "fig = results.plot_estimates()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ’¡ Green dot = best policy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Visualization\n",
        "\n",
        "You can also use standalone plot functions from `cje`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cje import plot_policy_estimates\n",
        "\n",
        "# Extract estimates as dictionaries\n",
        "estimates_dict = {policy: float(results.estimates[i]) for i, policy in enumerate(policies)}\n",
        "ses_dict = {policy: float(results.standard_errors[i]) for i, policy in enumerate(policies)}\n",
        "\n",
        "# Create forest plot\n",
        "fig = plot_policy_estimates(\n",
        "    estimates=estimates_dict,\n",
        "    standard_errors=ses_dict,\n",
        "    figsize=(10, 5)\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Forest plot shows point estimates with 95% confidence intervals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've learned how to:\n",
        "- âœ“ Run Direct mode policy evaluation (no logprobs needed!)\n",
        "- âœ“ Find the best policy with statistical tests\n",
        "- âœ“ Visualize results with confidence intervals\n",
        "\n",
        "### When to Use Direct Mode\n",
        "\n",
        "**Perfect for**:\n",
        "- Quick policy comparisons\n",
        "- A/B testing\n",
        "- Leaderboard evaluation\n",
        "- APIs without logprob access\n",
        "\n",
        "**Limitations**:\n",
        "- Evaluates performance on *this specific eval set*\n",
        "- Doesn't answer counterfactual: \"What if we deployed policy Ï€'?\"\n",
        "- For counterfactual estimates, use IPS or DR modes\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "**Want off-policy (counterfactual) evaluation?**\n",
        "- ğŸ“š See [`cje_advanced.ipynb`](cje_advanced.ipynb) for IPS and DR modes\n",
        "- Learn about importance sampling and doubly robust estimation\n",
        "- Understand when DR gives tighter confidence intervals than IPS\n",
        "\n",
        "**Documentation**:\n",
        "- [CJE README](https://github.com/cimo-labs/cje)\n",
        "- [API Reference](https://github.com/cimo-labs/cje/tree/main/cje/interface)\n",
        "- [Blog Post: Arena Experiment](https://www.cimolabs.com/blog/arena-experiment)\n",
        "\n",
        "**Install locally**: `pip install cje-eval`\n",
        "\n",
        "Questions? Open an issue on [GitHub](https://github.com/cimo-labs/cje/issues)!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
