{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CJE (Causal Judge Evaluation) Demo\n\n**Interactive demo using Arena 5K sample data (1000 prompts)**\n\nThis notebook demonstrates how to use CJE to evaluate LLM policies using judge scores and oracle labels. We'll walk through:\n\n1. **Setup**: Install CJE and download sample data\n2. **Inspect Data**: Understand the dataset structure\n3. **Direct Mode**: Simple policy comparison (no logprobs needed!)\n4. **IPS Mode**: Counterfactual estimates from logged data\n5. **DR Mode**: Maximum accuracy with both logged + fresh data\n6. **Policy Selection**: Statistical comparison with confidence intervals\n\n---\n\n## What is CJE?\n\nCJE turns LLM-as-judge scores into causally interpretable estimates. Instead of naively averaging judge scores, CJE:\n- **Calibrates** judge scores to match oracle labels (AutoCal-R)\n- **Stabilizes** importance weights for off-policy evaluation (SIMCal)\n- **Reports** confidence intervals that account for all sources of uncertainty\n\n**Key insight**: Judge scores are correlational. CJE makes them causal.\n\n**Three modes, increasing complexity:**\n- **Direct**: Compare policies on eval set (simplest - no logprobs!)\n- **IPS**: Counterfactual estimates from logged data (reuse existing logs)\n- **DR**: Best of both worlds (doubly robust - most accurate)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cimo-labs/cje/blob/main/examples/cje_tutorial.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Install CJE and download the Arena sample data.\n",
    "\n",
    "**Note:** Colab comes with numpy 2.3+ pre-installed, which breaks scipy. We force-reinstall numpy 2.0.x first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Colab comes with numpy 2.3+ which breaks scipy\n# Force install compatible numpy first\n!pip install -q 'numpy>=2.0,<2.1' --force-reinstall\n\n# Install latest CJE from PyPI with cache busting\n!pip install --no-cache-dir --upgrade cje-eval\n\n# Verify installation\nimport cje\nprint(f\"\u2713 CJE version {cje.__version__} installed\")\n\n# Check numpy version\nimport numpy as np\nprint(f\"\u2713 NumPy version {np.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Arena sample data from GitHub\n",
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data directory\n",
    "DATA_DIR = Path(\"arena_sample\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "(DATA_DIR / \"fresh_draws\").mkdir(exist_ok=True)\n",
    "\n",
    "# Base URL for raw files on GitHub\n",
    "BASE_URL = \"https://raw.githubusercontent.com/cimo-labs/cje/main/examples/arena_sample\"\n",
    "\n",
    "# Download logged data\n",
    "print(\"Downloading logged_data.jsonl...\")\n",
    "urllib.request.urlretrieve(\n",
    "    f\"{BASE_URL}/logged_data.jsonl\",\n",
    "    DATA_DIR / \"logged_data.jsonl\"\n",
    ")\n",
    "print(\"\u2713 Downloaded logged_data.jsonl\")\n",
    "\n",
    "# Download fresh draws (note: actual filenames have _responses.jsonl suffix)\n",
    "fresh_draw_files = {\n",
    "    \"clone\": \"clone_responses.jsonl\",\n",
    "    \"parallel_universe_prompt\": \"parallel_universe_prompt_responses.jsonl\",\n",
    "    \"unhelpful\": \"unhelpful_responses.jsonl\"\n",
    "}\n",
    "\n",
    "for policy, filename in fresh_draw_files.items():\n",
    "    print(f\"Downloading fresh_draws/{filename}...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        f\"{BASE_URL}/fresh_draws/{filename}\",\n",
    "        DATA_DIR / \"fresh_draws\" / filename\n",
    "    )\n",
    "    print(f\"\u2713 Downloaded {filename}\")\n",
    "\n",
    "print(\"\\n\u2713 All data downloaded successfully!\")\n",
    "print(f\"\\nData location: {DATA_DIR.absolute()}\")\n",
    "print(f\"Policies available: {list(fresh_draw_files.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inspect the Data\n",
    "\n",
    "Let's look at what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport pandas as pd\n\n# Load and inspect logged data\nwith open(DATA_DIR / \"logged_data.jsonl\") as f:\n    logged_samples = [json.loads(line) for line in f]\n\nprint(f\"Logged data: {len(logged_samples)} samples\")\nprint(f\"\\nTarget policies: {list(logged_samples[0]['target_policy_logprobs'].keys())}\")\nprint(f\"\\nExample sample:\")\nsample = logged_samples[0]\nprint(f\"  Prompt: {sample['prompt'][:100]}...\")\nprint(f\"  Response: {sample['response'][:150]}...\")\nprint(f\"  Judge score: {sample['judge_score']}\")\noracle_label = sample.get('oracle_label', 'N/A')\nprint(f\"  Oracle label: {oracle_label}\")\nprint(f\"  Has logprobs: \u2713\")\n\n# Check oracle coverage\nn_with_oracle = sum(1 for s in logged_samples if s.get('oracle_label') is not None)\ncoverage = n_with_oracle / len(logged_samples)\nprint(f\"\\nOracle label coverage: {n_with_oracle}/{len(logged_samples)} ({coverage:.1%})\")\nprint(f\"\u2192 AutoCal-R will use these {n_with_oracle} oracle labels to calibrate judge scores\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Mode 1 - Direct (On-Policy Evaluation)\n",
    "\n",
    "**Use case**: Compare policies on a specific evaluation set (simplest!)\n",
    "\n",
    "**Estimand**: \"Which policy performs best on *this* prompt set?\"\n",
    "\n",
    "**How it works**:\n",
    "1. Generate fresh responses from each policy on same prompts\n",
    "2. Calibrate judge scores using oracle labels (AutoCal-R)\n",
    "3. Average: V\u0302(\u03c0) = (1/m) \u03a3 R_\u03c0i\n",
    "\n",
    "**Key advantage**: No logprobs needed! Simplest mode for quick policy comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cje import analyze_dataset\n",
    "\n",
    "# Direct mode: Fresh draws only (learns calibration from oracle labels in fresh draws)\n",
    "results_direct = analyze_dataset(\n",
    "    fresh_draws_dir=str(DATA_DIR / \"fresh_draws\"),  # No logged data!\n",
    "    estimator=\"auto\",  # Auto-detects Direct mode\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Direct Mode Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {results_direct.metadata['mode']}\")\n",
    "print(f\"Estimator: {results_direct.metadata['estimator']}\")\n",
    "print(f\"Calibration: {results_direct.metadata.get('calibration', 'none')}\")\n",
    "print(f\"Oracle coverage: {results_direct.metadata.get('oracle_coverage', 0):.1%}\")\n",
    "print()\n",
    "\n",
    "# Show estimates with confidence intervals\n",
    "policies = results_direct.metadata['target_policies']\n",
    "print(f\"{'Policy':<30} {'Estimate':<12} {'Std Error':<12} {'95% CI':<20}\")\n",
    "print(\"-\" * 74)\n",
    "for i, policy in enumerate(policies):\n",
    "    est = results_direct.estimates[i]\n",
    "    se = results_direct.standard_errors[i]\n",
    "    ci_low = est - 1.96 * se\n",
    "    ci_high = est + 1.96 * se\n",
    "    print(f\"{policy:<30} {est:>6.3f}       {se:>6.3f}       [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mode 2 - IPS (Importance Sampling)\n",
    "\n",
    "**Use case**: Reuse logged data to estimate counterfactual performance\n",
    "\n",
    "**Estimand**: \"What would the KPI be if we deployed policy \u03c0' instead of \u03c0\u2080?\"\n",
    "\n",
    "**How it works**:\n",
    "1. Calibrate judge scores \u2192 oracle-scale rewards (AutoCal-R)\n",
    "2. Compute importance weights: W = \u03c0'(a|x) / \u03c0\u2080(a|x)\n",
    "3. Stabilize weights with monotone projection (SIMCal)\n",
    "4. Estimate: V\u0302(\u03c0') = (1/n) \u03a3 W_i \u00b7 R_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPS mode: Logged data only (auto-selects calibrated-ips estimator)\n",
    "results_ips = analyze_dataset(\n",
    "    logged_data_path=str(DATA_DIR / \"logged_data.jsonl\"),\n",
    "    estimator=\"auto\",  # Auto-detects IPS mode\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IPS Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {results_ips.metadata['mode']}\")\n",
    "print(f\"Estimator: {results_ips.metadata['estimator']}\")\n",
    "print(f\"Logprob coverage: {results_ips.metadata['mode_selection']['logprob_coverage']:.1%}\")\n",
    "print()\n",
    "\n",
    "# Show estimates with confidence intervals\n",
    "print(f\"{'Policy':<30} {'Estimate':<12} {'Std Error':<12} {'95% CI':<20}\")\n",
    "print(\"-\" * 74)\n",
    "for i, policy in enumerate(policies):\n",
    "    est = results_ips.estimates[i]\n",
    "    se = results_ips.standard_errors[i]\n",
    "    ci_low = est - 1.96 * se\n",
    "    ci_high = est + 1.96 * se\n",
    "    print(f\"{policy:<30} {est:>6.3f}       {se:>6.3f}       [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check IPS Diagnostics\n",
    "\n",
    "**ESS (Effective Sample Size)** is the key diagnostic for IPS reliability:\n",
    "- ESS \u2265 50%: Excellent overlap\n",
    "- ESS \u2208 [10%, 50%): Moderate (DR recommended)\n",
    "- ESS < 10%: Poor overlap (switch to DR or regenerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check diagnostics for each policy\nprint(\"IPS Diagnostics\")\nprint(\"=\"*70)\n\nfor policy in policies:\n    ess = results_ips.diagnostics.ess_per_policy.get(policy, 0.0)\n    \n    # Traffic light assessment\n    if ess >= 0.5:\n        status = \"\u2713 EXCELLENT\"\n    elif ess >= 0.1:\n        status = \"\u26a0 MODERATE (consider DR)\"\n    else:\n        status = \"\u2717 POOR (use DR or regenerate)\"\n    \n    print(f\"\\n{policy}:\")\n    print(f\"  ESS: {ess:.1%} {status}\")\n    \n    # Show weight statistics if available\n    max_weight = results_ips.diagnostics.max_weight_per_policy.get(policy)\n    if max_weight:\n        print(f\"  Max weight: {max_weight:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Mode 3 - DR (Doubly Robust)\n",
    "\n",
    "**Use case**: Most accurate counterfactual estimates when you have fresh draws\n",
    "\n",
    "**Estimand**: Same as IPS, but with better accuracy\n",
    "\n",
    "**How it works**:\n",
    "1. Everything from IPS mode\n",
    "2. Train outcome model \u011d(S) on fresh draws\n",
    "3. Combine: V\u0302_DR(\u03c0') = (1/n) \u03a3 [W_i \u00b7 (R_i - \u011d(S_i)) + \u011d(S_i)]\n",
    "\n",
    "**Double robustness**: Consistent if *either* weights or outcome model is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR mode: Logged data + fresh draws (auto-selects stacked-dr estimator)\n",
    "results_dr = analyze_dataset(\n",
    "    logged_data_path=str(DATA_DIR / \"logged_data.jsonl\"),\n",
    "    fresh_draws_dir=str(DATA_DIR / \"fresh_draws\"),\n",
    "    estimator=\"auto\",  # Auto-detects DR mode\n",
    "    estimator_config={\"parallel\": False},  # Disable parallel for Colab\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DR Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {results_dr.metadata['mode']}\")\n",
    "print(f\"Estimator: {results_dr.metadata['estimator']}\")\n",
    "print()\n",
    "\n",
    "# Compare all three modes side-by-side\n",
    "print(f\"{'Policy':<30} {'Direct':<10} {'IPS':<10} {'DR':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, policy in enumerate(policies):\n",
    "    direct_est = results_direct.estimates[i]\n",
    "    ips_est = results_ips.estimates[i]\n",
    "    dr_est = results_dr.estimates[i]\n",
    "    print(f\"{policy:<30} {direct_est:>6.3f}     {ips_est:>6.3f}     {dr_est:>6.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Standard Error Comparison (IPS vs DR)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Policy':<30} {'IPS SE':<12} {'DR SE':<12} {'Improvement':<15}\")\n",
    "print(\"-\" * 69)\n",
    "for i, policy in enumerate(policies):\n",
    "    ips_se = results_ips.standard_errors[i]\n",
    "    dr_se = results_dr.standard_errors[i]\n",
    "    \n",
    "    # Check if DR improved standard error\n",
    "    if dr_se < ips_se:\n",
    "        improvement = f\"\u2193 {(1 - dr_se/ips_se)*100:.0f}% SE\"\n",
    "    else:\n",
    "        improvement = \"(similar)\"\n",
    "    \n",
    "    print(f\"{policy:<30} {ips_se:>6.3f}       {dr_se:>6.3f}       {improvement:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\n\n# Use DR results (most accurate)\npolicies = results_dr.metadata['target_policies']\nestimates = np.array(results_dr.estimates)\nstd_errors = np.array(results_dr.standard_errors)\n\n# Find best policy using built-in method\nbest_idx = results_dr.best_policy()\nbest_policy = policies[best_idx]\nbest_est = estimates[best_idx]\nbest_se = std_errors[best_idx]\n\nprint(\"Policy Selection (DR Mode)\")\nprint(\"=\"*70)\nprint(f\"\\n\ud83c\udfc6 Best policy: {best_policy}\")\nprint(f\"   Estimate: {best_est:.3f} \u00b1 {best_se:.3f}\")\nprint(f\"   95% CI: [{best_est - 1.96*best_se:.3f}, {best_est + 1.96*best_se:.3f}]\")\n\n# Compare against baseline (clone) using built-in comparison\nbaseline_policy = 'clone'\nbaseline_idx = policies.index(baseline_policy)\n\nprint(f\"\\n\ud83d\udcca Comparison to baseline ({baseline_policy}):\")\nprint(f\"   Baseline: {estimates[baseline_idx]:.3f} \u00b1 {std_errors[baseline_idx]:.3f}\")\nprint()\nprint(f\"{'Policy':<30} {'Delta':<12} {'SE(\u0394)':<12} {'p-value':<12} {'Significant?':<15}\")\nprint(\"-\" * 81)\n\nfor i, policy in enumerate(policies):\n    if i == baseline_idx:\n        print(f\"{policy:<30} {'(baseline)':<12} {'':<12} {'':<12} {'':<15}\")\n        continue\n    \n    # Use built-in comparison (uses influence functions for paired variance)\n    comp = results_dr.compare_policies(i, baseline_idx)\n    \n    sig_text = \"\u2713 Yes (p<0.05)\" if comp['significant'] else \"No\"\n    method_text = \"(paired)\" if comp['used_influence'] else \"(indep.)\"\n    \n    print(f\"{policy:<30} {comp['difference']:+.3f}       \"\n          f\"{comp['se_difference']:.3f} {method_text:<6} \"\n          f\"{comp['p_value']:.3f}      {sig_text:<15}\")\n\nprint(\"\\n\ud83d\udca1 Note: Using influence functions for paired comparisons when available.\")\nprint(\"   This properly accounts for correlation between estimates on the same prompts.\")"
  },
  {
   "cell_type": "code",
   "source": "# Just evaluate the result to see a nice HTML table\nresults_dr",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Jupyter Auto-Display\n\nIn Jupyter notebooks (including Colab), results automatically display as formatted HTML tables when you evaluate them:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import matplotlib for plotting\nimport matplotlib.pyplot as plt\n\n# Use the convenience method\nfig = results_dr.plot_estimates()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 The .plot_estimates() method automatically extracts data from the result object\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quick Plotting with Convenience Method\n\nYou can also use the convenience method on `EstimationResult` for quick plotting:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import visualization functions from cje\nfrom cje import plot_policy_estimates\n\n# Plot DR results with all three policies\n# Extract estimates as dictionary\nestimates_dict = {policy: float(results_dr.estimates[i]) for i, policy in enumerate(policies)}\nses_dict = {policy: float(results_dr.standard_errors[i]) for i, policy in enumerate(policies)}\n\n# Create forest plot with confidence intervals\nfig = plot_policy_estimates(\n    estimates=estimates_dict,\n    standard_errors=ses_dict,\n    figsize=(10, 5)\n)\n\n# Display\nimport matplotlib.pyplot as plt\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 Forest plot shows point estimates with 95% confidence intervals\")\nprint(\"  Green dot = best policy, Gray square would be baseline if provided\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Visualization\n\nCJE provides built-in visualization functions to help understand and communicate results. You can import them directly from the main `cje` namespace:\n\n**Available plot functions:**\n- `plot_policy_estimates` - Forest plots with confidence intervals\n- `plot_calibration_comparison` - Judge \u2192 oracle calibration curves  \n- `plot_weight_dashboard_summary` - Weight diagnostics (ESS, concentration)\n- `plot_weight_dashboard_detailed` - Per-policy weight analysis\n- `plot_dr_dashboard` - Doubly robust diagnostics\n- `plot_transport_audit` - Calibrator transportability tests (advanced)\n- `plot_transport_comparison` - Multi-policy transport comparison (advanced)\n\nBelow we show a simple example with `plot_policy_estimates`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: When to Use Each Mode\n\n| Mode | Data Required | Estimand | Use When |\n|------|--------------|----------|----------|\n| **Direct** | Fresh draws (+ optional oracle labels) | Performance on eval set | Just want on-policy comparison, simplest! |\n| **IPS** | Logged data + logprobs | Counterfactual deployment value | Have logged data, want off-policy estimates |\n| **DR** | Logged data + fresh draws | Counterfactual (most accurate) | Have both logged and fresh data |\n\n### Key Diagnostics to Check\n\n- **Direct mode**: Oracle coverage (5-10% is often enough for calibration)\n- **IPS mode**: ESS \u2265 10% (prefer \u2265 50%)\n- **DR mode**: Standard errors typically lower than IPS\n\n### Pro Tips\n\n1. **Start with Direct mode**: Simplest, no logprobs needed, great for quick comparisons\n2. **Use `estimator=\"auto\"`**: CJE selects the best mode and estimator automatically\n3. **Check diagnostics first**: Don't trust estimates if ESS < 10%\n4. **Report confidence intervals**: CJE's CIs account for all uncertainty sources (oracle, calibration, sampling)\n5. **DR > IPS when possible**: Doubly robust is more accurate and robust\n\n---\n\n## Next Steps\n\n- **Documentation**: [CJE README](https://github.com/cimo-labs/cje)\n- **Technical Details**: [Arena Experiment - Full Benchmarking](https://www.cimolabs.com/blog/arena-experiment)\n- **Install locally**: `pip install cje-eval`\n- **More examples**: See [examples/](https://github.com/cimo-labs/cje/tree/main/examples)\n\nQuestions? Open an issue on [GitHub](https://github.com/cimo-labs/cje/issues)!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}